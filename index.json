[{"authors":["admin"],"categories":null,"content":" \u0026quot;A human being should be able to change a diaper, plan an invasion, butcher a hog, conn a ship, design a building, write a sonnet, balance accounts, build a wall, set a bone, comfort the dying, take orders, give orders, cooperate, act alone, solve equations, analyze a new problem, pitch manure, program a computer, cook a tasty meal, fight efficiently, die gallantly. Specialization is for insects.\u0026rdquo; - Robert A. Heinlein\n This is a page dedicated to showcasing my varied projects, both personal and for work. I love all things AI and Data Science, hence why I\u0026rsquo;m pursuing a Master\u0026rsquo;s in this field. I\u0026rsquo;d love to show you what I\u0026rsquo;ve produced, so hang around and click on stuff!\n","date":1554595200,"expirydate":-62135596800,"kind":"taxonomy","lang":"en","lastmod":1567641600,"objectID":"2525497d367e79493fd32b198b28f040","permalink":"https://nixramirez.github.io/authors/admin/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/admin/","section":"authors","summary":"\u0026quot;A human being should be able to change a diaper, plan an invasion, butcher a hog, conn a ship, design a building, write a sonnet, balance accounts, build a wall, set a bone, comfort the dying, take orders, give orders, cooperate, act alone, solve equations, analyze a new problem, pitch manure, program a computer, cook a tasty meal, fight efficiently, die gallantly. Specialization is for insects.\u0026rdquo; - Robert A. Heinlein","tags":null,"title":"Nicole Ramirez","type":"authors"},{"authors":null,"categories":null,"content":"Flexibility This feature can be used for publishing content such as:\n Online courses Project or software documentation Tutorials  The courses folder may be renamed. For example, we can rename it to docs for software/project documentation or tutorials for creating an online course.\nDelete tutorials To remove these pages, delete the courses folder and see below to delete the associated menu link.\nUpdate site menu After renaming or deleting the courses folder, you may wish to update any [[main]] menu links to it by editing your menu configuration at config/_default/menus.toml.\nFor example, if you delete this folder, you can remove the following from your menu configuration:\n[[main]] name = \u0026quot;Courses\u0026quot; url = \u0026quot;courses/\u0026quot; weight = 50  Or, if you are creating a software documentation site, you can rename the courses folder to docs and update the associated Courses menu configuration to:\n[[main]] name = \u0026quot;Docs\u0026quot; url = \u0026quot;docs/\u0026quot; weight = 50  Update the docs menu If you use the docs layout, note that the name of the menu in the front matter should be in the form [menu.X] where X is the folder name. Hence, if you rename the courses/example/ folder, you should also rename the menu definitions in the front matter of files within courses/example/ from [menu.example] to [menu.\u0026lt;NewFolderName\u0026gt;].\n","date":1536451200,"expirydate":-62135596800,"kind":"section","lang":"en","lastmod":1536451200,"objectID":"59c3ce8e202293146a8a934d37a4070b","permalink":"https://nixramirez.github.io/courses/example/","publishdate":"2018-09-09T00:00:00Z","relpermalink":"/courses/example/","section":"courses","summary":"Learn how to use Academic's docs layout for publishing online courses, software documentation, and tutorials.","tags":null,"title":"Overview","type":"docs"},{"authors":null,"categories":null,"content":"In this tutorial, I\u0026rsquo;ll share my top 10 tips for getting started with Academic:\nTip 1 Lorem ipsum dolor sit amet, consectetur adipiscing elit. Duis posuere tellus ac convallis placerat. Proin tincidunt magna sed ex sollicitudin condimentum. Sed ac faucibus dolor, scelerisque sollicitudin nisi. Cras purus urna, suscipit quis sapien eu, pulvinar tempor diam. Quisque risus orci, mollis id ante sit amet, gravida egestas nisl. Sed ac tempus magna. Proin in dui enim. Donec condimentum, sem id dapibus fringilla, tellus enim condimentum arcu, nec volutpat est felis vel metus. Vestibulum sit amet erat at nulla eleifend gravida.\nNullam vel molestie justo. Curabitur vitae efficitur leo. In hac habitasse platea dictumst. Sed pulvinar mauris dui, eget varius purus congue ac. Nulla euismod, lorem vel elementum dapibus, nunc justo porta mi, sed tempus est est vel tellus. Nam et enim eleifend, laoreet sem sit amet, elementum sem. Morbi ut leo congue, maximus velit ut, finibus arcu. In et libero cursus, rutrum risus non, molestie leo. Nullam congue quam et volutpat malesuada. Sed risus tortor, pulvinar et dictum nec, sodales non mi. Phasellus lacinia commodo laoreet. Nam mollis, erat in feugiat consectetur, purus eros egestas tellus, in auctor urna odio at nibh. Mauris imperdiet nisi ac magna convallis, at rhoncus ligula cursus.\nCras aliquam rhoncus ipsum, in hendrerit nunc mattis vitae. Duis vitae efficitur metus, ac tempus leo. Cras nec fringilla lacus. Quisque sit amet risus at ipsum pharetra commodo. Sed aliquam mauris at consequat eleifend. Praesent porta, augue sed viverra bibendum, neque ante euismod ante, in vehicula justo lorem ac eros. Suspendisse augue libero, venenatis eget tincidunt ut, malesuada at lorem. Donec vitae bibendum arcu. Aenean maximus nulla non pretium iaculis. Quisque imperdiet, nulla in pulvinar aliquet, velit quam ultrices quam, sit amet fringilla leo sem vel nunc. Mauris in lacinia lacus.\nSuspendisse a tincidunt lacus. Curabitur at urna sagittis, dictum ante sit amet, euismod magna. Sed rutrum massa id tortor commodo, vitae elementum turpis tempus. Lorem ipsum dolor sit amet, consectetur adipiscing elit. Aenean purus turpis, venenatis a ullamcorper nec, tincidunt et massa. Integer posuere quam rutrum arcu vehicula imperdiet. Mauris ullamcorper quam vitae purus congue, quis euismod magna eleifend. Vestibulum semper vel augue eget tincidunt. Fusce eget justo sodales, dapibus odio eu, ultrices lorem. Duis condimentum lorem id eros commodo, in facilisis mauris scelerisque. Morbi sed auctor leo. Nullam volutpat a lacus quis pharetra. Nulla congue rutrum magna a ornare.\nAliquam in turpis accumsan, malesuada nibh ut, hendrerit justo. Cum sociis natoque penatibus et magnis dis parturient montes, nascetur ridiculus mus. Quisque sed erat nec justo posuere suscipit. Donec ut efficitur arcu, in malesuada neque. Nunc dignissim nisl massa, id vulputate nunc pretium nec. Quisque eget urna in risus suscipit ultricies. Pellentesque odio odio, tincidunt in eleifend sed, posuere a diam. Nam gravida nisl convallis semper elementum. Morbi vitae felis faucibus, vulputate orci placerat, aliquet nisi. Aliquam erat volutpat. Maecenas sagittis pulvinar purus, sed porta quam laoreet at.\nTip 2 Lorem ipsum dolor sit amet, consectetur adipiscing elit. Duis posuere tellus ac convallis placerat. Proin tincidunt magna sed ex sollicitudin condimentum. Sed ac faucibus dolor, scelerisque sollicitudin nisi. Cras purus urna, suscipit quis sapien eu, pulvinar tempor diam. Quisque risus orci, mollis id ante sit amet, gravida egestas nisl. Sed ac tempus magna. Proin in dui enim. Donec condimentum, sem id dapibus fringilla, tellus enim condimentum arcu, nec volutpat est felis vel metus. Vestibulum sit amet erat at nulla eleifend gravida.\nNullam vel molestie justo. Curabitur vitae efficitur leo. In hac habitasse platea dictumst. Sed pulvinar mauris dui, eget varius purus congue ac. Nulla euismod, lorem vel elementum dapibus, nunc justo porta mi, sed tempus est est vel tellus. Nam et enim eleifend, laoreet sem sit amet, elementum sem. Morbi ut leo congue, maximus velit ut, finibus arcu. In et libero cursus, rutrum risus non, molestie leo. Nullam congue quam et volutpat malesuada. Sed risus tortor, pulvinar et dictum nec, sodales non mi. Phasellus lacinia commodo laoreet. Nam mollis, erat in feugiat consectetur, purus eros egestas tellus, in auctor urna odio at nibh. Mauris imperdiet nisi ac magna convallis, at rhoncus ligula cursus.\nCras aliquam rhoncus ipsum, in hendrerit nunc mattis vitae. Duis vitae efficitur metus, ac tempus leo. Cras nec fringilla lacus. Quisque sit amet risus at ipsum pharetra commodo. Sed aliquam mauris at consequat eleifend. Praesent porta, augue sed viverra bibendum, neque ante euismod ante, in vehicula justo lorem ac eros. Suspendisse augue libero, venenatis eget tincidunt ut, malesuada at lorem. Donec vitae bibendum arcu. Aenean maximus nulla non pretium iaculis. Quisque imperdiet, nulla in pulvinar aliquet, velit quam ultrices quam, sit amet fringilla leo sem vel nunc. Mauris in lacinia lacus.\nSuspendisse a tincidunt lacus. Curabitur at urna sagittis, dictum ante sit amet, euismod magna. Sed rutrum massa id tortor commodo, vitae elementum turpis tempus. Lorem ipsum dolor sit amet, consectetur adipiscing elit. Aenean purus turpis, venenatis a ullamcorper nec, tincidunt et massa. Integer posuere quam rutrum arcu vehicula imperdiet. Mauris ullamcorper quam vitae purus congue, quis euismod magna eleifend. Vestibulum semper vel augue eget tincidunt. Fusce eget justo sodales, dapibus odio eu, ultrices lorem. Duis condimentum lorem id eros commodo, in facilisis mauris scelerisque. Morbi sed auctor leo. Nullam volutpat a lacus quis pharetra. Nulla congue rutrum magna a ornare.\nAliquam in turpis accumsan, malesuada nibh ut, hendrerit justo. Cum sociis natoque penatibus et magnis dis parturient montes, nascetur ridiculus mus. Quisque sed erat nec justo posuere suscipit. Donec ut efficitur arcu, in malesuada neque. Nunc dignissim nisl massa, id vulputate nunc pretium nec. Quisque eget urna in risus suscipit ultricies. Pellentesque odio odio, tincidunt in eleifend sed, posuere a diam. Nam gravida nisl convallis semper elementum. Morbi vitae felis faucibus, vulputate orci placerat, aliquet nisi. Aliquam erat volutpat. Maecenas sagittis pulvinar purus, sed porta quam laoreet at.\n","date":1557010800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1557010800,"objectID":"74533bae41439377bd30f645c4677a27","permalink":"https://nixramirez.github.io/courses/example/example1/","publishdate":"2019-05-05T00:00:00+01:00","relpermalink":"/courses/example/example1/","section":"courses","summary":"In this tutorial, I\u0026rsquo;ll share my top 10 tips for getting started with Academic:\nTip 1 Lorem ipsum dolor sit amet, consectetur adipiscing elit. Duis posuere tellus ac convallis placerat. Proin tincidunt magna sed ex sollicitudin condimentum. Sed ac faucibus dolor, scelerisque sollicitudin nisi. Cras purus urna, suscipit quis sapien eu, pulvinar tempor diam. Quisque risus orci, mollis id ante sit amet, gravida egestas nisl. Sed ac tempus magna. Proin in dui enim.","tags":null,"title":"Example Page 1","type":"docs"},{"authors":null,"categories":null,"content":"Here are some more tips for getting started with Academic:\nTip 3 Lorem ipsum dolor sit amet, consectetur adipiscing elit. Duis posuere tellus ac convallis placerat. Proin tincidunt magna sed ex sollicitudin condimentum. Sed ac faucibus dolor, scelerisque sollicitudin nisi. Cras purus urna, suscipit quis sapien eu, pulvinar tempor diam. Quisque risus orci, mollis id ante sit amet, gravida egestas nisl. Sed ac tempus magna. Proin in dui enim. Donec condimentum, sem id dapibus fringilla, tellus enim condimentum arcu, nec volutpat est felis vel metus. Vestibulum sit amet erat at nulla eleifend gravida.\nNullam vel molestie justo. Curabitur vitae efficitur leo. In hac habitasse platea dictumst. Sed pulvinar mauris dui, eget varius purus congue ac. Nulla euismod, lorem vel elementum dapibus, nunc justo porta mi, sed tempus est est vel tellus. Nam et enim eleifend, laoreet sem sit amet, elementum sem. Morbi ut leo congue, maximus velit ut, finibus arcu. In et libero cursus, rutrum risus non, molestie leo. Nullam congue quam et volutpat malesuada. Sed risus tortor, pulvinar et dictum nec, sodales non mi. Phasellus lacinia commodo laoreet. Nam mollis, erat in feugiat consectetur, purus eros egestas tellus, in auctor urna odio at nibh. Mauris imperdiet nisi ac magna convallis, at rhoncus ligula cursus.\nCras aliquam rhoncus ipsum, in hendrerit nunc mattis vitae. Duis vitae efficitur metus, ac tempus leo. Cras nec fringilla lacus. Quisque sit amet risus at ipsum pharetra commodo. Sed aliquam mauris at consequat eleifend. Praesent porta, augue sed viverra bibendum, neque ante euismod ante, in vehicula justo lorem ac eros. Suspendisse augue libero, venenatis eget tincidunt ut, malesuada at lorem. Donec vitae bibendum arcu. Aenean maximus nulla non pretium iaculis. Quisque imperdiet, nulla in pulvinar aliquet, velit quam ultrices quam, sit amet fringilla leo sem vel nunc. Mauris in lacinia lacus.\nSuspendisse a tincidunt lacus. Curabitur at urna sagittis, dictum ante sit amet, euismod magna. Sed rutrum massa id tortor commodo, vitae elementum turpis tempus. Lorem ipsum dolor sit amet, consectetur adipiscing elit. Aenean purus turpis, venenatis a ullamcorper nec, tincidunt et massa. Integer posuere quam rutrum arcu vehicula imperdiet. Mauris ullamcorper quam vitae purus congue, quis euismod magna eleifend. Vestibulum semper vel augue eget tincidunt. Fusce eget justo sodales, dapibus odio eu, ultrices lorem. Duis condimentum lorem id eros commodo, in facilisis mauris scelerisque. Morbi sed auctor leo. Nullam volutpat a lacus quis pharetra. Nulla congue rutrum magna a ornare.\nAliquam in turpis accumsan, malesuada nibh ut, hendrerit justo. Cum sociis natoque penatibus et magnis dis parturient montes, nascetur ridiculus mus. Quisque sed erat nec justo posuere suscipit. Donec ut efficitur arcu, in malesuada neque. Nunc dignissim nisl massa, id vulputate nunc pretium nec. Quisque eget urna in risus suscipit ultricies. Pellentesque odio odio, tincidunt in eleifend sed, posuere a diam. Nam gravida nisl convallis semper elementum. Morbi vitae felis faucibus, vulputate orci placerat, aliquet nisi. Aliquam erat volutpat. Maecenas sagittis pulvinar purus, sed porta quam laoreet at.\nTip 4 Lorem ipsum dolor sit amet, consectetur adipiscing elit. Duis posuere tellus ac convallis placerat. Proin tincidunt magna sed ex sollicitudin condimentum. Sed ac faucibus dolor, scelerisque sollicitudin nisi. Cras purus urna, suscipit quis sapien eu, pulvinar tempor diam. Quisque risus orci, mollis id ante sit amet, gravida egestas nisl. Sed ac tempus magna. Proin in dui enim. Donec condimentum, sem id dapibus fringilla, tellus enim condimentum arcu, nec volutpat est felis vel metus. Vestibulum sit amet erat at nulla eleifend gravida.\nNullam vel molestie justo. Curabitur vitae efficitur leo. In hac habitasse platea dictumst. Sed pulvinar mauris dui, eget varius purus congue ac. Nulla euismod, lorem vel elementum dapibus, nunc justo porta mi, sed tempus est est vel tellus. Nam et enim eleifend, laoreet sem sit amet, elementum sem. Morbi ut leo congue, maximus velit ut, finibus arcu. In et libero cursus, rutrum risus non, molestie leo. Nullam congue quam et volutpat malesuada. Sed risus tortor, pulvinar et dictum nec, sodales non mi. Phasellus lacinia commodo laoreet. Nam mollis, erat in feugiat consectetur, purus eros egestas tellus, in auctor urna odio at nibh. Mauris imperdiet nisi ac magna convallis, at rhoncus ligula cursus.\nCras aliquam rhoncus ipsum, in hendrerit nunc mattis vitae. Duis vitae efficitur metus, ac tempus leo. Cras nec fringilla lacus. Quisque sit amet risus at ipsum pharetra commodo. Sed aliquam mauris at consequat eleifend. Praesent porta, augue sed viverra bibendum, neque ante euismod ante, in vehicula justo lorem ac eros. Suspendisse augue libero, venenatis eget tincidunt ut, malesuada at lorem. Donec vitae bibendum arcu. Aenean maximus nulla non pretium iaculis. Quisque imperdiet, nulla in pulvinar aliquet, velit quam ultrices quam, sit amet fringilla leo sem vel nunc. Mauris in lacinia lacus.\nSuspendisse a tincidunt lacus. Curabitur at urna sagittis, dictum ante sit amet, euismod magna. Sed rutrum massa id tortor commodo, vitae elementum turpis tempus. Lorem ipsum dolor sit amet, consectetur adipiscing elit. Aenean purus turpis, venenatis a ullamcorper nec, tincidunt et massa. Integer posuere quam rutrum arcu vehicula imperdiet. Mauris ullamcorper quam vitae purus congue, quis euismod magna eleifend. Vestibulum semper vel augue eget tincidunt. Fusce eget justo sodales, dapibus odio eu, ultrices lorem. Duis condimentum lorem id eros commodo, in facilisis mauris scelerisque. Morbi sed auctor leo. Nullam volutpat a lacus quis pharetra. Nulla congue rutrum magna a ornare.\nAliquam in turpis accumsan, malesuada nibh ut, hendrerit justo. Cum sociis natoque penatibus et magnis dis parturient montes, nascetur ridiculus mus. Quisque sed erat nec justo posuere suscipit. Donec ut efficitur arcu, in malesuada neque. Nunc dignissim nisl massa, id vulputate nunc pretium nec. Quisque eget urna in risus suscipit ultricies. Pellentesque odio odio, tincidunt in eleifend sed, posuere a diam. Nam gravida nisl convallis semper elementum. Morbi vitae felis faucibus, vulputate orci placerat, aliquet nisi. Aliquam erat volutpat. Maecenas sagittis pulvinar purus, sed porta quam laoreet at.\n","date":1557010800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1557010800,"objectID":"1c2b5a11257c768c90d5050637d77d6a","permalink":"https://nixramirez.github.io/courses/example/example2/","publishdate":"2019-05-05T00:00:00+01:00","relpermalink":"/courses/example/example2/","section":"courses","summary":"Here are some more tips for getting started with Academic:\nTip 3 Lorem ipsum dolor sit amet, consectetur adipiscing elit. Duis posuere tellus ac convallis placerat. Proin tincidunt magna sed ex sollicitudin condimentum. Sed ac faucibus dolor, scelerisque sollicitudin nisi. Cras purus urna, suscipit quis sapien eu, pulvinar tempor diam. Quisque risus orci, mollis id ante sit amet, gravida egestas nisl. Sed ac tempus magna. Proin in dui enim. Donec condimentum, sem id dapibus fringilla, tellus enim condimentum arcu, nec volutpat est felis vel metus.","tags":null,"title":"Example Page 2","type":"docs"},{"authors":[],"categories":null,"content":" Click on the Slides button above to view the built-in slides feature.   Slides can be added in a few ways:\n Create slides using Academic\u0026rsquo;s Slides feature and link using slides parameter in the front matter of the talk file Upload an existing slide deck to static/ and link using url_slides parameter in the front matter of the talk file Embed your slides (e.g. Google Slides) or presentation video on this page using shortcodes .  Further talk details can easily be added to this page using Markdown and $\\rm \\LaTeX$ math code.\n","date":1906549200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1906549200,"objectID":"96344c08df50a1b693cc40432115cbe3","permalink":"https://nixramirez.github.io/talk/example/","publishdate":"2017-01-01T00:00:00Z","relpermalink":"/talk/example/","section":"talk","summary":"An example talk using Academic's Markdown slides feature.","tags":[],"title":"Example Talk","type":"talk"},{"authors":null,"categories":null,"content":"For the second semester of 2020, I had the privilege of teaching another workshop for the Data Science Club, this time covering Data Visualisation packages Matplotlib and Seaborn in Python.\nI co-taught the workshop with a fellow club executive member, Saahil, also another Data Science Master\u0026rsquo;s student from the University.\nIn the workshop, we work through a data-set and explain syntax and customisation options for univariate (i.e. single variable) and multivariate (i.e. more than 1 variable) plotting tasks.\nWatch on below!  The materials accompanying the workshop can be downloaded from here \n","date":1598313600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1598313600,"objectID":"27c6a49e3834cb1a5985cefaac077600","permalink":"https://nixramirez.github.io/project/intro-to-data-visualisation/","publishdate":"2020-08-25T00:00:00Z","relpermalink":"/project/intro-to-data-visualisation/","section":"project","summary":"Delivered a 1 hr workshop on Python Visualisation Libraries for The University of Auckland Data Science Club","tags":["Data Projects"],"title":"Intro to Data Visualisation Workshop","type":"project"},{"authors":null,"categories":null,"content":"Here\u0026rsquo;s a proud moment - the first front-end I\u0026rsquo;ve built with React.js and JSX! It\u0026rsquo;s very simple, but it taught me a lot on how React worked internally.\nThis app displays either two screens dependent on whether it’s summer or winter wherever the user is currently.\nA user in New Zealand (i.e. me) is in the southern hemisphere where it’s currenty winter (May 2nd, when this blogpost was written). The app displays the winter screen in this case.\nA user in San Francisco, on the other hand, is in the northern hemisphere where it’s currently summer. The app displays the summer screen for them.\nHow do we know where the user is? Well, we can detect user location with the Mozilla Geolocation API . We can grab latitude and longitude information about the user from this service.\nWe can also force the Google Chrome browser to consider a location like San Francisco away from us by changing its geolocation in its ‘Sensors’ tab like so (mostly for testing the different views):\nIf the user has disabled location-sharing in any-way, the screen below encourages them to enable it\nSemantic UI and some CSS was used to style the webpage.\nThis project taught me React\u0026rsquo;s fundamentals - class based and functional components, the component life cycle methods, how to update component state, and how to pass information down from parent to children components using the props system.\nI\u0026rsquo;m currently building and deploying a more complex full-stack web app, integrating node.js/express.js for the back-end and a mongodb database. I might write a tutorial on that when it\u0026rsquo;s done, so if you\u0026rsquo;re interested, please stay tuned for that post! :)\n","date":1588377600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1588377600,"objectID":"4aa82b775b17672cf4a83521e43fedd6","permalink":"https://nixramirez.github.io/project/reactfirstproj/","publishdate":"2020-05-02T00:00:00Z","relpermalink":"/project/reactfirstproj/","section":"project","summary":"An app detecting user location and displaying information relevant to it","tags":["Programming"],"title":"Detecting Seasons in React.js","type":"project"},{"authors":null,"categories":null,"content":"In this project, I taught an introductory workshop on Python\u0026rsquo;s base syntax, plus some basic libraries for working with data (i.e. pandas, numpy, matplotlib).\nThis was done for the University of Auckland Data Science Club, which I help run.\nIt was one workshop in a series teaching essential end-to-end data science skills. Workshops on Data Cleaning, Data Visualisation and Data modelling were planned for the future.\nI work as a programming tutor, so I thought teaching such a workshop would be easy. It wasn\u0026rsquo;t!\nFirstly, I had to come up with all of the material on my own \u0026ndash; the recordings, the jupyter notebooks, the coding challenges, and sourcing the data-set. The recorded format also made it all the more daunting, as any mistakes I made would go on record officially! I also worried about delivering an engaging workshop. After all, the online nature of the workshop meant people could drop out anytime they wished.\nWe were surprised to see good engagement with this workshop, though, with people attending our Live Q\u0026amp;A sessions after the workshop to ask for help regarding our coding challenges. This suggested people were watching our videos \u0026lsquo;til the end. I\u0026rsquo;ve also received positive verbal feedback about the workshop from members, and the youtube view count for both videos suggested pretty good engagement!\nThis was such a rewarding experience, which highlighted a few things: 1.) Teaching isn\u0026rsquo;t easy 2.) Teaching is very hard to get right 3.) I love teaching and sharing the beauty of programming.\nSo now I present, the University of Auckland Data Science Club\u0026rsquo;s Intro to Python workshop!\nPart One: Python Base Snytax  Part Two: Python for Data Science  The materials accompanying the workshop are found here . These include the Jupyter notebooks used in both parts, a set of coding challenges to try after going through the workshop (and their associated answers), and the data-set used in part two.\nPlease reach out if you have any feedback or queries! :)\n","date":1587081600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1587081600,"objectID":"d60e1ba5e0ade2c09bffbb419fa6bba3","permalink":"https://nixramirez.github.io/project/pythonworkshop/","publishdate":"2020-04-17T00:00:00Z","relpermalink":"/project/pythonworkshop/","section":"project","summary":"Delivered a 2 hour, 2 part workshop on Python for Data Science for The University of Auckland Data Science Club","tags":["Programming"],"title":"Intro To Python Workshop","type":"project"},{"authors":null,"categories":null,"content":"Helpful Prerequisites  Intermediate-level knowledge of Python 3 (NumPy and Pandas preferably, but not required) Exposure to PyTorch usage Basic understanding of Deep Learning, Natural Language Processing (NLP) and Language Models (BERT specifically)  Project Outline TL;DR: We implement transfer learning using BERT to achieve 98.6% accuracy on emotion classification of tweets. We show the whole end-to-end process in this notebook.\nFeel free to click on the below links to skip to that section.\n Task 1: Introduction  Task 2: Exploratory Data Analysis  Task 3: Data Pre-processing  Task 4: Training and Validation Split  Task 5: Loading Tokenizer and Encoding our Data  Task 6: Setting up BERT Pretrained Model  Task 7: Creating Data Loaders  Task 8: Setting Up Optimizer and Scheduler  Task 9: Defining our Performance Metrics  Task 10: Creating our Training and Evaluation Loops  Task 11: Loading and Evaluating our Model Task 1: Introduction The Problem Emotion classification, or the task of ascribing an emotion category to a textual document, is a typical NLP problem solved either through machine learning (ML) or deep learning (DL) methods. The popularity of the latter solution over the former has grown in recent years, as powerful language models utilising DL have been increasingly open-sourced and utilised, alongside the tools used to build and customise them.\nNLP benefits from DL\u0026rsquo;s ability to preserve more context and require less feature-engineering than ML. There\u0026rsquo;s no free lunch, however, as utilising DL methods comes at a cost: more powerful computational resources are needed to run DL models, they may be less explainable than traditional ML ones, and model training and/or inference might take longer. Deciding which framework to use ultimately requires balancing these pros and cons.\nA particularly exciting benefit of language models trained through DL, however, is their ability to learn large amounts of information from a huge data-source and transfer their learning to solve tasks that aren\u0026rsquo;t necessarily within their training domain.\nThis concept of transfer learning has revolutionised NLP. In a nutshell, transfer learning involves training a large neural network on extremely large data-sets to learn rich, nuanced, generalisable information that it can transfer to a smaller model fine-tuned to solve a specific task. With transfer learning, the smaller model performs better at this task and/or solves it quicker after gaining this external knowledge than if it was left alone to tackle the task.\nState-of-the-art achievements in transfer learning were enabled by innovatively architected deep neural networks called transformers .\nIn this notebook, we try to classify the emotion of tweets through leveraging the knowledge of a particular transformer-based model called BERT, trained and open-sourced by Google. BERT is a large-scale transformer-based Language Model that can be finetuned for a variety of tasks. It beat multiple NLP benchmarks during its release in 2018 .\nArmed with the generalist language knowledge of BERT, we fine-tune BERT on our own dataset , a collection of 3,085 tweets each classified according to 5 emotions (i.e. anger, disgust, happiness, surprise and sadness). This collection of tweets mentions 13 Twitter handles associated with British museums, and was gathered between May 2013 and June 2015. It was created for the purpose of classifying emotions, expressed on Twitter, towards arts and cultural experiences in museums.\nFine-tuning on this particular data-set allows us to classify a tweet into one of 5 emotion categories later on. We\u0026rsquo;ll tackle the technical details in relevant sections of this notebook.\nFor more information about BERT, the original publication for it is linked here .\nWe are using Huggingface\u0026rsquo;s implementation of BERT for this project, written in PyTorch.\nTask 2: Exploratory Data Analysis First, let\u0026rsquo;s make sure we\u0026rsquo;re in the directory containing our data-set. In my system (working off Google Colaboratory connected to a Google Drive back-end), my directory is specified in the PATH variable below.\n#we list the contents of our directory !ls  sample_data  PATH = './drive/My Drive/Colab Notebooks/bert-emotion-tweets-tutorial/'  import os  We change to the directory via the os.chdir() command\nos.chdir(PATH)  #we check that our current directory has our data-set !ls  Epoch-6.model\tTutorial-Bert-emotional-analysis.ipynb smile-annotations-final.csv  We then import necessary libraries\nimport torch #the pytorch library, used for modeling and formatting our data to be compatible in a pytorch environment import pandas as pd #for dataframe reading, cleaning functions from tqdm.notebook import tqdm #used as a progress bar  Before reading our data in, let\u0026rsquo;s check what it looks like \u0026ndash; i.e. if it has a header, what its columns are, etc. We do this with a terminal command below\n#inspect first 5 entires !head -n 5 smile-annotations-final.csv  611857364396965889,\u0026quot;@aandraous @britishmuseum @AndrewsAntonio Merci pour le partage! @openwinemap\u0026quot;,nocode 614484565059596288,\u0026quot;Dorian Gray with Rainbow Scarf #LoveWins (from @britishmuseum http://t.co/Q4XSwL0esu) http://t.co/h0evbTBWRq\u0026quot;,happy 614746522043973632,\u0026quot;@SelectShowcase @Tate_StIves ... Replace with your wish which the artist uses in next installation! It was entralling!\u0026quot;,happy 614877582664835073,\u0026quot;@Sofabsports thank you for following me back. Great to hear from a diverse \u0026amp;amp; interesting panel #DefeatingDepression @RAMMuseum\u0026quot;,happy 611932373039644672,\u0026quot;@britishmuseum @TudorHistory What a beautiful jewel / portrait. Is the 'R' for Rex ?\u0026quot;,happy  We read the data in, and specify colum names since it doesnt have any. We also index the rows by id.\ndf_train = pd.read_csv('smile-annotations-final.csv', names=['id', 'entry', 'emotion'], index_col='id')  #inspecting the above process df_train.head()   .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; }  \n  entry emotion   id       611857364396965889 @aandraous @britishmuseum @AndrewsAntonio Merc... nocode   614484565059596288 Dorian Gray with Rainbow Scarf #LoveWins (from... happy   614746522043973632 @SelectShowcase @Tate_StIves ... Replace with ... happy   614877582664835073 @Sofabsports thank you for following me back. ... happy   611932373039644672 @britishmuseum @TudorHistory What a beautiful ... happy     #inspecting the dimensions of our data df_train.shape  (3085, 2)  Our data-set has 2 columns, one for the actual tweet (i.e. the entry column), and one for the label (i.e. the emotion column). Let\u0026rsquo;s see the different values for emotion our data-set has\ndf_train['emotion'].unique()  array(['nocode', 'happy', 'not-relevant', 'angry', 'disgust|angry', 'disgust', 'happy|surprise', 'sad', 'surprise', 'happy|sad', 'sad|disgust', 'sad|angry', 'sad|disgust|angry'], dtype=object)  We have single emotions, a combination of emotions, and two categories irrelevant for our purposes (i.e. the nocode and not relevant category, where the tweet\u0026rsquo;s emotion category was unclear). We also see that our data-set is highly imbalanced, with some categories having thousands of examples, whilst others (ie.. the disgust category) having less than 10. Our modeling approach must take this imbalance into account later on.\ndf_train['emotion'].value_counts()  nocode 1572 happy 1137 not-relevant 214 angry 57 surprise 35 sad 32 happy|surprise 11 happy|sad 9 disgust|angry 7 disgust 6 sad|angry 2 sad|disgust 2 sad|disgust|angry 1 Name: emotion, dtype: int64  Task 3: Pre-processing Before fine-tuning BERT onto our data-set, we perform very minimal pre-processing on our tweets. The pre-processing steps we undertake are outlined below. While machine learning methods benefit from a lot of pre-processing, there might be a loss of accuracy with extensive pre-processing prior to modeling with deep learning. This is due to deep learning being very effective at dealing with raw text, and with deep learning models being trained on text collections with a lot of noise/error.\nMore importantly, for us, BERT was trained on Wikipedia (that’s about 2,500 million words) and a book corpus (800 million words). Imaginably, these sources were proof-read and edited and likely use more formal language, low in errors.\nSince we\u0026rsquo;re fine-tuning on a relatively informal and error-laden data-set of tweets, we pre-process to come as close as possible to the corpus BERT was trained on.\nWe also remove categories that have more than one emotion-label (i.e. happy|surprised), as that is a multi-label classification problem. We focus on multi-class, single-label classification in this notebook.\n NB: Multi-class refers to more than 2 classes for a target. A target with exclusively 2 classes is termed \u0026lsquo;binary\u0026rsquo; instead. Multi-label refers to having more than 1 label per class  Our pre-processing steps involve:\n Contractions Mapping Punctuation Removal @sign, URL, excess whitespace, HTML tag removal Correcting accented characters Emoji replacement Removal of multi-label categories  We first have to install the contractions library with: !pip install contractions if we don\u0026rsquo;t have it yet\n Contractions Mapping - we expand out contractions, so words like y\u0026rsquo;all, should\u0026rsquo;ve will be converted to \u0026lsquo;you all\u0026rsquo; and \u0026lsquo;should have\u0026rsquo;  import contractions  contractions.fix(\u0026quot;im hungry and its cold yall\u0026quot;)  'I am hungry and its cold you all'  #expanding out contractions df_train['entry'] = df_train['entry'].apply(lambda entry: contractions.fix(entry))  2.), 3.) and 4.) above are then done as follows, with self explanatory function names. We don\u0026rsquo;t remove \u0026lsquo;!', \u0026lsquo;?\u0026rsquo; and \u0026lsquo;.\u0026rsquo; completely as these contribute to the tone/meaning of a sentence, but do remove their duplicates.\nfrom bs4 import BeautifulSoup # a library for parsing HTML import string import unicodedata import re  # remove HTML tags def strip_html_tags(text): soup = BeautifulSoup(text, \u0026quot;html.parser\u0026quot;) return soup.get_text().replace(\u0026quot;\\n\u0026quot;, \u0026quot;\u0026quot;)  # we then apply the function for removing HTML Tags df_train['entry'] = df_train['entry'].apply(strip_html_tags)  # normalise accented characters i.e. convert à to a def remove_accented_chars(text): text = unicodedata.normalize('NFKD', text).encode('ascii', 'ignore').decode('utf-8','ignore') return text  df_train['entry'] = df_train['entry'].apply(remove_accented_chars)  #remove @name mentions and urls in a tweet def remove_mentions_and_urls(text): text = re.sub('(@[A-Za-z0-9]+)|(\\w+:\\/\\/\\S+)|(www.[A-Za-z0-9]+.[A-Za-z0-9]+)',' ', text) return text  df_train['entry'] = df_train['entry'].apply(remove_mentions_and_urls)  #remove punctuations except '?' and '!' and '.' def remove_punctuation(text): text = re.sub(r'[\\'\\\u0026quot;\\\\\\/\\,#]', '', text) text = re.sub(r'[^\\w\\s\\?\\!\\.]', ' ', text) return text  df_train['entry'] = df_train['entry'].apply(remove_punctuation)  #remove multiple '.', keep just one def remove_excess_fullstops(text): text = re.sub(r'\\.{2,}', '.', text) return text  df_train['entry'] = df_train['entry'].apply(remove_excess_fullstops)  #remove excess and trailing/leading whitespace def remove_excess_whitespace(text): text = re.sub(r'\\s{2,}', ' ', text).strip() return text  df_train['entry'] = df_train['entry'].apply(remove_excess_whitespace)  We look at 10 pre-processed entries:\ndf_train.entry.sample(10)  id 615468630256566272 that is great thank you very much! 612763483654918144 Gold brooch of Helios or sun god from the work... 613687848303206400 MT Feast Day of JohntheBaptist. Explore his li... 613986003922092032 Off to for definingbeauty wanted to go for age... 611487712521121792 Pick up a copy of the beautiful Catalogue of T... 613241383470690304 no worries. Good to hear we might see you on 4... 614964929473421312 They all gazed at him as if he a statue. Plato... 610505107084570624 Reviewed Leonora Carringtons visit to for here... 611838280888385536 _MADRE e battle sia 611625177944862721 Ooo we bet this was good ! presentfilms Name: entry, dtype: object  #we remove 'nocode' and 'not relevant' categories, as these don't indicate emotion df_train.drop(df_train[(df_train.emotion == 'nocode') | (df_train.emotion == 'not-relevant')].index, inplace=True)  #we remove categories with a '|' in them, our multi-label categories df_train.drop(df_train[df_train.emotion.str.contains('\\\\|')].index, inplace=True)  We then see below how many examples we have for each emotion category. We confirm that we have a highly imbalanced dataset (i.e. the largest category has 1137 entries, while the smallest has only 6)\ndf_train.emotion.value_counts()  happy 1137 angry 57 surprise 35 sad 32 disgust 6 Name: emotion, dtype: int64  Task 4: Training/Validation Split After pre-processing, our data-set is ready to be split into training and validation data-sets. As our data-set is imbalanced, we\u0026rsquo;ll do stratified sampling. Say we want a split of 85% training and 15% validation data\u0026ndash;this sampling technique ensures that the split happens within each category as opposed to considering all categories as a collective.\nWithout the stratification, we might oversample categories with a large number of examples, and completely exclude categories with a low number of examples (i.e. the disgust category with 6 examples might be left out of training). With the stratification, we split the digust category, for example, 85-15 as well (i.e. 5 training examples, 1 validation example).\n#importing modules for splitting the data-set from sklearn.model_selection import train_test_split  #now we create a list of unique emotion labels possible_labels = df_train['emotion'].unique()  BERT specifically requires that labels passed into it are converted to numbers, hence we do this in the below step.\n#we convert those labels to numbers, for use in our algorithm later on label_dict = {} for index, possible_label in enumerate(possible_labels): label_dict[possible_label] = index  label_dict  {'angry': 1, 'disgust': 2, 'happy': 0, 'sad': 3, 'surprise': 4}  #We add a new column to our original data-frame, of numbers corresponding to each emotion label df_train['label'] = [label_dict[str_label] for str_label in df_train['emotion']]  df_train.sample(5)   .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; }  \n  entry emotion label   id        615150243592728576 Two very different but equally beautiful events happy 0   611115939082424321 F is for Funky Fiddle Leaf but also for the fa... happy 0   611980866865242116 Thank you for fascinating talk on art and syna... happy 0   613086766199894016 The Cantabridgia Daili is out! Stories via _Ca... happy 0   615120985738637312 Great Baramundi fish with Clive Loveless. Abor... happy 0     We then split our data into training and validation sets. Validation sets are useful for detecting overfitting in our classifier later on. We set a random state of 17 so that anyone who wants to reproduce this notebook can get the exact same results as us.\nx_train, x_val, y_train, y_val = train_test_split( df_train.index.values, df_train['label'], test_size = 0.20, #let's do 85-15 train-validation split random_state=17, #reproducible between my instance and whoever wants to reproduce stratify= df_train['label'].values #the command for stratification )  We then check whether the split successfully produced a stratified 85-15 split within each emotion category\n#create a dummy column housing data types - either train/val later on df_train['data_type'] = ['not_set']*df_train.shape[0]  #if id of sample exists in x_train, make it 'train', otherwise existing in x_val, make it 'val' df_train.loc[x_train, 'data_type'] = 'train' df_train.loc[x_val, 'data_type'] = 'val'  #check stratification of training and validation data-sets df_train.groupby(['emotion', 'data_type'])['entry'].count()  emotion data_type angry train 45 val 12 disgust train 5 val 1 happy train 909 val 228 sad train 26 val 6 surprise train 28 val 7 Name: entry, dtype: int64  We see that the stratification worked!\nNow that we have our training and validation data-sets, we still have to convert them to a format that BERT accepts.\nThere\u0026rsquo;s a lot that\u0026rsquo;s been written on this required format, so we\u0026rsquo;ll just briefly mention them here.\nIn order to fine-tune with the pre-trained BERT model, we need to use its tokeniser. This is because 1.) BERT has a specific, fixed vocabulary and 2.) BERT has a particular way of handling words outside this vocabulary.\nIn addition, we need to add special tokens to the start and end of each sentence, pad and truncate all sentences to a fixed length, and specify which parts of the sentences are padded with an \u0026lsquo;attention mask\u0026rsquo;\nLuckily, the HuggingFace implementation of BERT has a method we can call on the BERT tokeniser, named encode_plus, that does all of the above for us.\nThe encode_plus method of BERT\u0026rsquo;s tokenizer will:\n split our text into tokens, add the special [CLS] and [SEP] tokens, and convert these tokens into indexes of the tokenizer vocabulary, pad or truncate sentences to a specified max length, and create an attention mask.  Task 5: Loading Tokenizer and Encoding our Data We first install the transformers library from HuggingFace to get access to the BERTokenizer and our eventual BERTForSequenceClassification model, the one that\u0026rsquo;s fine-tuned for classification tasks. We can do this by running the command:\n!pip install transformers==3.0.0\nWe also import functionality from PyTorch for creating a TensorDataSet, which is a multi-dimensional tensor data-structure that\u0026rsquo;s used heavily in a PyTorch environment.\nfrom transformers import BertTokenizer from torch.utils.data import TensorDataset #setting up our dataset so it's usable in a pytorch environment  #set up a tokenizer object, using pre-trained BERT's own tokenizer tokenizer = BertTokenizer.from_pretrained( 'bert-base-uncased', #we ask the tokenizer to lowercase our sentences do_lower_case=True )  HBox(children=(FloatProgress(value=0.0, description='Downloading', max=231508.0, style=ProgressStyle(descripti…  Recall that we have to pad sentences to a specified length. We first figure out the maximum length of all of our tokenised sentences in the whole data-set via the code block below. We then pass in that max length value to encode_plus, who will handle padding each sentence to that length for us.\n#getting the maximum tokenised length out of tweets in our training data-set max_len = 0 # For every sentence... for sent in df_train['entry']: # Tokenize the text and add `[CLS]` and `[SEP]` tokens. input_ids = tokenizer.encode(sent, add_special_tokens=True) # Update the maximum sentence length. max_len = max(max_len, len(input_ids)) print('Max sentence length: ', max_len)  Max sentence length: 36  #encode our training and validation data-sets with the tokenizer above encoded_data_train = tokenizer.batch_encode_plus( #change below to appropriate setup df_train.entry.values, add_special_tokens=True, #add the CLS and SEP tokens truncation=True, return_attention_mask=True, pad_to_max_length=True, max_length=max_len, return_tensors='pt' #returns pytorch tensor ) encoded_data_val = tokenizer.batch_encode_plus( #change below to appropriate setup df_train[df_train.data_type=='val'].entry.values, add_special_tokens=True, #adds the CLS and SEP tokens truncation=True, return_attention_mask=True, pad_to_max_length=True, max_length=max_len, return_tensors='pt' ) #encoding process above returns dictionaries. We grab input ID tokens, attention mask, and labels from this input_ids_train = encoded_data_train['input_ids'] #return each sentence as a # attention_masks_train = encoded_data_train['attention_mask'] #returns a pytorch tensor #change below to appropriate setup, resampled or not labels_train = torch.tensor(df_train.label.values) input_ids_val = encoded_data_val['input_ids'] #return each sentence as a # attention_masks_val = encoded_data_val['attention_mask'] #returns a pytorch tensor labels_val = torch.tensor(df_train[df_train.data_type=='val'].label.values)  #we construct a tensor dataset from input ID tokens, attention mask, and labels dataset_train = TensorDataset(input_ids_train, attention_masks_train, labels_train) dataset_validation = TensorDataset(input_ids_val, attention_masks_val, labels_val)  Task 6: Setting Up BERT Pre-Trained Model Now onto our fine-tuning step!\nThe term \u0026lsquo;fine-tuning\u0026rsquo; is generally interchangeable with the term \u0026lsquo;transfer-learning\u0026rsquo;.\nTo perform this step, a deep learning model is \u0026lsquo;chopped off\u0026rsquo; at one of its later layers, with the subsequent layers being replaced by a classifier.\nThe intuition behind fine-tuning, as so eloquently put in this Tensorflow Documentation page is \u0026lsquo;that if a model is trained on a large and general enough dataset, this model will effectively serve as a generic model of the visual world. You can then take advantage of these learned feature maps without having to start from scratch by training a large model on a large dataset.\u0026rsquo;\nHuggingFace has implemented a model called BERTForSequenceClassification that has a sequence classification/regression head on top (i.e. a linear layer on top of the pooled output) of the BERT deep learning model. This is what we\u0026rsquo;ll use for our fine-tuning.\nfrom transformers import BertForSequenceClassification  We import this model. Below is how we initialise the fine-tuning step. We add another layer on top of it of 6 nodes (i.e. one corresponding to each emotion category).\n#each tweet is its own sequence, which will be classified into one of 6 classes model = BertForSequenceClassification.from_pretrained( 'bert-base-uncased', num_labels = len(label_dict), output_attentions = False, #dont need attention mask output_hidden_states = False #last layer before output ) pass  HBox(children=(FloatProgress(value=0.0, description='Downloading', max=433.0, style=ProgressStyle(description_… HBox(children=(FloatProgress(value=0.0, description='Downloading', max=440473133.0, style=ProgressStyle(descri… Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias'] - This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model). - This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model). Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias'] You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.  Task 7: Creating Data Loaders A DataLoader is an iterable data structure within PyTorch that contains a collection of TensorDataSets. We iterate through a DataLoader either sequentially, from the first example to the last, or randomly.\nThe former is a useful format for our validation data-set, as we\u0026rsquo;d want to tie the predictions in our validation data-set back to the original dataframe (and hence we want the ordering to be preserved). The latter is a useful format for our training data-set, as we prevent biasing the training of our mdodel when we randomly sample our batches.\n#we import our DataLoader and Samplers from torch.utils.data import DataLoader, RandomSampler, SequentialSampler  The batch size that we allow per iteration will affect how much compute power is used when fine-tuning on our data. Larger batch-sizes are more suitable for more powerful hardware (i.e. GPUs) whilst smaller batch-sizes are better for CPUs, for example.\nAs we\u0026rsquo;re using a Google Colab GPU, we can use a batch size of 16.\nThe BERT Github page/publication has information on other Batch Sizes and Learning Rates to go with it.\nIf you want to use a GPU yourself, in your Google Colab notebook, navigate to Runtime --\u0026gt; Change Runtime Type --\u0026gt; GPU\nbatch_size = 16 dataloader_train = DataLoader( dataset_train, sampler=RandomSampler(dataset_train), #Randomly train on data, so we don't bias training batch_size=batch_size ) dataloader_val = DataLoader( dataset_validation, sampler=SequentialSampler(dataset_validation), #Sequential sampling on validation data so we can tie results to original dataframe batch_size=batch_size )  Task 8: Setting Up Optimizer and Scheduler Optimisers are a set of algorithms responsible for changing attributes of a neural network such as its weights and learning rates in order to reduce loss. They tune hyperparameters per epoch.\nWe use the AdamW optimiser and a learning rate of 1e-5. Again, the BERT paper recommends a set of learning rates and finding the best one for your particular fine-tuning task is a matter of trial and error.\nIn our scheduler, we can specify whether we\u0026rsquo;d want to include warm-up steps, as well as the number of total training steps we\u0026rsquo;re undertaking. Since an epoch is one full pass over the entire training data-set, our value for training steps is the length of a batch * the number of epochs.\nfrom transformers import AdamW, get_linear_schedule_with_warmup  optimizer = AdamW( model.parameters(), lr=1e-5, #2e-5 \u0026gt; 5e-5: A HYPERPARAMETER eps=1e-8 )  epochs=6 scheduler = get_linear_schedule_with_warmup( optimizer, num_warmup_steps=0, num_training_steps=len(dataloader_train) * epochs )  Task 9: Defining our Performance Metrics Prior to our training pass, let\u0026rsquo;s first define useful helper functions to calculate certain metrics like multi-class accuracy and F1 score.\nWe also include a softmax() function to normalise the predictions generated later, and an emotion_prediction() function to convert the normalised predictions to an emotion category.\nThe accuracy metric was modified from this tutorial .\nimport numpy as np from sklearn.metrics import f1_score  def f1_score_func(preds, labels): \u0026quot;\u0026quot;\u0026quot; Helper function for calculating F1-score between predicted and true values \u0026quot;\u0026quot;\u0026quot; preds_flat = np.argmax(preds, axis=1).flatten() #why flatten? we dont want a list of lists, we just want a single array return f1_score(labels, preds_flat, average='weighted')#weights classes according to its distribution. disgust with 6 classes is downweighted #weighted vs macro  def accuracy_per_class(preds, labels): \u0026quot;\u0026quot;\u0026quot; Helper function for calculating the accuracy per class and displaying it Modified for sentiment Analysis. Not using emotion analysis code \u0026quot;\u0026quot;\u0026quot; preds_flat = np.argmax(preds, axis=1).flatten() for label in np.unique(labels): y_preds = preds_flat[labels==label] y_true = labels[labels==label] print(f'Class: {label_dict_inverse[label]}') print(f'Accuracy: {len(y_preds[y_preds==label])}/{len(y_true[y_true==label])} in percentage: {len(y_preds[y_preds==label])/len(y_true[y_true==label])}')  def softmax(matrix): \u0026quot;\u0026quot;\u0026quot; A function to normalise row values of a matrix to 1.0 @param matrix - a numpy matrix which has non-normalised values per row @returns - the matrix with values all normalised to 1.0 \u0026quot;\u0026quot;\u0026quot; return (np.exp(matrix.T) / np.sum(np.exp(matrix), axis=1)).T  def emotion_prediction(normalised_matrix): \u0026quot;\u0026quot;\u0026quot; A function to grab the dominant class (i.e. the prediction) @param normalised_matrix - a numpy matrix, which has normalised values per row, achieved from applying an activation function \u0026quot;\u0026quot;\u0026quot; return np.argmax(normalised_matrix, axis=1).flatten()  We provide a dictionary that maps the raw prediction outputs, which would be numbers, to the emotion categories in words.\nlabel_dict_inverse = {v:k for (k,v) in label_dict.items()}  Task 10: Creating our Training and Evaluation Loops Our Approach for training was adapted from an older version of HuggingFace\u0026rsquo;s run_glue.py script accessible here and recommended by the HuggingFace team.\n#again we set a seed value of 17 to make our training loop reproducible import random seed_val = 17 #so our results/process is reproducible by whoever wants to reproduce random.seed(seed_val) np.random.seed(seed_val) torch.manual_seed(seed_val) torch.cuda.manual_seed_all(seed_val) #include for when using a GPU  As PyTorch code is GPU compatible, we have to explicitly specify what device type we\u0026rsquo;re working from.\nWe eventually have to transfer our model and data-structures onto this device type later on.\n#to check GPU vs CPU device = torch.device('cuda' if torch.cuda.is_available() else 'cpu') model.to(device) print(device)  cuda  We then write our code for evaluating the validation data-set in the evaluate() function below before we write the code for our training data-set. This is because we use evaluate() in our training loop.\nevaluate() and our training loop right below it have very similar structures. We point out the differences later on.\n#quite similar to training, except for the differences mentioned below def evaluate(dataloader_val): model.eval() loss_val_total = 0 predictions, true_vals = [], [] for batch in tqdm(dataloader_val): batch = tuple(b.to(device) for b in batch) inputs = {'input_ids': batch[0], 'attention_mask': batch[1], 'labels': batch[2], } #ignore/disable gradients with torch.no_grad(): outputs = model(**inputs) loss = outputs[0] logits = outputs[1] loss_val_total += loss.item() #detach from CPU means pulling values out of GPU to CPU #so we can use numpy, which works only on CPU logits = logits.detach().cpu().numpy() label_ids = inputs['labels'].cpu().numpy() predictions.append(logits) true_vals.append(label_ids) loss_val_avg = loss_val_total/len(dataloader_val) predictions = np.concatenate(predictions, axis=0) true_vals = np.concatenate(true_vals, axis=0) return loss_val_avg, predictions, true_vals  We wrap our training loop below into a tqdm() object to display a progress bar\n#our training loop! for epoch in tqdm(range(1, epochs+1)): model.train() #set to 0 initially, then add each batch's loss iteratively loss_train_total = 0 progress_bar = tqdm(dataloader_train, desc='Epoch {:1d}'.format(epoch), leave=False, #let it overwrite after each epoch disable=False, ) for batch in progress_bar: #first batch = set gradients to 0 model.zero_grad() #dataloader has 3 variables. so it's going to be a tuple of 3 items. We make sure each item is on the correct device batch = tuple(b.to(device) for b in batch) inputs = { 'input_ids' : batch[0], 'attention_mask' : batch[1], 'labels' : batch[2] } #unpacks dictionary straight into model outputs = model(**inputs) #bert model returns loss and logits loss = outputs[0] loss_train_total += loss.item() #add up loss loss.backward() #backpropagate #all weights will be a norm of 1 (normalised weights) torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0) optimizer.step() scheduler.step() #update progress bar to display loss per batch progress_bar.set_postfix({'training_loss' : '{:3f}'.format(loss.item()/len(batch))}) #outside the batch loop and inside the epoch loop, so per epoch #save model checkpoint and print progress torch.save(model.state_dict(), f'Epoch-{epoch}.model') tqdm.write(f'\\nEpoch {epoch}') loss_train_avg = loss_train_total/len(dataloader_train) #loss per epoch: tqdm.write(f'Training loss: {loss_train_avg}') #to detect overtraining - happens when training loss goes down and val loss goes up. Starts to #train perfectly on our data such that its no longer generalisable val_loss, predictions, true_vals = evaluate(dataloader_val) #predictions are the logits val_f1 = f1_score_func(predictions, true_vals) tqdm.write(f'Validation loss: {val_loss}') tqdm.write(f'F1 Score (weighted): {val_f1}')  HBox(children=(FloatProgress(value=0.0, max=6.0), HTML(value=''))) HBox(children=(FloatProgress(value=0.0, description='Epoch 1', max=159.0, style=ProgressStyle(description_widt… Epoch 1 Training loss: 0.5584828776524127 HBox(children=(FloatProgress(value=0.0, max=32.0), HTML(value=''))) Validation loss: 0.2766137867583893 F1 Score (weighted): 0.8965437215084957 HBox(children=(FloatProgress(value=0.0, description='Epoch 2', max=159.0, style=ProgressStyle(description_widt… Epoch 2 Training loss: 0.24347842407980994 HBox(children=(FloatProgress(value=0.0, max=32.0), HTML(value=''))) Validation loss: 0.1528040450939443 F1 Score (weighted): 0.9427103377445136 HBox(children=(FloatProgress(value=0.0, description='Epoch 3', max=159.0, style=ProgressStyle(description_widt… Epoch 3 Training loss: 0.1628657704359799 HBox(children=(FloatProgress(value=0.0, max=32.0), HTML(value=''))) Validation loss: 0.07538615397061221 F1 Score (weighted): 0.9586534476712678 HBox(children=(FloatProgress(value=0.0, description='Epoch 4', max=159.0, style=ProgressStyle(description_widt… Epoch 4 Training loss: 0.10585488408142056 HBox(children=(FloatProgress(value=0.0, max=32.0), HTML(value=''))) Validation loss: 0.06744529563729884 F1 Score (weighted): 0.9632545931758529 HBox(children=(FloatProgress(value=0.0, description='Epoch 5', max=159.0, style=ProgressStyle(description_widt… Epoch 5 Training loss: 0.08138270038853651 HBox(children=(FloatProgress(value=0.0, max=32.0), HTML(value=''))) Validation loss: 0.048784602870000526 F1 Score (weighted): 0.9769995855781185 HBox(children=(FloatProgress(value=0.0, description='Epoch 6', max=159.0, style=ProgressStyle(description_widt… Epoch 6 Training loss: 0.0737762286301421 HBox(children=(FloatProgress(value=0.0, max=32.0), HTML(value=''))) Validation loss: 0.04574976687581511 F1 Score (weighted): 0.9864752200092636  What our training loop has that our evaluate() function doesn\u0026rsquo;t have:\n The ability to backpropagate The ability to monitor training and validation loss per epoch The ability to save a trained model as a checkpoint  Task 11: Loading and Evaluating our Model During training, we have saved our trained model parameters with a .model extension in our current directory.\nThe 6th Epoch of training lead to the highest weighted macro F1 score of 0.986! The training and validation losses in this epoch were 0.07 and 0.05, respectively.\nThis is a STAGGERING result, and highlights the power of transfer learning models.\nAs the training and validation losses are similar in magnitude and our validation loss hasn\u0026rsquo;t plateaued, we don\u0026rsquo;t seem to have overfit our model to the training data! Yay!\nNow let\u0026rsquo;s load a fresh BERT Model, load our 6th Epoch model checkpoints onto it, evaluate our validation data-set again with evaluate() then look closer at accuracies. We ignore the messages it generates.\n#fresh model model = BertForSequenceClassification.from_pretrained(\u0026quot;bert-base-uncased\u0026quot;, num_labels=len(label_dict), output_attentions=False, output_hidden_states=False)  Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias'] - This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model). - This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model). Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias'] You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.  #pass on the fresh model to the correct device, either GPU or CPU model.to(device) pass #so we dont have all that text printed out  # cuda indicates a GPU is available. Replace with 'cpu' when using a cpu. model.load_state_dict( torch.load('Epoch-6.model', map_location=torch.device('cuda')))  \u0026lt;All keys matched successfully\u0026gt;  #grabbing predictions from validation data-set _, predictions_val, labels_val = evaluate(dataloader_val)  HBox(children=(FloatProgress(value=0.0, max=16.0), HTML(value='')))  evaluate() generated a matrix of predictions for the validation data-set of with the dimensions below: 254 rows, each corresponding to a tweet in the validation data-set and 5 columns, each corresponding to an emotion category.\npredictions_val.shape  (254, 5)  #looking at the fifth example in the predictions matrix predictions_val[4]  array([ 6.018326 , -1.8091979, -2.1678815, -1.2804692, -2.0098157], dtype=float32)  Looking at the fifth example in the prediction matrix, we see that the values for each of the 5 columns aren\u0026rsquo;t normalised to 1.0. Let\u0026rsquo;s normalise it with an activation function, the softmax The results of this normalisation for each category is the probability that the sentence belongs to that category.\n#grab predictions variable here and do a softmax, to visualise results against df percent_emotions_val = softmax(predictions_val)  As we see below, the fifth example\u0026rsquo;s most probable category is the first column (whatever it is). The probability that it belongs to the category represented by this column is 99%!\npercent_emotions_val[4]  array([9.9832332e-01, 3.9794299e-04, 2.7800113e-04, 6.7521929e-04, 3.2560693e-04], dtype=float32)  We then determine the actual emotion category by running it through one final function, which takes the softmax-ed predictions matrix and grabs the most probable emotion label:\n#from soft-maxed probabilities of emotions to picking the most dominant emotion emotions_val = emotion_prediction(percent_emotions_val)  The output is naturally a number (as we trained it with numbers!), so we pass in the inverse dictionary that maps the number to its string category. We see that the first tweet was predicted as a happy one:\nlabel_dict_inverse[emotions_val[4]]  'happy'  df_train   .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; }  \n  entry emotion label data_type   id         614484565059596288 Dorian Gray with Rainbow Scarf LoveWins from happy 0 val   614746522043973632 _StIves . Replace with your wish which the art... happy 0 val   614877582664835073 thank you for following me back. Great to hear... happy 0 train   611932373039644672 What a beautiful jewel portrait. Is the R for ... happy 0 train   611570404268883969 I have always loved this painting. happy 0 train   ... ... ... ... ...   614053885733412864 Good to see s art collection Thx to _StIves _r... happy 0 train   610405281604993024 thanks we will have a look next week after Fri... happy 0 train   612214539468279808 Thanks for ranking us 1 in things to do in Lon... happy 0 train   613678555935973376 MT Looking forward to our public engagement ev... happy 0 train   615246897670922240 Mesmerising. happy 0 train    1267 rows × 4 columns\n Let\u0026rsquo;s have a look at the actual tweet and it\u0026rsquo;s actual label though:\nprint(f\u0026quot;Tweet: {df_train[df_train.data_type=='val'].iloc[4,0]}\\noriginal label: {df_train[df_train.data_type=='val'].iloc[4,1]}\\npredicted label: {label_dict_inverse[emotions_val[4]]}\u0026quot;)  Tweet: Wonderful experience hearing Tim Knoxs objects2015 keynote on contents decor of UK country houses in gallery 3 of _UK! original label: happy predicted label: happy  Let\u0026rsquo;s do the above process for all tweets in our validation data-set, and officially calculate accuracy metrics for each emotion category\n#calculating accuracy per class print('Accuracy per class of val dataset:\\n') accuracy_per_class(predictions_val, labels_val)  Accuracy per class of val dataset: Class: happy Accuracy: 228/228 in percentage: 1.0 Class: angry Accuracy: 12/12 in percentage: 1.0 Class: disgust Accuracy: 0/1 in percentage: 0.0 Class: sad Accuracy: 4/6 in percentage: 0.6666666666666666 Class: surprise Accuracy: 7/7 in percentage: 1.0  We see above that the model performed badly for the disgust category, miss-classifying the only disgust entry in the validation data-set as something else. The rest of the categories were predicted accurately, with stunning accuracy scores ranging from 60% to 100%!\nWe then look at the macro-weighted F1 score over our entire validation data-set. This should match the output of our training loop above (i.e. Epoch 6\u0026rsquo;s validation data loss should be 0.98)\n#f1 score overall print('Weighted F1 score of val dataset:') print(f1_score_func(predictions_val, labels_val))  Weighted F1 score of val dataset: 0.9864752200092636  Overall, a very pleasing result :)\nAs we saw, we achieved highly accurate results leveraging the knowledge of generalist BERT for our specialist task of classifying emotions in tweets. We fine-tuned on a single GPU in less than 30 minutes to achieve 98.6% accuracy, a very impressive feat that\u0026rsquo;s impossible without transformers!\nClosing Remarks:\n  If we wanted to improve the accuracy of the sad/disgust categories, we can consider resampling categories so the numbers of all categories are more balanced\n  The following links helped greatly in building this notebook:\n  Jay Alammar\u0026rsquo;s illustration-based explanation of the BERT architecture   Jay Alammar\u0026rsquo;s visual exploration of BERT embeddings   Chris McCormick\u0026rsquo;s BERT-Fine Tuning with PyTorch     Stay tuned for further posts on how I use BERT for topic modeling, building a chatbot, and deploying a BERT model onto the cloud!\n  ","date":1586908800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1586908800,"objectID":"a2a0f2a8bc864312c9770bc4dd3e3b1f","permalink":"https://nixramirez.github.io/project/bert-emotion-classification/","publishdate":"2020-04-15T00:00:00Z","relpermalink":"/project/bert-emotion-classification/","section":"project","summary":"Achieving 98.6% accuracy, I implement transfer-learning using BERT to classify emotions in tweets","tags":["Data Projects"],"title":"Multi-Class Emotion Classification with Deep Learning using BERT","type":"project"},{"authors":null,"categories":null,"content":"The Idea An ex-colleague, Dhilip and I wanted to do a series of end-to-end projects in Data Science and Analytics for fun and to pick up skills along the way that would serve our future careers in this space. When Dhilip approached me for project ideas, I already had one I\u0026rsquo;ve thought about for quite some time.\nIt was a timely project for us Qrious interns, who were deliberating on our next career move following the summer internship. Towards the end of our internship, a few of us (excluding me but including Dhilip) were graduating and applying for full-time jobs in the data space. Our discussions naturally gravitated to what starting pay we could expect for careers in data science, data engineering, and analytics. I imagined if we had an amount for each job averaged from recent, accurate salary data collected over a large number of companies operating in different industries, we had evidence to make sure future salary negotiations were fair!\nThe Execution Glassdoor is a job reviews site with job benefits and salary information reported anonymously by employees of various companies. It was our main data source for this project. Collecting a vast amount of data from the site would require automation, using a technique called scraping. This is a technique used to extract content from specific HTML tags in a webpage, and in our case, exploits two technologies to do so: Selenium and BeautifulSoup.\nSelenium is a Java-based tool used in website testing to automate specific interactions with webpages (i.e. clicking on links, logging in, navigating the page, etc). Since it automates interacting with the DOM, it\u0026rsquo;s being used in Data Science to scrape data from websites. BeautifulSoup, on the other hand, is a Python library that parses HTML and makes it easy to extract specific elements from it.\nThe code 1. First, we import the necessary libraries: Python Selenium libraries (Scraping)\n webdriver - contains tools for working with an automated web browser webdriver.chrome - specifies Google Chrome as our automated browser time - the sleep function allows our automated tasks (i.e. clicking on links) to have a delay. Important for not being blocked as a bot by some websites BeautifulSoup - a python library for parsing HTML pages and grabbing content from specific HTML tags easily lxml - a parser for BeautifulSoup, allowing the HTML pages to be parsed as xml (and queried with xPath)  Standard Python Libraries\n pandas - a library for working with data-frames csv - a tool for reading and writing CSV files itertools - an iterator library  from selenium import webdriver from selenium.webdriver.chrome.options import Options from webdriver_manager.chrome import ChromeDriverManager from time import sleep from bs4 import BeautifulSoup import lxml import pandas as pd import csv from itertools import zip_longest  2. Then, we set up an automated browser for scraping In this step, we navigate to Glassdoor\u0026rsquo;s Salaries page and type in whatever Job Title we\u0026rsquo;d want salaries for, in whatever location. In this example, we\u0026rsquo;d want to grab Data Engineers\u0026rsquo; salary information from the United States. We have to be logged into a Glassdoor account to do this.\nWe then copy paste the URL of the resulting page into the driver.get(url) method below.\nNext, we find the last page of results Glassdoor has for this particular search. This is important as we\u0026rsquo;re setting up our browser to automatically cycle through all pages of the search, grabbing salary information from each. We do this by modifying the URL in our browser, adding the string _IP1500 just before the .htm. This lets us jump to the very last page of salary information, as it is likely that there won\u0026rsquo;t be 1500 pages worth of search results. If there is, just adjust the IP number to be larger.\nOnce we\u0026rsquo;ve jumped to the last page of results, note down what that page is (by looking at the last number in the carousel button as such (red in the image below):\nand copying that number onto the lastPageNo variable in the below code. In below\u0026rsquo;s example, page 191 is the last page, and the for loop cycles through each page until it reaches that, employing a delay of at least 1.5s before it goes on to the next page.\n#creating empty arrays to hold job title, company name, job mean pay and pay range information job_title = [] company_name = [] mean_pay = [] pay_range = [] lastPageNo = 191; #going through 184 pages of salary information for pageno in range(1,lastPageNo): driver = webdriver.Chrome(ChromeDriverManager().install()) #getting webpage in glassdoor if pageno == 1: driver.get(\u0026quot;https://www.glassdoor.co.nz/Salaries/us-data-engineer-salary-SRCH_IL.0,2_IN1_KO3,16.htm\u0026quot;) else: driver.get( \u0026quot;https://www.glassdoor.co.nz/Salaries/us-data-engineer-salary-SRCH_IL.0,2_IN1_KO3,16.htm\u0026quot; + \u0026quot;_IP\u0026quot; + str(pageno) + \u0026quot;.htm\u0026quot; ) time.sleep(1.5)  3. We parse the HTML of each search result page\u0026hellip;and SCRAPE! \u0026hellip;Using Beautifulsoup. The page_source attribute of the driver grabs the page with its corresponding HTML tags, and parses it with lxml, which as mentioned above, allows us to query the results using xpath if we wished.\nAfter parsing, we then grab specific HTML content. We do this by:\n Inspecting the html tags where our information lies, by using Google Chrome\u0026rsquo;s inspect option Collecting information that would distinguish our target HTML tag/s from others  Tag Classes and IDs are useful for this. We see in the below screenshot, for example, that each salary block is enclosed by a \u0026lt;div\u0026gt; with class \u0026ldquo;row align-items-center m-0 salaryRow__SalaryRowStyle__row\u0026rdquo;. To grab each salary block from a page, we then use BeautifulSoup\u0026rsquo;s findAll() method, passing on the \u0026lt;div class=\u0026quot;\u0026quot;\u0026gt; information mentioned.\nWe do the same for every piece of information we want (i.e. job titles, company name, average salary, and salary range in this example). These bits of info were obtained the same way as above. We looped through each salary block above to grab the specific information, as the below code shows.\n#continuation from code above (still inside the for loop) #parsing the page through lxml option of beautifulsoup html = driver.page_source soup = BeautifulSoup(html, 'lxml') #getting each salary block salaryBlocks = soup.findAll(\u0026quot;div\u0026quot;, {'class' : 'row align-items-center m-0 salaryRow__SalaryRowStyle__row'}) #for each salary block, find the job title, company name, average pay, and pay range, and append them to the lists initialised above for block in salaryBlocks: entry = [] jobTitle = block.find(\u0026quot;div\u0026quot;, {'class' : 'salaryRow__JobInfoStyle__jobTitle strong'}).find(\u0026quot;a\u0026quot;).text job_title.append(jobTitle) companyName = block.find(\u0026quot;div\u0026quot;, {'class' : 'salaryRow__JobInfoStyle__employerName'}).text company_name.append(companyName) meanPay = block.find(\u0026quot;div\u0026quot;, {'class' : 'salaryRow__JobInfoStyle__meanBasePay common__formFactorHelpers__showHH'}).find('span').text mean_pay.append(meanPay) #if a pay range exists, grab it, otherwise, indicate none exists try: if block.find(\u0026quot;div\u0026quot;, {'class' : 'col-2 d-none d-md-block px-0 py salaryRow__SalaryRowStyle__amt'}).find(\u0026quot;div\u0026quot;, {'class' : 'strong'}): payRange = block.find(\u0026quot;div\u0026quot;, {'class' : 'col-2 d-none d-md-block px-0 py salaryRow__SalaryRowStyle__amt'}).find(\u0026quot;div\u0026quot;, {'class' : 'strong'}).text pay_range.append(payRange) elif block.find(\u0026quot;div\u0026quot;, {'class' : 'col-2 d-none d-md-block px-0 py salaryRow__SalaryRowStyle__amt'}).find(\u0026quot;span\u0026quot;, {'class' : 'strong'}): pay_range.append(\u0026quot;N/A\u0026quot;) except: pay_range.append(\u0026quot;N/A\u0026quot;) driver.quit()  4. We save the results to a .csv file Once we\u0026rsquo;ve obtained all the information we need, we store them into a python data-frame, which allows us to store the data in a tabular format. The columns of the table correspond to job title, company name, mean pay, and the pay range, whilst the rows are individual companies.\nThe below code shows how the results are stored in a data-frame and eventually converted to a .csv file for easy reading into your favourite data analysis program/language later on.\n#process the lists into a final dataframe, and save to a CSV final = [] for item in zip_longest(job_title, company_name, mean_pay, pay_range): final.append(item) df = pd.DataFrame( final, columns=['jobTitle', 'companyName', 'meanPay', 'payRange']) df.to_csv(\u0026quot;Data Engineer Salaries United States.csv\u0026quot;)  The final output of this scraping is a 28,000 row file, containing salary information for Data Engineers, Analysts, Scientists, and Machine Learning Engineers in Australia, New Zealand, and the United States. The file can be downloaded here for free :)\n","date":1582761600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1582761600,"objectID":"4b110a2d210d35fefabedf840d4417e5","permalink":"https://nixramirez.github.io/project/data-scraping/","publishdate":"2020-02-27T00:00:00Z","relpermalink":"/project/data-scraping/","section":"project","summary":"Used Selenium and Python's BeautifulSoup to scrape Salary Information from Glassdoor","tags":["Data Projects"],"title":"Web Scraping","type":"project"},{"authors":null,"categories":null,"content":"The below chart was made in Tableau and hosted in Tableau Public. It was created as part of my internship with Qrious and presented to stakeholders.\n var divElement = document.getElementById('viz1589195919608'); var vizElement = divElement.getElementsByTagName('object')[0]; if ( divElement.offsetWidth  800 ) { vizElement.style.width='1000px';vizElement.style.height='800px';} else if ( divElement.offsetWidth  500 ) { vizElement.style.width='1000px';vizElement.style.height='800px';} else { vizElement.style.width='100%';vizElement.style.height='2100px';} var scriptElement = document.createElement('script'); scriptElement.src = 'https://public.tableau.com/javascripts/api/viz_v1.js'; vizElement.parentNode.insertBefore(scriptElement, vizElement);  This chart is the output of running the topic-modeling algorithm Latent Dirichlet Allocation on a corpus of social media commentary about Spark Sports. The topic to visualise can be selected from the drop-down menu. Once selected, the chart shows the frequency of each term associated to that topic.\nEach term was taken into consideration in naming the topic.\n","date":1581638400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1581638400,"objectID":"27b900ce7e57cfbc8cb50819270b512a","permalink":"https://nixramirez.github.io/project/tableau-interactive-viz/","publishdate":"2020-02-14T00:00:00Z","relpermalink":"/project/tableau-interactive-viz/","section":"project","summary":"Made an interactive horizontal bar chart","tags":["Visualisations"],"title":"Tableau Interactive Chart","type":"project"},{"authors":null,"categories":null,"content":"The below map was made in Tableau and hosted in Tableau Public. It was created as part of my internship with Qrious and presented to stakeholders.\nIf you hover over each area unit, you\u0026rsquo;ll see more information about that area unit.\n var divElement = document.getElementById('viz1589194826274'); var vizElement = divElement.getElementsByTagName('object')[0]; vizElement.style.width='100%';vizElement.style.height=(divElement.offsetWidth*0.75)+'px'; var scriptElement = document.createElement('script'); scriptElement.src = 'https://public.tableau.com/javascripts/api/viz_v1.js'; vizElement.parentNode.insertBefore(scriptElement, vizElement);  The New Zealand map was created by using a shapefile of New Zealand Area Unit informations, obtained from Stats.NZ. The rest of the data was overlaid onto this map. The two data-sets (i.e. geospatial data + household internet access data) were combined by doing a left outer join of the former to the latter on Area Unit ID in tableau.\n","date":1579046400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1579046400,"objectID":"043342b79de5349e5b094a1282eb2319","permalink":"https://nixramirez.github.io/project/tableau-interactive-map/","publishdate":"2020-01-15T00:00:00Z","relpermalink":"/project/tableau-interactive-map/","section":"project","summary":"Made an interactive map of New Zealand Internet Access with Tableau","tags":["Visualisations"],"title":"Tableau Interactive Map","type":"project"},{"authors":null,"categories":null,"content":"Academic is designed to give technical content creators a seamless experience. You can focus on the content and Academic handles the rest.\nHighlight your code snippets, take notes on math classes, and draw diagrams from textual representation.\nOn this page, you\u0026rsquo;ll find some examples of the types of technical content that can be rendered with Academic.\nExamples Code Academic supports a Markdown extension for highlighting code syntax. You can enable this feature by toggling the highlight option in your config/_default/params.toml file.\n```python import pandas as pd data = pd.read_csv(\u0026quot;data.csv\u0026quot;) data.head() ```  renders as\nimport pandas as pd data = pd.read_csv(\u0026quot;data.csv\u0026quot;) data.head()  Math Academic supports a Markdown extension for $\\LaTeX$ math. You can enable this feature by toggling the math option in your config/_default/params.toml file.\nTo render inline or block math, wrap your LaTeX math with $...$ or $$...$$, respectively.\nExample math block:\n$$\\gamma_{n} = \\frac{ \\left | \\left (\\mathbf x_{n} - \\mathbf x_{n-1} \\right )^T \\left [\\nabla F (\\mathbf x_{n}) - \\nabla F (\\mathbf x_{n-1}) \\right ] \\right |} {\\left \\|\\nabla F(\\mathbf{x}_{n}) - \\nabla F(\\mathbf{x}_{n-1}) \\right \\|^2}$$  renders as\n$$\\gamma_{n} = \\frac{ \\left | \\left (\\mathbf x_{n} - \\mathbf x_{n-1} \\right )^T \\left [\\nabla F (\\mathbf x_{n}) - \\nabla F (\\mathbf x_{n-1}) \\right ] \\right |}{\\left |\\nabla F(\\mathbf{x}_{n}) - \\nabla F(\\mathbf{x}_{n-1}) \\right |^2}$$\nExample inline math $\\nabla F(\\mathbf{x}_{n})$ renders as $\\nabla F(\\mathbf{x}_{n})$.\nExample multi-line math using the \\\\ math linebreak:\n$$f(k;p_0^*) = \\begin{cases} p_0^* \u0026amp; \\text{if }k=1, \\\\ 1-p_0^* \u0026amp; \\text {if }k=0.\\end{cases}$$  renders as\n$$f(k;p_0^*) = \\begin{cases} p_0^* \u0026amp; \\text{if }k=1, \\\n1-p_0^* \u0026amp; \\text {if }k=0.\\end{cases}$$\nDiagrams Academic supports a Markdown extension for diagrams. You can enable this feature by toggling the diagram option in your config/_default/params.toml file or by adding diagram: true to your page front matter.\nAn example flowchart:\n```mermaid graph TD A[Hard] --\u0026gt;|Text| B(Round) B --\u0026gt; C{Decision} C --\u0026gt;|One| D[Result 1] C --\u0026gt;|Two| E[Result 2] ```  renders as\ngraph TD A[Hard] --\u0026gt;|Text| B(Round) B --\u0026gt; C{Decision} C --\u0026gt;|One| D[Result 1] C --\u0026gt;|Two| E[Result 2]  An example sequence diagram:\n```mermaid sequenceDiagram Alice-\u0026gt;\u0026gt;John: Hello John, how are you? loop Healthcheck John-\u0026gt;\u0026gt;John: Fight against hypochondria end Note right of John: Rational thoughts! John--\u0026gt;\u0026gt;Alice: Great! John-\u0026gt;\u0026gt;Bob: How about you? Bob--\u0026gt;\u0026gt;John: Jolly good! ```  renders as\nsequenceDiagram Alice-\u0026gt;\u0026gt;John: Hello John, how are you? loop Healthcheck John-\u0026gt;\u0026gt;John: Fight against hypochondria end Note right of John: Rational thoughts! John--\u0026gt;\u0026gt;Alice: Great! John-\u0026gt;\u0026gt;Bob: How about you? Bob--\u0026gt;\u0026gt;John: Jolly good!  An example Gantt diagram:\n```mermaid gantt section Section Completed :done, des1, 2014-01-06,2014-01-08 Active :active, des2, 2014-01-07, 3d Parallel 1 : des3, after des1, 1d Parallel 2 : des4, after des1, 1d Parallel 3 : des5, after des3, 1d Parallel 4 : des6, after des4, 1d ```  renders as\ngantt section Section Completed :done, des1, 2014-01-06,2014-01-08 Active :active, des2, 2014-01-07, 3d Parallel 1 : des3, after des1, 1d Parallel 2 : des4, after des1, 1d Parallel 3 : des5, after des3, 1d Parallel 4 : des6, after des4, 1d  An example class diagram:\n```mermaid classDiagram Class01 \u0026lt;|-- AveryLongClass : Cool \u0026lt;\u0026lt;interface\u0026gt;\u0026gt; Class01 Class09 --\u0026gt; C2 : Where am i? Class09 --* C3 Class09 --|\u0026gt; Class07 Class07 : equals() Class07 : Object[] elementData Class01 : size() Class01 : int chimp Class01 : int gorilla class Class10 { \u0026lt;\u0026lt;service\u0026gt;\u0026gt; int id size() } ```  renders as\nclassDiagram Class01 \u0026lt;|-- AveryLongClass : Cool \u0026lt;\u0026lt;interface\u0026gt;\u0026gt; Class01 Class09 --\u0026gt; C2 : Where am i? Class09 --* C3 Class09 --|\u0026gt; Class07 Class07 : equals() Class07 : Object[] elementData Class01 : size() Class01 : int chimp Class01 : int gorilla class Class10 { \u0026lt;\u0026lt;service\u0026gt;\u0026gt; int id size() }  An example state diagram:\n```mermaid stateDiagram [*] --\u0026gt; Still Still --\u0026gt; [*] Still --\u0026gt; Moving Moving --\u0026gt; Still Moving --\u0026gt; Crash Crash --\u0026gt; [*] ```  renders as\nstateDiagram [*] --\u0026gt; Still Still --\u0026gt; [*] Still --\u0026gt; Moving Moving --\u0026gt; Still Moving --\u0026gt; Crash Crash --\u0026gt; [*]  Todo lists You can even write your todo lists in Academic too:\n- [x] Write math example - [x] Write diagram example - [ ] Do something else  renders as\n Write math example Write diagram example Do something else  Tables Represent your data in tables:\n| First Header | Second Header | | ------------- | ------------- | | Content Cell | Content Cell | | Content Cell | Content Cell |  renders as\n   First Header Second Header     Content Cell Content Cell   Content Cell Content Cell    Asides Academic supports a shortcode for asides , also referred to as notices, hints, or alerts. By wrapping a paragraph in {{% alert note %}} ... {{% /alert %}}, it will render as an aside.\n{{% alert note %}} A Markdown aside is useful for displaying notices, hints, or definitions to your readers. {{% /alert %}}  renders as\n A Markdown aside is useful for displaying notices, hints, or definitions to your readers.   Icons Academic enables you to use a wide range of icons from Font Awesome and Academicons in addition to emojis .\nHere are some examples using the icon shortcode to render icons:\n{{\u0026lt; icon name=\u0026quot;terminal\u0026quot; pack=\u0026quot;fas\u0026quot; \u0026gt;}} Terminal {{\u0026lt; icon name=\u0026quot;python\u0026quot; pack=\u0026quot;fab\u0026quot; \u0026gt;}} Python {{\u0026lt; icon name=\u0026quot;r-project\u0026quot; pack=\u0026quot;fab\u0026quot; \u0026gt;}} R  renders as\n  Terminal\n Python\n R\nDid you find this page helpful? Consider sharing it 🙌 ","date":1562889600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1562889600,"objectID":"07e02bccc368a192a0c76c44918396c3","permalink":"https://nixramirez.github.io/post/writing-technical-content/","publishdate":"2019-07-12T00:00:00Z","relpermalink":"/post/writing-technical-content/","section":"post","summary":"Academic is designed to give technical content creators a seamless experience. You can focus on the content and Academic handles the rest.\nHighlight your code snippets, take notes on math classes, and draw diagrams from textual representation.\nOn this page, you\u0026rsquo;ll find some examples of the types of technical content that can be rendered with Academic.\nExamples Code Academic supports a Markdown extension for highlighting code syntax. You can enable this feature by toggling the highlight option in your config/_default/params.","tags":null,"title":"Writing technical content in Academic","type":"post"},{"authors":["Nicole Ramirez"],"categories":null,"content":" Click the Slides button above to demo Academic\u0026rsquo;s Markdown slides feature.   Supplementary notes can be added here, including code and math .\n","date":1554595200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1554595200,"objectID":"557dc08fd4b672a0c08e0a8cf0c9ff7d","permalink":"https://nixramirez.github.io/publication/preprint/","publishdate":"2017-01-01T00:00:00Z","relpermalink":"/publication/preprint/","section":"publication","summary":"Lorem ipsum dolor sit amet, consectetur adipiscing elit. Duis posuere tellus ac convallis placerat. Proin tincidunt magna sed ex sollicitudin condimentum.","tags":["Source Themes"],"title":"An example preprint / working paper","type":"publication"},{"authors":["Nicole Ramirez"],"categories":[],"content":"from IPython.core.display import Image Image('https://www.python.org/static/community_logos/python-logo-master-v3-TM-flattened.png')  print(\u0026quot;Welcome to Academic!\u0026quot;)  Welcome to Academic!  Install Python and JupyterLab  Install Anaconda which includes Python 3 and JupyterLab.\nAlternatively, install JupyterLab with pip3 install jupyterlab.\nCreate or upload a Jupyter notebook Run the following commands in your Terminal, substituting \u0026lt;MY-WEBSITE-FOLDER\u0026gt; and \u0026lt;SHORT-POST-TITLE\u0026gt; with the file path to your Academic website folder and a short title for your blog post (use hyphens instead of spaces), respectively:\nmkdir -p \u0026lt;MY-WEBSITE-FOLDER\u0026gt;/content/post/\u0026lt;SHORT-POST-TITLE\u0026gt;/ cd \u0026lt;MY-WEBSITE-FOLDER\u0026gt;/content/post/\u0026lt;SHORT-POST-TITLE\u0026gt;/ jupyter lab index.ipynb  The jupyter command above will launch the JupyterLab editor, allowing us to add Academic metadata and write the content.\nEdit your post metadata The first cell of your Jupter notebook will contain your post metadata ( front matter ).\nIn Jupter, choose Markdown as the type of the first cell and wrap your Academic metadata in three dashes, indicating that it is YAML front matter:\n--- title: My post's title date: 2019-09-01 # Put any other Academic metadata here... ---  Edit the metadata of your post, using the documentation as a guide to the available options.\nTo set a featured image , place an image named featured into your post\u0026rsquo;s folder.\nFor other tips, such as using math, see the guide on writing content with Academic .\nConvert notebook to Markdown jupyter nbconvert index.ipynb --to markdown --NbConvertApp.output_files_dir=.  Example This post was created with Jupyter. The orginal files can be found at https://github.com/gcushen/hugo-academic/tree/master/exampleSite/content/post/jupyter\n","date":1549324800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1567641600,"objectID":"6e929dc84ed3ef80467b02e64cd2ed64","permalink":"https://nixramirez.github.io/post/jupyter/","publishdate":"2019-02-05T00:00:00Z","relpermalink":"/post/jupyter/","section":"post","summary":"Learn how to blog in Academic using Jupyter notebooks","tags":[],"title":"Display Jupyter Notebooks with Academic","type":"post"},{"authors":[],"categories":[],"content":"Create slides in Markdown with Academic  Academic | Documentation  Features  Efficiently write slides in Markdown 3-in-1: Create, Present, and Publish your slides Supports speaker notes Mobile friendly slides   Controls  Next: Right Arrow or Space Previous: Left Arrow Start: Home Finish: End Overview: Esc Speaker notes: S Fullscreen: F Zoom: Alt + Click  PDF Export : E   Code Highlighting Inline code: variable\nCode block:\nporridge = \u0026quot;blueberry\u0026quot; if porridge == \u0026quot;blueberry\u0026quot;: print(\u0026quot;Eating...\u0026quot;)   Math In-line math: $x + y = z$\nBlock math:\n$$ f\\left( x \\right) = ;\\frac{{2\\left( {x + 4} \\right)\\left( {x - 4} \\right)}}{{\\left( {x + 4} \\right)\\left( {x + 1} \\right)}} $$\n Fragments Make content appear incrementally\n{{% fragment %}} One {{% /fragment %}} {{% fragment %}} **Two** {{% /fragment %}} {{% fragment %}} Three {{% /fragment %}}  Press Space to play!\nOne  Two  Three \n A fragment can accept two optional parameters:\n class: use a custom style (requires definition in custom CSS) weight: sets the order in which a fragment appears   Speaker Notes Add speaker notes to your presentation\n{{% speaker_note %}} - Only the speaker can read these notes - Press `S` key to view {{% /speaker_note %}}  Press the S key to view the speaker notes!\n Only the speaker can read these notes Press S key to view    Themes  black: Black background, white text, blue links (default) white: White background, black text, blue links league: Gray background, white text, blue links beige: Beige background, dark text, brown links sky: Blue background, thin dark text, blue links    night: Black background, thick white text, orange links serif: Cappuccino background, gray text, brown links simple: White background, black text, blue links solarized: Cream-colored background, dark green text, blue links   Custom Slide Customize the slide style and background\n{{\u0026lt; slide background-image=\u0026quot;/img/boards.jpg\u0026quot; \u0026gt;}} {{\u0026lt; slide background-color=\u0026quot;#0000FF\u0026quot; \u0026gt;}} {{\u0026lt; slide class=\u0026quot;my-style\u0026quot; \u0026gt;}}   Custom CSS Example Let\u0026rsquo;s make headers navy colored.\nCreate assets/css/reveal_custom.css with:\n.reveal section h1, .reveal section h2, .reveal section h3 { color: navy; }   Questions?  Ask  Documentation ","date":1549324800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1549324800,"objectID":"0e6de1a61aa83269ff13324f3167c1a9","permalink":"https://nixramirez.github.io/slides/example/","publishdate":"2019-02-05T00:00:00Z","relpermalink":"/slides/example/","section":"slides","summary":"An introduction to using Academic's Slides feature.","tags":[],"title":"Slides","type":"slides"},{"authors":["Nicole Ramirez"],"categories":["Demo"],"content":"Create a free website with Academic using Markdown, Jupyter, or RStudio. Choose a beautiful color theme and build anything with the Page Builder - over 40 widgets, themes, and language packs included!\n Check out the latest demo of what you\u0026rsquo;ll get in less than 10 minutes, or view the showcase of personal, project, and business sites.\n 👉 Get Started  📚 View the documentation  💬 Ask a question on the forum  👥 Chat with the community  🐦 Twitter: @source_themes @GeorgeCushen #MadeWithAcademic  💡 Request a feature or report a bug  ⬆️ Updating? View the Update Guide and Release Notes  ❤ Support development of Academic:  ☕️ Donate a coffee  💵 Become a backer on Patreon  🖼️ Decorate your laptop or journal with an Academic sticker  👕 Wear the T-shirt  👩‍💻 Contribute        Academic is mobile first with a responsive design to ensure that your site looks stunning on every device.   Key features:\n Page builder - Create anything with widgets and elements  Edit any type of content - Blog posts, publications, talks, slides, projects, and more! Create content in Markdown , Jupyter , or RStudio  Plugin System - Fully customizable color and font themes  Display Code and Math - Code highlighting and LaTeX math supported Integrations - Google Analytics , Disqus commenting , Maps, Contact Forms, and more! Beautiful Site - Simple and refreshing one page design Industry-Leading SEO - Help get your website found on search engines and social media Media Galleries - Display your images and videos with captions in a customizable gallery Mobile Friendly - Look amazing on every screen with a mobile friendly version of your site Multi-language - 15+ language packs including English, 中文, and Português Multi-user - Each author gets their own profile page Privacy Pack - Assists with GDPR Stand Out - Bring your site to life with animation, parallax backgrounds, and scroll effects One-Click Deployment - No servers. No databases. Only files.  Themes Academic comes with automatic day (light) and night (dark) mode built-in. Alternatively, visitors can choose their preferred mode - click the sun/moon icon in the top right of the Demo to see it in action! Day/night mode can also be disabled by the site admin in params.toml.\n Choose a stunning theme and font for your site. Themes are fully customizable .\nEcosystem   Academic Admin : An admin tool to import publications from BibTeX or import assets for an offline site  Academic Scripts : Scripts to help migrate content to new versions of Academic  Install You can choose from one of the following four methods to install:\n  one-click install using your web browser (recommended)   install on your computer using Git with the Command Prompt/Terminal app   install on your computer by downloading the ZIP files   install on your computer with RStudio   Then personalize and deploy your new site .\nUpdating  View the Update Guide .\nFeel free to star the project on Github to help keep track of updates .\nLicense Copyright 2016-present George Cushen .\nReleased under the MIT license.\n","date":1461110400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1555459200,"objectID":"279b9966ca9cf3121ce924dca452bb1c","permalink":"https://nixramirez.github.io/post/getting-started/","publishdate":"2016-04-20T00:00:00Z","relpermalink":"/post/getting-started/","section":"post","summary":"Create a beautifully simple website in under 10 minutes.","tags":["Academic"],"title":"Academic: the website builder for Hugo","type":"post"},{"authors":["Nicole Ramirez","Robert Ford"],"categories":null,"content":" Click the Cite button above to demo the feature to enable visitors to import publication metadata into their reference management software.    Click the Slides button above to demo Academic\u0026rsquo;s Markdown slides feature.   Supplementary notes can be added here, including code and math .\n","date":1441065600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1441065600,"objectID":"966884cc0d8ac9e31fab966c4534e973","permalink":"https://nixramirez.github.io/publication/journal-article/","publishdate":"2017-01-01T00:00:00Z","relpermalink":"/publication/journal-article/","section":"publication","summary":"Lorem ipsum dolor sit amet, consectetur adipiscing elit. Duis posuere tellus ac convallis placerat. Proin tincidunt magna sed ex sollicitudin condimentum.","tags":["Source Themes"],"title":"An example journal article","type":"publication"},{"authors":["Nicole Ramirez","Robert Ford"],"categories":null,"content":" Click the Cite button above to demo the feature to enable visitors to import publication metadata into their reference management software.    Click the Slides button above to demo Academic\u0026rsquo;s Markdown slides feature.   Supplementary notes can be added here, including code and math .\n","date":1372636800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1372636800,"objectID":"69425fb10d4db090cfbd46854715582c","permalink":"https://nixramirez.github.io/publication/conference-paper/","publishdate":"2017-01-01T00:00:00Z","relpermalink":"/publication/conference-paper/","section":"publication","summary":"Lorem ipsum dolor sit amet, consectetur adipiscing elit. Duis posuere tellus ac convallis placerat. Proin tincidunt magna sed ex sollicitudin condimentum.","tags":["Source Themes"],"title":"An example conference paper","type":"publication"}]