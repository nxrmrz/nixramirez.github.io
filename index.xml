<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Hello, World</title>
    <link>https://nixramirez.github.io/</link>
      <atom:link href="https://nixramirez.github.io/index.xml" rel="self" type="application/rss+xml" />
    <description>Hello, World</description>
    <generator>Source Themes Academic (https://sourcethemes.com/academic/)</generator><language>en-us</language><lastBuildDate>Sat, 01 Jun 2030 13:00:00 +0000</lastBuildDate>
    <image>
      <url>img/map[gravatar:%!s(bool=false) shape:circle]</url>
      <title>Hello, World</title>
      <link>https://nixramirez.github.io/</link>
    </image>
    
    <item>
      <title>Multi-Class Emotion Classification with Deep Learning using BERT</title>
      <link>https://nixramirez.github.io/project/bert-emotion-classification/</link>
      <pubDate>Wed, 15 Apr 2020 00:00:00 +0000</pubDate>
      <guid>https://nixramirez.github.io/project/bert-emotion-classification/</guid>
      <description>&lt;h3 id=&#34;helpful-prerequisites&#34;&gt;Helpful Prerequisites&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Intermediate-level knowledge of Python 3 (NumPy and Pandas preferably, but not required)&lt;/li&gt;
&lt;li&gt;Exposure to PyTorch usage&lt;/li&gt;
&lt;li&gt;Basic understanding of Deep Learning, Natural Language Processing (NLP) and Language Models (BERT specifically)&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;project-outline&#34;&gt;Project Outline&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;TL;DR&lt;/strong&gt;: We implement transfer learning using BERT to achieve 98.6% accuracy on emotion classification of tweets. We show the whole end-to-end process in this notebook.&lt;/p&gt;
&lt;p&gt;Feel free to click on the below links to skip to that section.&lt;/p&gt;
&lt;p&gt;
&lt;a href=&#34;#introduction&#34;&gt;&lt;strong&gt;Task 1&lt;/strong&gt;: Introduction&lt;/a&gt;
&lt;/p&gt;
&lt;p&gt;
&lt;a href=&#34;#eda&#34;&gt;&lt;strong&gt;Task 2&lt;/strong&gt;: Exploratory Data Analysis&lt;/a&gt;
&lt;/p&gt;
&lt;p&gt;
&lt;a href=&#34;#preprocessing&#34;&gt;&lt;strong&gt;Task 3&lt;/strong&gt;: Data Pre-processing&lt;/a&gt;
&lt;/p&gt;
&lt;p&gt;
&lt;a href=&#34;#trainvalsplit&#34;&gt;&lt;strong&gt;Task 4&lt;/strong&gt;: Training and Validation Split&lt;/a&gt;
&lt;/p&gt;
&lt;p&gt;
&lt;a href=&#34;#tokenisation&#34;&gt;&lt;strong&gt;Task 5&lt;/strong&gt;: Loading Tokenizer and Encoding our Data&lt;/a&gt;
&lt;/p&gt;
&lt;p&gt;
&lt;a href=&#34;#BERTsetup&#34;&gt;&lt;strong&gt;Task 6&lt;/strong&gt;: Setting up BERT Pretrained Model&lt;/a&gt;
&lt;/p&gt;
&lt;p&gt;
&lt;a href=&#34;#dataloaders&#34;&gt;&lt;strong&gt;Task 7&lt;/strong&gt;: Creating Data Loaders&lt;/a&gt;
&lt;/p&gt;
&lt;p&gt;
&lt;a href=&#34;#optimise&#34;&gt;&lt;strong&gt;Task 8&lt;/strong&gt;: Setting Up Optimizer and Scheduler&lt;/a&gt;
&lt;/p&gt;
&lt;p&gt;
&lt;a href=&#34;#performancemetrics&#34;&gt;&lt;strong&gt;Task 9&lt;/strong&gt;: Defining our Performance Metrics&lt;/a&gt;
&lt;/p&gt;
&lt;p&gt;
&lt;a href=&#34;#trainevalloops&#34;&gt;&lt;strong&gt;Task 10&lt;/strong&gt;: Creating our Training and Evaluation Loops&lt;/a&gt;
&lt;/p&gt;
&lt;p&gt;
&lt;a href=&#34;#evaluatemodel&#34;&gt;&lt;strong&gt;Task 11&lt;/strong&gt;: Loading and Evaluating our Model&lt;/a&gt;
&lt;/p&gt;
&lt;h1&gt;&lt;a id=&#34;introduction&#34;&gt;Task 1: Introduction&lt;/a&gt;&lt;/h1&gt;
&lt;h3 id=&#34;the-problem&#34;&gt;The Problem&lt;/h3&gt;
&lt;p&gt;Emotion classification, or the task of ascribing an emotion category to a textual document, is a typical NLP problem solved either through machine learning (ML) or deep learning (DL) methods. The popularity of the latter solution over the former has grown in recent years, as powerful language models utilising DL have been increasingly open-sourced and utilised, alongside the tools used to build and customise them.&lt;/p&gt;
&lt;p&gt;NLP benefits from DL&amp;rsquo;s ability to preserve more context and require less feature-engineering than ML. There&amp;rsquo;s no free lunch, however, as utilising DL methods comes at a cost: more powerful computational resources are needed to run DL models, they may be less explainable than traditional ML ones, and model training and/or inference might take longer. Deciding which framework to use ultimately requires balancing these pros and cons.&lt;/p&gt;
&lt;p&gt;A particularly exciting benefit of language models trained through DL, however, is their ability to learn large amounts of information from a huge data-source and &lt;strong&gt;transfer their learning&lt;/strong&gt; to solve tasks that aren&amp;rsquo;t necessarily within their training domain.&lt;/p&gt;
&lt;p&gt;This concept of 
&lt;a href=&#34;http://jalammar.github.io/illustrated-bert/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;transfer learning&lt;/a&gt;
 has revolutionised NLP. In a nutshell, transfer learning involves training a large neural network on extremely large data-sets to learn rich, nuanced, generalisable information that it can transfer to a smaller model &lt;strong&gt;fine-tuned&lt;/strong&gt; to solve a specific task. With transfer learning, the smaller model performs better at this task and/or solves it quicker after gaining this external knowledge than if it was left alone to tackle the task.&lt;/p&gt;
&lt;p&gt;State-of-the-art achievements in transfer learning were enabled by innovatively architected deep neural networks called 
&lt;a href=&#34;http://jalammar.github.io/illustrated-transformer/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;transformers&lt;/a&gt;
.&lt;/p&gt;
&lt;p&gt;In this notebook, we try to classify the emotion of tweets through leveraging the knowledge of a particular transformer-based model called BERT, trained and open-sourced by Google. BERT is a large-scale transformer-based Language Model that can be finetuned for a variety of tasks. It 
&lt;a href=&#34;https://ai.googleblog.com/2018/11/open-sourcing-bert-state-of-art-pre.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;beat multiple NLP benchmarks during its release in 2018&lt;/a&gt;
.&lt;/p&gt;
&lt;p&gt;Armed with the generalist language knowledge of BERT, we fine-tune BERT on 
&lt;a href=&#34;https://figshare.com/articles/smile_annotations_final_csv/3187909&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;our own dataset&lt;/a&gt;
, a collection of 3,085 tweets each classified according to 5 emotions (i.e. anger, disgust, happiness, surprise and sadness). This collection of tweets mentions 13 Twitter handles associated with British museums, and was gathered between May 2013 and June 2015. It was created for the purpose of classifying emotions, expressed on Twitter, towards arts and cultural experiences in museums.&lt;/p&gt;
&lt;p&gt;Fine-tuning on this particular data-set allows us to classify a tweet into one of 5 emotion categories later on. We&amp;rsquo;ll tackle the technical details in relevant sections of this notebook.&lt;/p&gt;
&lt;p&gt;For more information about BERT, the original publication for it is linked 
&lt;a href=&#34;https://arxiv.org/abs/1810.04805&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;here&lt;/a&gt;
.&lt;/p&gt;
&lt;p&gt;We are using 
&lt;a href=&#34;https://huggingface.co/transformers/model_doc/bert.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Huggingface&amp;rsquo;s implementation of BERT&lt;/a&gt;
 for this project, written in PyTorch.&lt;/p&gt;
&lt;h1&gt;&lt;a id=&#34;eda&#34;&gt;Task 2: Exploratory Data Analysis&lt;/a&gt;&lt;/h1&gt;
&lt;p&gt;First, let&amp;rsquo;s make sure we&amp;rsquo;re in the directory containing our data-set. In my system (working off Google Colaboratory connected to a Google Drive back-end), my directory is specified in the PATH variable below.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;#we list the contents of our directory
!ls
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;sample_data
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;PATH = &#39;./drive/My Drive/Colab Notebooks/bert-emotion-tweets-tutorial/&#39;
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;import os
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We change to the directory via the &lt;code&gt;os.chdir()&lt;/code&gt; command&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;os.chdir(PATH)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;#we check that our current directory has our data-set
!ls
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;Epoch-6.model		     Tutorial-Bert-emotional-analysis.ipynb
smile-annotations-final.csv
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We then import necessary libraries&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;import torch #the pytorch library, used for modeling and formatting our data to be compatible in a pytorch environment
import pandas as pd #for dataframe reading, cleaning functions
from tqdm.notebook import tqdm #used as a progress bar
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Before reading our data in, let&amp;rsquo;s check what it looks like &amp;ndash; i.e. if it has a header, what its columns are, etc. We do this with a terminal command below&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;#inspect first 5 entires
!head -n 5 smile-annotations-final.csv
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;611857364396965889,&amp;quot;@aandraous @britishmuseum @AndrewsAntonio Merci pour le partage! @openwinemap&amp;quot;,nocode
614484565059596288,&amp;quot;Dorian Gray with Rainbow Scarf #LoveWins (from @britishmuseum http://t.co/Q4XSwL0esu) http://t.co/h0evbTBWRq&amp;quot;,happy
614746522043973632,&amp;quot;@SelectShowcase @Tate_StIves ... Replace with your wish which the artist uses in next installation! It was entralling!&amp;quot;,happy
614877582664835073,&amp;quot;@Sofabsports thank you for following me back. Great to hear from a diverse &amp;amp;amp; interesting panel #DefeatingDepression @RAMMuseum&amp;quot;,happy
611932373039644672,&amp;quot;@britishmuseum @TudorHistory What a beautiful jewel / portrait. Is the &#39;R&#39; for Rex ?&amp;quot;,happy
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We read the data in, and specify colum names since it doesnt have any. We also index the rows by id.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;df_train = pd.read_csv(&#39;smile-annotations-final.csv&#39;, names=[&#39;id&#39;, &#39;entry&#39;, &#39;emotion&#39;], index_col=&#39;id&#39;)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;#inspecting the above process
df_train.head()
&lt;/code&gt;&lt;/pre&gt;
&lt;div&gt;
&lt;style scoped&gt;
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }
&lt;pre&gt;&lt;code&gt;.dataframe tbody tr th {
    vertical-align: top;
}

.dataframe thead th {
    text-align: right;
}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;/style&gt;&lt;/p&gt;
&lt;table border=&#34;1&#34; class=&#34;dataframe&#34;&gt;
  &lt;thead&gt;
    &lt;tr style=&#34;text-align: right;&#34;&gt;
      &lt;th&gt;&lt;/th&gt;
      &lt;th&gt;entry&lt;/th&gt;
      &lt;th&gt;emotion&lt;/th&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;id&lt;/th&gt;
      &lt;th&gt;&lt;/th&gt;
      &lt;th&gt;&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;th&gt;611857364396965889&lt;/th&gt;
      &lt;td&gt;@aandraous @britishmuseum @AndrewsAntonio Merc...&lt;/td&gt;
      &lt;td&gt;nocode&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;614484565059596288&lt;/th&gt;
      &lt;td&gt;Dorian Gray with Rainbow Scarf #LoveWins (from...&lt;/td&gt;
      &lt;td&gt;happy&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;614746522043973632&lt;/th&gt;
      &lt;td&gt;@SelectShowcase @Tate_StIves ... Replace with ...&lt;/td&gt;
      &lt;td&gt;happy&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;614877582664835073&lt;/th&gt;
      &lt;td&gt;@Sofabsports thank you for following me back. ...&lt;/td&gt;
      &lt;td&gt;happy&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;611932373039644672&lt;/th&gt;
      &lt;td&gt;@britishmuseum @TudorHistory What a beautiful ...&lt;/td&gt;
      &lt;td&gt;happy&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;
&lt;/div&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;#inspecting the dimensions of our data
df_train.shape
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;(3085, 2)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Our data-set has 2 columns, one for the actual tweet (i.e. the entry column), and one for the label (i.e. the emotion column). Let&amp;rsquo;s see the different values for emotion our data-set has&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;df_train[&#39;emotion&#39;].unique()
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;array([&#39;nocode&#39;, &#39;happy&#39;, &#39;not-relevant&#39;, &#39;angry&#39;, &#39;disgust|angry&#39;,
       &#39;disgust&#39;, &#39;happy|surprise&#39;, &#39;sad&#39;, &#39;surprise&#39;, &#39;happy|sad&#39;,
       &#39;sad|disgust&#39;, &#39;sad|angry&#39;, &#39;sad|disgust|angry&#39;], dtype=object)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We have single emotions, a combination of emotions, and two categories irrelevant for our purposes (i.e. the nocode and not relevant category, where the tweet&amp;rsquo;s emotion category was unclear). We also see that our data-set is highly imbalanced, with some categories having thousands of examples, whilst others (ie.. the disgust category) having less than 10. Our modeling approach must take this imbalance into account later on.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;df_train[&#39;emotion&#39;].value_counts()
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;nocode               1572
happy                1137
not-relevant          214
angry                  57
surprise               35
sad                    32
happy|surprise         11
happy|sad               9
disgust|angry           7
disgust                 6
sad|angry               2
sad|disgust             2
sad|disgust|angry       1
Name: emotion, dtype: int64
&lt;/code&gt;&lt;/pre&gt;
&lt;h1&gt;&lt;a id=&#34;preprocessing&#34;&gt;Task 3: Pre-processing&lt;/a&gt;&lt;/h1&gt;
&lt;p&gt;Before fine-tuning BERT onto our data-set, we perform very minimal pre-processing on our tweets. The pre-processing steps we undertake are outlined below. While machine learning methods benefit from a lot of pre-processing, there might be a loss of accuracy with extensive pre-processing prior to modeling with deep learning. This is due to deep learning being very effective at dealing with raw text, and with deep learning models being trained on text collections with a lot of noise/error.&lt;/p&gt;
&lt;p&gt;More importantly, for us, BERT was trained on Wikipedia (that’s about 2,500 million words) and a book corpus (800 million words). Imaginably, these sources were proof-read and edited and likely use more formal language, low in errors.&lt;/p&gt;
&lt;p&gt;Since we&amp;rsquo;re fine-tuning on a relatively informal and error-laden data-set of tweets, we pre-process to come as close as possible to the corpus BERT was trained on.&lt;/p&gt;
&lt;p&gt;We also remove categories that have more than one emotion-label (i.e. &lt;code&gt;happy|surprised&lt;/code&gt;), as that is a multi-label classification problem. We focus on multi-class, single-label classification in this notebook.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;em&gt;NB: Multi-class refers to more than 2 classes for a target. A target with exclusively 2 classes is termed &amp;lsquo;binary&amp;rsquo; instead. Multi-label refers to having more than 1 label per class&lt;/em&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Our pre-processing steps involve:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Contractions Mapping&lt;/li&gt;
&lt;li&gt;Punctuation Removal&lt;/li&gt;
&lt;li&gt;@sign, URL, excess whitespace, HTML tag removal&lt;/li&gt;
&lt;li&gt;Correcting accented characters&lt;/li&gt;
&lt;li&gt;Emoji replacement&lt;/li&gt;
&lt;li&gt;Removal of multi-label categories&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;We first have to install the contractions library with:
&lt;code&gt;!pip install contractions&lt;/code&gt; if we don&amp;rsquo;t have it yet&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Contractions Mapping - we expand out contractions, so words like y&amp;rsquo;all, should&amp;rsquo;ve will be converted to &amp;lsquo;you all&amp;rsquo; and &amp;lsquo;should have&amp;rsquo;&lt;/li&gt;
&lt;/ol&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;import contractions 
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;contractions.fix(&amp;quot;im hungry and its cold yall&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;&#39;I am hungry and its cold you all&#39;
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;#expanding out contractions
df_train[&#39;entry&#39;] = df_train[&#39;entry&#39;].apply(lambda entry: contractions.fix(entry))
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;2.), 3.) and 4.) above are then done as follows, with self explanatory function names. We don&amp;rsquo;t remove &amp;lsquo;!&#39;, &amp;lsquo;?&amp;rsquo; and &amp;lsquo;.&amp;rsquo; completely as these contribute to the tone/meaning of a sentence, but do remove their duplicates.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from bs4 import BeautifulSoup # a library for parsing HTML
import string
import unicodedata
import re
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# remove HTML tags
def strip_html_tags(text):
    soup = BeautifulSoup(text, &amp;quot;html.parser&amp;quot;)    
    return soup.get_text().replace(&amp;quot;\n&amp;quot;, &amp;quot;&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# we then apply the function for removing HTML Tags
df_train[&#39;entry&#39;] = df_train[&#39;entry&#39;].apply(strip_html_tags)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# normalise accented characters i.e. convert à to a
def remove_accented_chars(text):
    text = unicodedata.normalize(&#39;NFKD&#39;, text).encode(&#39;ascii&#39;, &#39;ignore&#39;).decode(&#39;utf-8&#39;,&#39;ignore&#39;)
    return text
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;df_train[&#39;entry&#39;] = df_train[&#39;entry&#39;].apply(remove_accented_chars)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;#remove @name mentions and urls in a tweet
def remove_mentions_and_urls(text):
  text = re.sub(&#39;(@[A-Za-z0-9]+)|(\w+:\/\/\S+)|(www.[A-Za-z0-9]+.[A-Za-z0-9]+)&#39;,&#39; &#39;, text)
  return text
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;df_train[&#39;entry&#39;] = df_train[&#39;entry&#39;].apply(remove_mentions_and_urls)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;#remove punctuations except &#39;?&#39; and &#39;!&#39; and &#39;.&#39;
def remove_punctuation(text):
    text = re.sub(r&#39;[\&#39;\&amp;quot;\\\/\,#]&#39;, &#39;&#39;, text)
    text = re.sub(r&#39;[^\w\s\?\!\.]&#39;, &#39; &#39;, text)
    return text
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;df_train[&#39;entry&#39;] = df_train[&#39;entry&#39;].apply(remove_punctuation)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;#remove multiple &#39;.&#39;, keep just one
def remove_excess_fullstops(text):
  text = re.sub(r&#39;\.{2,}&#39;, &#39;.&#39;, text)
  return text
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;df_train[&#39;entry&#39;] = df_train[&#39;entry&#39;].apply(remove_excess_fullstops)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;#remove excess and trailing/leading whitespace
def remove_excess_whitespace(text):
  text = re.sub(r&#39;\s{2,}&#39;, &#39; &#39;, text).strip()
  return text
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;df_train[&#39;entry&#39;] = df_train[&#39;entry&#39;].apply(remove_excess_whitespace)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We look at 10 pre-processed entries:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;df_train.entry.sample(10)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;id
615468630256566272                   that is great thank you very much!
612763483654918144    Gold brooch of Helios or sun god from the work...
613687848303206400    MT Feast Day of JohntheBaptist. Explore his li...
613986003922092032    Off to for definingbeauty wanted to go for age...
611487712521121792    Pick up a copy of the beautiful Catalogue of T...
613241383470690304    no worries. Good to hear we might see you on 4...
614964929473421312    They all gazed at him as if he a statue. Plato...
610505107084570624    Reviewed Leonora Carringtons visit to for here...
611838280888385536                                  _MADRE e battle sia
611625177944862721              Ooo we bet this was good ! presentfilms
Name: entry, dtype: object
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;#we remove &#39;nocode&#39; and &#39;not relevant&#39; categories, as these don&#39;t indicate emotion
df_train.drop(df_train[(df_train.emotion == &#39;nocode&#39;) | (df_train.emotion == &#39;not-relevant&#39;)].index, inplace=True)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;#we remove categories with a &#39;|&#39; in them, our multi-label categories
df_train.drop(df_train[df_train.emotion.str.contains(&#39;\\|&#39;)].index, inplace=True)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We then see below how many examples we have for each emotion category. We confirm that we have a highly imbalanced dataset (i.e. the largest category has 1137 entries, while the smallest has only 6)&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;df_train.emotion.value_counts()
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;happy       1137
angry         57
surprise      35
sad           32
disgust        6
Name: emotion, dtype: int64
&lt;/code&gt;&lt;/pre&gt;
&lt;h1&gt;&lt;a id=&#34;trainvalsplit&#34;&gt;Task 4: Training/Validation Split&lt;/a&gt;&lt;/h1&gt;
&lt;p&gt;After pre-processing, our data-set is ready to be split into training and validation data-sets. As our data-set is imbalanced, we&amp;rsquo;ll do &lt;strong&gt;stratified sampling&lt;/strong&gt;. Say we want a split of 85% training and 15% validation data&amp;ndash;this sampling technique ensures that the split happens within each category as opposed to considering all categories as a collective.&lt;/p&gt;
&lt;p&gt;Without the stratification, we might oversample categories with a large number of examples, and completely exclude categories with a low number of examples (i.e. the disgust category with 6 examples might be left out of training). With the stratification, we split the digust category, for example, 85-15 as well (i.e. 5 training examples, 1 validation example).&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;#importing modules for splitting the data-set
from sklearn.model_selection import train_test_split
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;#now we create a list of unique emotion labels
possible_labels = df_train[&#39;emotion&#39;].unique()
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;BERT specifically requires that labels passed into it are converted to numbers, hence we do this in the below step.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;#we convert those labels to numbers, for use in our algorithm later on
label_dict = {}
for index, possible_label in enumerate(possible_labels):
    label_dict[possible_label] = index
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;label_dict
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;{&#39;angry&#39;: 1, &#39;disgust&#39;: 2, &#39;happy&#39;: 0, &#39;sad&#39;: 3, &#39;surprise&#39;: 4}
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;#We add a new column to our original data-frame, of numbers corresponding to each emotion label
df_train[&#39;label&#39;] = [label_dict[str_label] for str_label in df_train[&#39;emotion&#39;]]
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;df_train.sample(5) 
&lt;/code&gt;&lt;/pre&gt;
&lt;div&gt;
&lt;style scoped&gt;
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }
&lt;pre&gt;&lt;code&gt;.dataframe tbody tr th {
    vertical-align: top;
}

.dataframe thead th {
    text-align: right;
}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;/style&gt;&lt;/p&gt;
&lt;table border=&#34;1&#34; class=&#34;dataframe&#34;&gt;
  &lt;thead&gt;
    &lt;tr style=&#34;text-align: right;&#34;&gt;
      &lt;th&gt;&lt;/th&gt;
      &lt;th&gt;entry&lt;/th&gt;
      &lt;th&gt;emotion&lt;/th&gt;
      &lt;th&gt;label&lt;/th&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;id&lt;/th&gt;
      &lt;th&gt;&lt;/th&gt;
      &lt;th&gt;&lt;/th&gt;
      &lt;th&gt;&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;th&gt;615150243592728576&lt;/th&gt;
      &lt;td&gt;Two very different but equally beautiful events&lt;/td&gt;
      &lt;td&gt;happy&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;611115939082424321&lt;/th&gt;
      &lt;td&gt;F is for Funky Fiddle Leaf but also for the fa...&lt;/td&gt;
      &lt;td&gt;happy&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;611980866865242116&lt;/th&gt;
      &lt;td&gt;Thank you for fascinating talk on art and syna...&lt;/td&gt;
      &lt;td&gt;happy&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;613086766199894016&lt;/th&gt;
      &lt;td&gt;The Cantabridgia Daili is out! Stories via _Ca...&lt;/td&gt;
      &lt;td&gt;happy&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;615120985738637312&lt;/th&gt;
      &lt;td&gt;Great Baramundi fish with Clive Loveless. Abor...&lt;/td&gt;
      &lt;td&gt;happy&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;
&lt;/div&gt;
&lt;p&gt;We then split our data into training and validation sets. Validation sets are useful for detecting overfitting in our classifier later on. We set a random state of &lt;code&gt;17&lt;/code&gt; so that anyone who wants to reproduce this notebook can get the exact same results as us.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;x_train, x_val, y_train, y_val = train_test_split(
    df_train.index.values,
    df_train[&#39;label&#39;],
    test_size = 0.20, #let&#39;s do 85-15 train-validation split
    random_state=17, #reproducible between my instance and whoever wants to reproduce
    stratify= df_train[&#39;label&#39;].values #the command for stratification
)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We then check whether the split successfully produced a stratified 85-15 split within each emotion category&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;#create a dummy column housing data types - either train/val later on
df_train[&#39;data_type&#39;] = [&#39;not_set&#39;]*df_train.shape[0]
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;#if id of sample exists in x_train, make it &#39;train&#39;, otherwise existing in x_val, make it &#39;val&#39;
df_train.loc[x_train, &#39;data_type&#39;] = &#39;train&#39;
df_train.loc[x_val, &#39;data_type&#39;] = &#39;val&#39;
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;#check stratification of training and validation data-sets
df_train.groupby([&#39;emotion&#39;, &#39;data_type&#39;])[&#39;entry&#39;].count()
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;emotion   data_type
angry     train         45
          val           12
disgust   train          5
          val            1
happy     train        909
          val          228
sad       train         26
          val            6
surprise  train         28
          val            7
Name: entry, dtype: int64
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We see that the stratification worked!&lt;/p&gt;
&lt;p&gt;Now that we have our training and validation data-sets, we still have to convert them to a format that BERT accepts.&lt;/p&gt;
&lt;p&gt;There&amp;rsquo;s a lot that&amp;rsquo;s been written on this required format, so we&amp;rsquo;ll just briefly mention them here.&lt;/p&gt;
&lt;p&gt;In order to fine-tune with the pre-trained BERT model, we need to use its tokeniser. This is because 1.) BERT has a specific, fixed vocabulary and 2.) BERT has a particular way of handling words outside this vocabulary.&lt;/p&gt;
&lt;p&gt;In addition, we need to add special tokens to the start and end of each sentence, pad and truncate all sentences to a fixed length, and specify which parts of the sentences are padded with an &amp;lsquo;attention mask&amp;rsquo;&lt;/p&gt;
&lt;p&gt;Luckily, the HuggingFace implementation of BERT has a method we can call on the BERT tokeniser, named &lt;code&gt;encode_plus&lt;/code&gt;, that does all of the above for us.&lt;/p&gt;
&lt;p&gt;The &lt;code&gt;encode_plus method&lt;/code&gt; of BERT&amp;rsquo;s tokenizer will:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;split our text into tokens,&lt;/li&gt;
&lt;li&gt;add the special [CLS] and [SEP] tokens, and&lt;/li&gt;
&lt;li&gt;convert these tokens into indexes of the tokenizer vocabulary,&lt;/li&gt;
&lt;li&gt;pad or truncate sentences to a specified max length, and&lt;/li&gt;
&lt;li&gt;create an attention mask.&lt;/li&gt;
&lt;/ol&gt;
&lt;h1&gt;&lt;a id=&#34;tokenisation&#34;&gt;Task 5: Loading Tokenizer and Encoding our Data&lt;/a&gt;&lt;/h1&gt;
&lt;p&gt;We first install the transformers library from HuggingFace to get access to the &lt;code&gt;BERTokenizer&lt;/code&gt; and our eventual &lt;code&gt;BERTForSequenceClassification&lt;/code&gt; model, the one that&amp;rsquo;s fine-tuned for classification tasks. We can do this by running the command:&lt;/p&gt;
&lt;p&gt;&lt;code&gt;!pip install transformers==3.0.0&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;We also import functionality from PyTorch for creating a &lt;code&gt;TensorDataSet&lt;/code&gt;, which is a multi-dimensional tensor data-structure that&amp;rsquo;s used heavily in a PyTorch environment.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from transformers import BertTokenizer
from torch.utils.data import TensorDataset #setting up our dataset so it&#39;s usable in a pytorch environment
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;#set up a tokenizer object, using pre-trained BERT&#39;s own tokenizer
tokenizer = BertTokenizer.from_pretrained(
    &#39;bert-base-uncased&#39;, #we ask the tokenizer to lowercase our sentences
    do_lower_case=True
)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;HBox(children=(FloatProgress(value=0.0, description=&#39;Downloading&#39;, max=231508.0, style=ProgressStyle(descripti…
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Recall that we have to pad sentences to a specified length. We first figure out the maximum length of all of our tokenised sentences in the whole data-set via the code block below. We then pass in that max length value to &lt;code&gt;encode_plus&lt;/code&gt;, who will handle padding each sentence to that length for us.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;#getting the maximum tokenised length out of tweets in our training data-set
max_len = 0

# For every sentence...
for sent in df_train[&#39;entry&#39;]:

    # Tokenize the text and add `[CLS]` and `[SEP]` tokens.
    input_ids = tokenizer.encode(sent, add_special_tokens=True)

    # Update the maximum sentence length.
    max_len = max(max_len, len(input_ids))

print(&#39;Max sentence length: &#39;, max_len)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;Max sentence length:  36
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;#encode our training and validation data-sets with the tokenizer above
encoded_data_train = tokenizer.batch_encode_plus(
    #change below to appropriate setup
    df_train.entry.values,
    add_special_tokens=True, #add the CLS and SEP tokens
    truncation=True,
    return_attention_mask=True, 
    pad_to_max_length=True,
    max_length=max_len,
    return_tensors=&#39;pt&#39; #returns pytorch tensor
)

encoded_data_val = tokenizer.batch_encode_plus(
    #change below to appropriate setup
    df_train[df_train.data_type==&#39;val&#39;].entry.values,
    add_special_tokens=True, #adds the CLS and SEP tokens
    truncation=True,
    return_attention_mask=True,
    pad_to_max_length=True,
    max_length=max_len,
    return_tensors=&#39;pt&#39;
)

#encoding process above returns dictionaries. We grab input ID tokens, attention mask, and labels from this
input_ids_train = encoded_data_train[&#39;input_ids&#39;] #return each sentence as a #
attention_masks_train = encoded_data_train[&#39;attention_mask&#39;] #returns a pytorch tensor
#change below to appropriate setup, resampled or not
labels_train = torch.tensor(df_train.label.values)

input_ids_val = encoded_data_val[&#39;input_ids&#39;] #return each sentence as a #
attention_masks_val = encoded_data_val[&#39;attention_mask&#39;] #returns a pytorch tensor
labels_val = torch.tensor(df_train[df_train.data_type==&#39;val&#39;].label.values)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;#we construct a tensor dataset from input ID tokens, attention mask, and labels
dataset_train = TensorDataset(input_ids_train, 
                              attention_masks_train, labels_train)
dataset_validation = TensorDataset(input_ids_val,
                            attention_masks_val, labels_val)
&lt;/code&gt;&lt;/pre&gt;
&lt;h1&gt;&lt;a id=&#34;BERTsetup&#34;&gt;Task 6: Setting Up BERT Pre-Trained Model&lt;/a&gt;&lt;/h1&gt;
&lt;p&gt;Now onto our fine-tuning step!&lt;/p&gt;
&lt;p&gt;The term &amp;lsquo;fine-tuning&amp;rsquo; is generally interchangeable with the term &amp;lsquo;transfer-learning&amp;rsquo;.&lt;/p&gt;
&lt;p&gt;To perform this step, a deep learning model is &amp;lsquo;chopped off&amp;rsquo; at one of its later layers, with the subsequent layers being replaced by a classifier.&lt;/p&gt;
&lt;p&gt;The intuition behind fine-tuning, as so eloquently put 
&lt;a href=&#34;https://www.tensorflow.org/tutorials/images/transfer_learning&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;in this Tensorflow Documentation page&lt;/a&gt;
 is &amp;lsquo;that if a model is trained on a large and general enough dataset, this model will effectively serve as a generic model of the visual world. You can then take advantage of these learned feature maps without having to start from scratch by training a large model on a large dataset.&amp;rsquo;&lt;/p&gt;
&lt;p&gt;HuggingFace has implemented a model called &lt;code&gt;BERTForSequenceClassification&lt;/code&gt; that has a sequence classification/regression head on top (i.e. a linear layer on top of the pooled output) of the BERT deep learning model. This is what we&amp;rsquo;ll use for our fine-tuning.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from transformers import BertForSequenceClassification
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We import this model. Below is how we initialise the fine-tuning step. We add another layer on top of it of 6 nodes (i.e. one corresponding to each emotion category).&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;#each tweet is its own sequence, which will be classified into one of 6 classes
model = BertForSequenceClassification.from_pretrained(
    &#39;bert-base-uncased&#39;,  
    num_labels = len(label_dict),
    output_attentions = False, #dont need attention mask
    output_hidden_states = False #last layer before output
)
pass
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;HBox(children=(FloatProgress(value=0.0, description=&#39;Downloading&#39;, max=433.0, style=ProgressStyle(description_…






HBox(children=(FloatProgress(value=0.0, description=&#39;Downloading&#39;, max=440473133.0, style=ProgressStyle(descri…





Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: [&#39;cls.predictions.bias&#39;, &#39;cls.predictions.transform.dense.weight&#39;, &#39;cls.predictions.transform.dense.bias&#39;, &#39;cls.predictions.decoder.weight&#39;, &#39;cls.seq_relationship.weight&#39;, &#39;cls.seq_relationship.bias&#39;, &#39;cls.predictions.transform.LayerNorm.weight&#39;, &#39;cls.predictions.transform.LayerNorm.bias&#39;]
- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model).
- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: [&#39;classifier.weight&#39;, &#39;classifier.bias&#39;]
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
&lt;/code&gt;&lt;/pre&gt;
&lt;h1&gt;&lt;a id=&#34;dataloaders&#34;&gt;Task 7: Creating Data Loaders&lt;/a&gt;&lt;/h1&gt;
&lt;p&gt;A &lt;code&gt;DataLoader&lt;/code&gt; is an iterable data structure within PyTorch that contains a collection of &lt;code&gt;TensorDataSets&lt;/code&gt;. We iterate through a &lt;code&gt;DataLoader&lt;/code&gt; either sequentially, from the first example to the last, or randomly.&lt;/p&gt;
&lt;p&gt;The former is a useful format for our validation data-set, as we&amp;rsquo;d want to tie the predictions in our validation data-set back to the original dataframe (and hence we want the ordering to be preserved). The latter is a useful format for our training data-set, as we prevent biasing the training of our mdodel when we randomly sample our batches.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;#we import our DataLoader and Samplers
from torch.utils.data import DataLoader, RandomSampler, SequentialSampler
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The batch size that we allow per iteration will affect how much compute power is used when fine-tuning on our data. Larger batch-sizes are more suitable for more powerful hardware (i.e. GPUs) whilst smaller batch-sizes are better for CPUs, for example.&lt;/p&gt;
&lt;p&gt;As we&amp;rsquo;re using a Google Colab GPU, we can use a batch size of 16.&lt;/p&gt;
&lt;p&gt;The 
&lt;a href=&#34;https://github.com/google-research/bert&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;BERT Github page/publication&lt;/a&gt;
 has information on other Batch Sizes and Learning Rates to go with it.&lt;/p&gt;
&lt;p&gt;If you want to use a GPU yourself, in your Google Colab notebook, navigate to &lt;code&gt;Runtime --&amp;gt; Change Runtime Type --&amp;gt; GPU&lt;/code&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;batch_size = 16

dataloader_train = DataLoader(
    dataset_train,
    sampler=RandomSampler(dataset_train), #Randomly train on data, so we don&#39;t bias training
    batch_size=batch_size
)

dataloader_val = DataLoader(
    dataset_validation,
    sampler=SequentialSampler(dataset_validation), #Sequential sampling on validation data so we can tie results to original dataframe
    batch_size=batch_size
)
&lt;/code&gt;&lt;/pre&gt;
&lt;h1&gt;&lt;a id=&#34;optimise&#34;&gt;Task 8: Setting Up Optimizer and Scheduler&lt;/a&gt;&lt;/h1&gt;
&lt;p&gt;Optimisers are a set of algorithms responsible for changing attributes of a neural network such as its weights and learning rates in order to reduce loss. They tune hyperparameters per epoch.&lt;/p&gt;
&lt;p&gt;We use the AdamW optimiser and a learning rate of 1e-5. Again, the BERT paper recommends a set of learning rates and finding the best one for your particular fine-tuning task is a matter of trial and error.&lt;/p&gt;
&lt;p&gt;In our scheduler, we can specify whether we&amp;rsquo;d want to include warm-up steps, as well as the number of total training steps we&amp;rsquo;re undertaking. Since an epoch is one full pass over the entire training data-set, our value for training steps is the length of a batch * the number of epochs.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from transformers import AdamW, get_linear_schedule_with_warmup
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;optimizer = AdamW(
    model.parameters(),
    lr=1e-5, #2e-5 &amp;gt; 5e-5: A HYPERPARAMETER
    eps=1e-8
)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;epochs=6 

scheduler = get_linear_schedule_with_warmup(
    optimizer,
    num_warmup_steps=0,
    num_training_steps=len(dataloader_train) * epochs
)
&lt;/code&gt;&lt;/pre&gt;
&lt;h1&gt;&lt;a id=&#34;performancemetrics&#34;&gt;Task 9: Defining our Performance Metrics&lt;/a&gt;&lt;/h1&gt;
&lt;p&gt;Prior to our training pass, let&amp;rsquo;s first define useful helper functions to calculate certain metrics like multi-class accuracy and F1 score.&lt;/p&gt;
&lt;p&gt;We also include a &lt;code&gt;softmax()&lt;/code&gt; function to normalise the predictions generated later, and an &lt;code&gt;emotion_prediction()&lt;/code&gt; function to convert the normalised predictions to an emotion category.&lt;/p&gt;
&lt;p&gt;The accuracy metric was modified from 
&lt;a href=&#34;https://mccormickml.com/2019/07/22/BERT-fine-tuning/#41-bertforsequenceclassification&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;this tutorial&lt;/a&gt;
.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;import numpy as np
from sklearn.metrics import f1_score
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;def f1_score_func(preds, labels):
    &amp;quot;&amp;quot;&amp;quot;
    Helper function for calculating F1-score between predicted and true values
    &amp;quot;&amp;quot;&amp;quot;
    preds_flat = np.argmax(preds, axis=1).flatten() #why flatten? we dont want a list of lists, we just want a single array
    return f1_score(labels, preds_flat, average=&#39;weighted&#39;)#weights classes according to its distribution. disgust with 6 classes is downweighted
    #weighted vs macro 
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;def accuracy_per_class(preds, labels):
    &amp;quot;&amp;quot;&amp;quot;
    Helper function for calculating the accuracy per class and displaying it
    Modified for sentiment Analysis. Not using emotion analysis code
    &amp;quot;&amp;quot;&amp;quot;
    preds_flat = np.argmax(preds, axis=1).flatten()
    
    for label in np.unique(labels):
        y_preds = preds_flat[labels==label]
        y_true = labels[labels==label]
        print(f&#39;Class: {label_dict_inverse[label]}&#39;)
        print(f&#39;Accuracy: {len(y_preds[y_preds==label])}/{len(y_true[y_true==label])} in percentage: {len(y_preds[y_preds==label])/len(y_true[y_true==label])}&#39;)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;def softmax(matrix):
    &amp;quot;&amp;quot;&amp;quot;
    A function to normalise row values of a matrix to 1.0
    @param matrix - a numpy matrix which has non-normalised values per row
    @returns - the matrix with values all normalised to 1.0
    &amp;quot;&amp;quot;&amp;quot;
    return (np.exp(matrix.T) / np.sum(np.exp(matrix), axis=1)).T
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;def emotion_prediction(normalised_matrix):
    &amp;quot;&amp;quot;&amp;quot;
    A function to grab the dominant class (i.e. the prediction)
    @param normalised_matrix - a numpy matrix, which has normalised values per row, achieved
    from applying an activation function
    &amp;quot;&amp;quot;&amp;quot;
    return np.argmax(normalised_matrix, axis=1).flatten()
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We provide a dictionary that maps the raw prediction outputs, which would be numbers, to the emotion categories in words.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;label_dict_inverse = {v:k for (k,v) in label_dict.items()}
&lt;/code&gt;&lt;/pre&gt;
&lt;h1&gt;&lt;a id=&#34;trainevalloops&#34;&gt;Task 10: Creating our Training and Evaluation Loops&lt;/a&gt;&lt;/h1&gt;
&lt;p&gt;Our Approach for training was adapted from an older version of HuggingFace&amp;rsquo;s &lt;code&gt;run_glue.py&lt;/code&gt; script accessible 
&lt;a href=&#34;https://github.com/huggingface/transformers/blob/5bfcd0485ece086ebcbed2d008813037968a9e58/examples/run_glue.py#L128&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;here&lt;/a&gt;
 and recommended by the HuggingFace team.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;#again we set a seed value of 17 to make our training loop reproducible

import random

seed_val = 17 #so our results/process is reproducible by whoever wants to reproduce
random.seed(seed_val)
np.random.seed(seed_val)
torch.manual_seed(seed_val)
torch.cuda.manual_seed_all(seed_val) #include for when using a GPU
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;As PyTorch code is GPU compatible, we have to explicitly specify what device type we&amp;rsquo;re working from.&lt;/p&gt;
&lt;p&gt;We eventually have to transfer our model and data-structures onto this device type later on.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;#to check GPU vs CPU
device = torch.device(&#39;cuda&#39; if torch.cuda.is_available() else &#39;cpu&#39;)
model.to(device)

print(device)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;cuda
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We then write our code for evaluating the validation data-set in the &lt;code&gt;evaluate()&lt;/code&gt; function below before we write the code for our training data-set. This is because we use &lt;code&gt;evaluate()&lt;/code&gt; in our training loop.&lt;/p&gt;
&lt;p&gt;&lt;code&gt;evaluate()&lt;/code&gt; and our training loop right below it have very similar structures. We point out the differences later on.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;#quite similar to training, except for the differences mentioned below
def evaluate(dataloader_val):

    model.eval()
    
    loss_val_total = 0
    predictions, true_vals = [], []
    
    for batch in tqdm(dataloader_val):
        
        batch = tuple(b.to(device) for b in batch)
        
        inputs = {&#39;input_ids&#39;:      batch[0],
                  &#39;attention_mask&#39;: batch[1],
                  &#39;labels&#39;:         batch[2],
                 }
        #ignore/disable gradients
        with torch.no_grad():        
            outputs = model(**inputs)
            
        loss = outputs[0]
        logits = outputs[1]
        loss_val_total += loss.item()

        #detach from CPU means pulling values out of GPU to CPU
        #so we can use numpy, which works only on CPU
        logits = logits.detach().cpu().numpy()
        label_ids = inputs[&#39;labels&#39;].cpu().numpy()
        predictions.append(logits)
        true_vals.append(label_ids)
    
    loss_val_avg = loss_val_total/len(dataloader_val) 
    
    predictions = np.concatenate(predictions, axis=0)
    true_vals = np.concatenate(true_vals, axis=0)
            
    return loss_val_avg, predictions, true_vals
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We wrap our training loop below into a &lt;code&gt;tqdm()&lt;/code&gt; object to display a progress bar&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;#our training loop!
for epoch in tqdm(range(1, epochs+1)):
    model.train()
    
    #set to 0 initially, then add each batch&#39;s loss iteratively
    loss_train_total = 0
    
    progress_bar = tqdm(dataloader_train, 
                        desc=&#39;Epoch {:1d}&#39;.format(epoch),
                        leave=False, #let it overwrite after each epoch
                        disable=False, 
                       )
    for batch in progress_bar:
        
        #first batch = set gradients to 0
        model.zero_grad()
        
        #dataloader has 3 variables. so it&#39;s going to be a tuple of 3 items. We make sure each item is on the correct device
        batch = tuple(b.to(device) for b in batch)
        
        inputs = {
            &#39;input_ids&#39; : batch[0],
            &#39;attention_mask&#39; : batch[1],
            &#39;labels&#39; : batch[2]
        }
        
        #unpacks dictionary straight into model
        outputs = model(**inputs)
        
        #bert model returns loss and logits
        loss = outputs[0]
        loss_train_total += loss.item() #add up loss
        loss.backward() #backpropagate
        
        #all weights will be a norm of 1 (normalised weights)
        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)
        
        optimizer.step()
        scheduler.step()
        
        #update progress bar to display loss per batch
        progress_bar.set_postfix({&#39;training_loss&#39; : &#39;{:3f}&#39;.format(loss.item()/len(batch))})
        
    #outside the batch loop and inside the epoch loop, so per epoch
    #save model checkpoint and print progress
    torch.save(model.state_dict(), f&#39;Epoch-{epoch}.model&#39;)
    
    tqdm.write(f&#39;\nEpoch {epoch}&#39;)
    
    loss_train_avg = loss_train_total/len(dataloader_train)
    #loss per epoch:
    tqdm.write(f&#39;Training loss: {loss_train_avg}&#39;)
    
    #to detect overtraining - happens when training loss goes down and val loss goes up. Starts to
    #train perfectly on our data such that its no longer generalisable
    val_loss, predictions, true_vals = evaluate(dataloader_val) #predictions are the logits

    val_f1 = f1_score_func(predictions, true_vals)
    tqdm.write(f&#39;Validation loss: {val_loss}&#39;)
    tqdm.write(f&#39;F1 Score (weighted): {val_f1}&#39;)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;HBox(children=(FloatProgress(value=0.0, max=6.0), HTML(value=&#39;&#39;)))



HBox(children=(FloatProgress(value=0.0, description=&#39;Epoch 1&#39;, max=159.0, style=ProgressStyle(description_widt…



Epoch 1
Training loss: 0.5584828776524127



HBox(children=(FloatProgress(value=0.0, max=32.0), HTML(value=&#39;&#39;)))



Validation loss: 0.2766137867583893
F1 Score (weighted): 0.8965437215084957



HBox(children=(FloatProgress(value=0.0, description=&#39;Epoch 2&#39;, max=159.0, style=ProgressStyle(description_widt…



Epoch 2
Training loss: 0.24347842407980994



HBox(children=(FloatProgress(value=0.0, max=32.0), HTML(value=&#39;&#39;)))



Validation loss: 0.1528040450939443
F1 Score (weighted): 0.9427103377445136



HBox(children=(FloatProgress(value=0.0, description=&#39;Epoch 3&#39;, max=159.0, style=ProgressStyle(description_widt…



Epoch 3
Training loss: 0.1628657704359799



HBox(children=(FloatProgress(value=0.0, max=32.0), HTML(value=&#39;&#39;)))



Validation loss: 0.07538615397061221
F1 Score (weighted): 0.9586534476712678



HBox(children=(FloatProgress(value=0.0, description=&#39;Epoch 4&#39;, max=159.0, style=ProgressStyle(description_widt…



Epoch 4
Training loss: 0.10585488408142056



HBox(children=(FloatProgress(value=0.0, max=32.0), HTML(value=&#39;&#39;)))



Validation loss: 0.06744529563729884
F1 Score (weighted): 0.9632545931758529



HBox(children=(FloatProgress(value=0.0, description=&#39;Epoch 5&#39;, max=159.0, style=ProgressStyle(description_widt…



Epoch 5
Training loss: 0.08138270038853651



HBox(children=(FloatProgress(value=0.0, max=32.0), HTML(value=&#39;&#39;)))



Validation loss: 0.048784602870000526
F1 Score (weighted): 0.9769995855781185



HBox(children=(FloatProgress(value=0.0, description=&#39;Epoch 6&#39;, max=159.0, style=ProgressStyle(description_widt…



Epoch 6
Training loss: 0.0737762286301421



HBox(children=(FloatProgress(value=0.0, max=32.0), HTML(value=&#39;&#39;)))



Validation loss: 0.04574976687581511
F1 Score (weighted): 0.9864752200092636
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;What our training loop has that our &lt;code&gt;evaluate()&lt;/code&gt; function doesn&amp;rsquo;t have:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;The ability to backpropagate&lt;/li&gt;
&lt;li&gt;The ability to monitor training and validation loss per epoch&lt;/li&gt;
&lt;li&gt;The ability to save a trained model as a checkpoint&lt;/li&gt;
&lt;/ul&gt;
&lt;h1&gt;&lt;a id=&#34;evaluatemodel&#34;&gt;Task 11: Loading and Evaluating our Model&lt;/a&gt;&lt;/h1&gt;
&lt;p&gt;During training, we have saved our trained model parameters with a &lt;code&gt;.model&lt;/code&gt; extension in our current directory.&lt;/p&gt;
&lt;p&gt;The 6th Epoch of training lead to the highest weighted macro F1 score of &lt;strong&gt;0.986&lt;/strong&gt;! The training and validation losses in this epoch were 0.07 and 0.05, respectively.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;This is a STAGGERING result&lt;/strong&gt;, and highlights the power of transfer learning models.&lt;/p&gt;
&lt;p&gt;As the training and validation losses are similar in magnitude and our validation loss hasn&amp;rsquo;t plateaued, we don&amp;rsquo;t seem to have overfit our model to the training data! Yay!&lt;/p&gt;
&lt;p&gt;Now let&amp;rsquo;s load a fresh BERT Model, load our 6th Epoch model checkpoints onto it, evaluate our validation data-set again with &lt;code&gt;evaluate()&lt;/code&gt; then look closer at accuracies. We ignore the messages it generates.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;#fresh model
model = BertForSequenceClassification.from_pretrained(&amp;quot;bert-base-uncased&amp;quot;,
                                                      num_labels=len(label_dict),
                                                      output_attentions=False,
                                                      output_hidden_states=False)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: [&#39;cls.predictions.bias&#39;, &#39;cls.predictions.transform.dense.weight&#39;, &#39;cls.predictions.transform.dense.bias&#39;, &#39;cls.predictions.decoder.weight&#39;, &#39;cls.seq_relationship.weight&#39;, &#39;cls.seq_relationship.bias&#39;, &#39;cls.predictions.transform.LayerNorm.weight&#39;, &#39;cls.predictions.transform.LayerNorm.bias&#39;]
- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model).
- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: [&#39;classifier.weight&#39;, &#39;classifier.bias&#39;]
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;#pass on the fresh model to the correct device, either GPU or CPU
model.to(device)
pass #so we dont have all that text printed out
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# cuda indicates a GPU is available. Replace with &#39;cpu&#39; when using a cpu.
model.load_state_dict(
    torch.load(&#39;Epoch-6.model&#39;,
              map_location=torch.device(&#39;cuda&#39;)))
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;&amp;lt;All keys matched successfully&amp;gt;
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;#grabbing predictions from validation data-set
_, predictions_val, labels_val = evaluate(dataloader_val)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;HBox(children=(FloatProgress(value=0.0, max=16.0), HTML(value=&#39;&#39;)))
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;code&gt;evaluate()&lt;/code&gt; generated a matrix of predictions for the validation data-set of with the dimensions below: 254 rows, each corresponding to a tweet in the validation data-set and 5 columns, each corresponding to an emotion category.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;predictions_val.shape
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;(254, 5)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;#looking at the fifth example in the predictions matrix
predictions_val[4]
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;array([ 6.018326 , -1.8091979, -2.1678815, -1.2804692, -2.0098157],
      dtype=float32)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Looking at the fifth example in the prediction matrix, we see that the values for each of the 5 columns aren&amp;rsquo;t normalised to 1.0. Let&amp;rsquo;s normalise it with an activation function, 
&lt;a href=&#34;http://rasbt.github.io/mlxtend/user_guide/classifier/SoftmaxRegression/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;the softmax&lt;/a&gt;
&lt;/p&gt;
&lt;p&gt;The results of this normalisation for each category is the probability that the sentence belongs to that category.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;#grab predictions variable here and do a softmax, to visualise results against df
percent_emotions_val = softmax(predictions_val)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;As we see below, the fifth example&amp;rsquo;s most probable category is the first column (whatever it is). The probability that it belongs to the category represented by this column is 99%!&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;percent_emotions_val[4]
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;array([9.9832332e-01, 3.9794299e-04, 2.7800113e-04, 6.7521929e-04,
       3.2560693e-04], dtype=float32)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We then determine the actual emotion category by running it through one final function, which takes the softmax-ed predictions matrix and grabs the most probable emotion label:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;#from soft-maxed probabilities of emotions to picking the most dominant emotion
emotions_val = emotion_prediction(percent_emotions_val)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The output is naturally a number (as we trained it with numbers!), so we pass in the inverse dictionary that maps the number to its string category. We see that the first tweet was predicted as a happy one:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;label_dict_inverse[emotions_val[4]]
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;&#39;happy&#39;
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;df_train
&lt;/code&gt;&lt;/pre&gt;
&lt;div&gt;
&lt;style scoped&gt;
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }
&lt;pre&gt;&lt;code&gt;.dataframe tbody tr th {
    vertical-align: top;
}

.dataframe thead th {
    text-align: right;
}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;/style&gt;&lt;/p&gt;
&lt;table border=&#34;1&#34; class=&#34;dataframe&#34;&gt;
  &lt;thead&gt;
    &lt;tr style=&#34;text-align: right;&#34;&gt;
      &lt;th&gt;&lt;/th&gt;
      &lt;th&gt;entry&lt;/th&gt;
      &lt;th&gt;emotion&lt;/th&gt;
      &lt;th&gt;label&lt;/th&gt;
      &lt;th&gt;data_type&lt;/th&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;id&lt;/th&gt;
      &lt;th&gt;&lt;/th&gt;
      &lt;th&gt;&lt;/th&gt;
      &lt;th&gt;&lt;/th&gt;
      &lt;th&gt;&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;th&gt;614484565059596288&lt;/th&gt;
      &lt;td&gt;Dorian Gray with Rainbow Scarf LoveWins from&lt;/td&gt;
      &lt;td&gt;happy&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;val&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;614746522043973632&lt;/th&gt;
      &lt;td&gt;_StIves . Replace with your wish which the art...&lt;/td&gt;
      &lt;td&gt;happy&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;val&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;614877582664835073&lt;/th&gt;
      &lt;td&gt;thank you for following me back. Great to hear...&lt;/td&gt;
      &lt;td&gt;happy&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;train&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;611932373039644672&lt;/th&gt;
      &lt;td&gt;What a beautiful jewel portrait. Is the R for ...&lt;/td&gt;
      &lt;td&gt;happy&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;train&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;611570404268883969&lt;/th&gt;
      &lt;td&gt;I have always loved this painting.&lt;/td&gt;
      &lt;td&gt;happy&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;train&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;...&lt;/th&gt;
      &lt;td&gt;...&lt;/td&gt;
      &lt;td&gt;...&lt;/td&gt;
      &lt;td&gt;...&lt;/td&gt;
      &lt;td&gt;...&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;614053885733412864&lt;/th&gt;
      &lt;td&gt;Good to see s art collection Thx to _StIves _r...&lt;/td&gt;
      &lt;td&gt;happy&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;train&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;610405281604993024&lt;/th&gt;
      &lt;td&gt;thanks we will have a look next week after Fri...&lt;/td&gt;
      &lt;td&gt;happy&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;train&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;612214539468279808&lt;/th&gt;
      &lt;td&gt;Thanks for ranking us 1 in things to do in Lon...&lt;/td&gt;
      &lt;td&gt;happy&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;train&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;613678555935973376&lt;/th&gt;
      &lt;td&gt;MT Looking forward to our public engagement ev...&lt;/td&gt;
      &lt;td&gt;happy&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;train&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;615246897670922240&lt;/th&gt;
      &lt;td&gt;Mesmerising.&lt;/td&gt;
      &lt;td&gt;happy&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;train&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;1267 rows × 4 columns&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;Let&amp;rsquo;s have a look at the actual tweet and it&amp;rsquo;s actual label though:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;print(f&amp;quot;Tweet: {df_train[df_train.data_type==&#39;val&#39;].iloc[4,0]}\noriginal label: {df_train[df_train.data_type==&#39;val&#39;].iloc[4,1]}\npredicted label: {label_dict_inverse[emotions_val[4]]}&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;Tweet: Wonderful experience hearing Tim Knoxs objects2015 keynote on contents decor of UK country houses in gallery 3 of _UK!
original label: happy
predicted label: happy
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Let&amp;rsquo;s do the above process for all tweets in our validation data-set, and officially calculate accuracy metrics for each emotion category&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;#calculating accuracy per class
print(&#39;Accuracy per class of val dataset:\n&#39;)
accuracy_per_class(predictions_val, labels_val)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;Accuracy per class of val dataset:

Class: happy
Accuracy: 228/228 in percentage: 1.0
Class: angry
Accuracy: 12/12 in percentage: 1.0
Class: disgust
Accuracy: 0/1 in percentage: 0.0
Class: sad
Accuracy: 4/6 in percentage: 0.6666666666666666
Class: surprise
Accuracy: 7/7 in percentage: 1.0
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We see above that the model performed badly for the disgust category, miss-classifying the only disgust entry in the validation data-set as something else. The rest of the categories were predicted accurately, with stunning accuracy scores ranging from 60% to 100%!&lt;/p&gt;
&lt;p&gt;We then look at the macro-weighted F1 score over our entire validation data-set. This should match the output of our training loop above (i.e. Epoch 6&amp;rsquo;s validation data loss should be 0.98)&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;#f1 score overall
print(&#39;Weighted F1 score of val dataset:&#39;)
print(f1_score_func(predictions_val, labels_val))
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;Weighted F1 score of val dataset:
0.9864752200092636
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Overall, a very pleasing result :)&lt;/p&gt;
&lt;p&gt;As we saw, we achieved highly accurate results leveraging the knowledge of generalist BERT for our specialist task of classifying emotions in tweets. We fine-tuned on a single GPU in less than 30 minutes to achieve 98.6% accuracy, a very impressive feat that&amp;rsquo;s impossible without transformers!&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Closing Remarks:&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;If we wanted to improve the accuracy of the sad/disgust categories, we can consider resampling categories so the numbers of all categories are more balanced&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;The following links helped greatly in building this notebook:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;a href=&#34;http://jalammar.github.io/illustrated-bert/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Jay Alammar&amp;rsquo;s illustration-based explanation of the BERT architecture&lt;/a&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;a href=&#34;http://jalammar.github.io/a-visual-guide-to-using-bert-for-the-first-time/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Jay Alammar&amp;rsquo;s visual exploration of BERT embeddings&lt;/a&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;a href=&#34;http://mccormickml.com/2019/07/22/BERT-fine-tuning/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Chris McCormick&amp;rsquo;s BERT-Fine Tuning with PyTorch&lt;/a&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Stay tuned for further posts on how I use BERT for topic modeling, building a chatbot, and deploying a BERT model onto the cloud!&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
    <item>
      <title>Example Page 1</title>
      <link>https://nixramirez.github.io/courses/example/example1/</link>
      <pubDate>Sun, 05 May 2019 00:00:00 +0100</pubDate>
      <guid>https://nixramirez.github.io/courses/example/example1/</guid>
      <description>&lt;p&gt;In this tutorial, I&amp;rsquo;ll share my top 10 tips for getting started with Academic:&lt;/p&gt;
&lt;h2 id=&#34;tip-1&#34;&gt;Tip 1&lt;/h2&gt;
&lt;p&gt;Lorem ipsum dolor sit amet, consectetur adipiscing elit. Duis posuere tellus ac convallis placerat. Proin tincidunt magna sed ex sollicitudin condimentum. Sed ac faucibus dolor, scelerisque sollicitudin nisi. Cras purus urna, suscipit quis sapien eu, pulvinar tempor diam. Quisque risus orci, mollis id ante sit amet, gravida egestas nisl. Sed ac tempus magna. Proin in dui enim. Donec condimentum, sem id dapibus fringilla, tellus enim condimentum arcu, nec volutpat est felis vel metus. Vestibulum sit amet erat at nulla eleifend gravida.&lt;/p&gt;
&lt;p&gt;Nullam vel molestie justo. Curabitur vitae efficitur leo. In hac habitasse platea dictumst. Sed pulvinar mauris dui, eget varius purus congue ac. Nulla euismod, lorem vel elementum dapibus, nunc justo porta mi, sed tempus est est vel tellus. Nam et enim eleifend, laoreet sem sit amet, elementum sem. Morbi ut leo congue, maximus velit ut, finibus arcu. In et libero cursus, rutrum risus non, molestie leo. Nullam congue quam et volutpat malesuada. Sed risus tortor, pulvinar et dictum nec, sodales non mi. Phasellus lacinia commodo laoreet. Nam mollis, erat in feugiat consectetur, purus eros egestas tellus, in auctor urna odio at nibh. Mauris imperdiet nisi ac magna convallis, at rhoncus ligula cursus.&lt;/p&gt;
&lt;p&gt;Cras aliquam rhoncus ipsum, in hendrerit nunc mattis vitae. Duis vitae efficitur metus, ac tempus leo. Cras nec fringilla lacus. Quisque sit amet risus at ipsum pharetra commodo. Sed aliquam mauris at consequat eleifend. Praesent porta, augue sed viverra bibendum, neque ante euismod ante, in vehicula justo lorem ac eros. Suspendisse augue libero, venenatis eget tincidunt ut, malesuada at lorem. Donec vitae bibendum arcu. Aenean maximus nulla non pretium iaculis. Quisque imperdiet, nulla in pulvinar aliquet, velit quam ultrices quam, sit amet fringilla leo sem vel nunc. Mauris in lacinia lacus.&lt;/p&gt;
&lt;p&gt;Suspendisse a tincidunt lacus. Curabitur at urna sagittis, dictum ante sit amet, euismod magna. Sed rutrum massa id tortor commodo, vitae elementum turpis tempus. Lorem ipsum dolor sit amet, consectetur adipiscing elit. Aenean purus turpis, venenatis a ullamcorper nec, tincidunt et massa. Integer posuere quam rutrum arcu vehicula imperdiet. Mauris ullamcorper quam vitae purus congue, quis euismod magna eleifend. Vestibulum semper vel augue eget tincidunt. Fusce eget justo sodales, dapibus odio eu, ultrices lorem. Duis condimentum lorem id eros commodo, in facilisis mauris scelerisque. Morbi sed auctor leo. Nullam volutpat a lacus quis pharetra. Nulla congue rutrum magna a ornare.&lt;/p&gt;
&lt;p&gt;Aliquam in turpis accumsan, malesuada nibh ut, hendrerit justo. Cum sociis natoque penatibus et magnis dis parturient montes, nascetur ridiculus mus. Quisque sed erat nec justo posuere suscipit. Donec ut efficitur arcu, in malesuada neque. Nunc dignissim nisl massa, id vulputate nunc pretium nec. Quisque eget urna in risus suscipit ultricies. Pellentesque odio odio, tincidunt in eleifend sed, posuere a diam. Nam gravida nisl convallis semper elementum. Morbi vitae felis faucibus, vulputate orci placerat, aliquet nisi. Aliquam erat volutpat. Maecenas sagittis pulvinar purus, sed porta quam laoreet at.&lt;/p&gt;
&lt;h2 id=&#34;tip-2&#34;&gt;Tip 2&lt;/h2&gt;
&lt;p&gt;Lorem ipsum dolor sit amet, consectetur adipiscing elit. Duis posuere tellus ac convallis placerat. Proin tincidunt magna sed ex sollicitudin condimentum. Sed ac faucibus dolor, scelerisque sollicitudin nisi. Cras purus urna, suscipit quis sapien eu, pulvinar tempor diam. Quisque risus orci, mollis id ante sit amet, gravida egestas nisl. Sed ac tempus magna. Proin in dui enim. Donec condimentum, sem id dapibus fringilla, tellus enim condimentum arcu, nec volutpat est felis vel metus. Vestibulum sit amet erat at nulla eleifend gravida.&lt;/p&gt;
&lt;p&gt;Nullam vel molestie justo. Curabitur vitae efficitur leo. In hac habitasse platea dictumst. Sed pulvinar mauris dui, eget varius purus congue ac. Nulla euismod, lorem vel elementum dapibus, nunc justo porta mi, sed tempus est est vel tellus. Nam et enim eleifend, laoreet sem sit amet, elementum sem. Morbi ut leo congue, maximus velit ut, finibus arcu. In et libero cursus, rutrum risus non, molestie leo. Nullam congue quam et volutpat malesuada. Sed risus tortor, pulvinar et dictum nec, sodales non mi. Phasellus lacinia commodo laoreet. Nam mollis, erat in feugiat consectetur, purus eros egestas tellus, in auctor urna odio at nibh. Mauris imperdiet nisi ac magna convallis, at rhoncus ligula cursus.&lt;/p&gt;
&lt;p&gt;Cras aliquam rhoncus ipsum, in hendrerit nunc mattis vitae. Duis vitae efficitur metus, ac tempus leo. Cras nec fringilla lacus. Quisque sit amet risus at ipsum pharetra commodo. Sed aliquam mauris at consequat eleifend. Praesent porta, augue sed viverra bibendum, neque ante euismod ante, in vehicula justo lorem ac eros. Suspendisse augue libero, venenatis eget tincidunt ut, malesuada at lorem. Donec vitae bibendum arcu. Aenean maximus nulla non pretium iaculis. Quisque imperdiet, nulla in pulvinar aliquet, velit quam ultrices quam, sit amet fringilla leo sem vel nunc. Mauris in lacinia lacus.&lt;/p&gt;
&lt;p&gt;Suspendisse a tincidunt lacus. Curabitur at urna sagittis, dictum ante sit amet, euismod magna. Sed rutrum massa id tortor commodo, vitae elementum turpis tempus. Lorem ipsum dolor sit amet, consectetur adipiscing elit. Aenean purus turpis, venenatis a ullamcorper nec, tincidunt et massa. Integer posuere quam rutrum arcu vehicula imperdiet. Mauris ullamcorper quam vitae purus congue, quis euismod magna eleifend. Vestibulum semper vel augue eget tincidunt. Fusce eget justo sodales, dapibus odio eu, ultrices lorem. Duis condimentum lorem id eros commodo, in facilisis mauris scelerisque. Morbi sed auctor leo. Nullam volutpat a lacus quis pharetra. Nulla congue rutrum magna a ornare.&lt;/p&gt;
&lt;p&gt;Aliquam in turpis accumsan, malesuada nibh ut, hendrerit justo. Cum sociis natoque penatibus et magnis dis parturient montes, nascetur ridiculus mus. Quisque sed erat nec justo posuere suscipit. Donec ut efficitur arcu, in malesuada neque. Nunc dignissim nisl massa, id vulputate nunc pretium nec. Quisque eget urna in risus suscipit ultricies. Pellentesque odio odio, tincidunt in eleifend sed, posuere a diam. Nam gravida nisl convallis semper elementum. Morbi vitae felis faucibus, vulputate orci placerat, aliquet nisi. Aliquam erat volutpat. Maecenas sagittis pulvinar purus, sed porta quam laoreet at.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Example Page 2</title>
      <link>https://nixramirez.github.io/courses/example/example2/</link>
      <pubDate>Sun, 05 May 2019 00:00:00 +0100</pubDate>
      <guid>https://nixramirez.github.io/courses/example/example2/</guid>
      <description>&lt;p&gt;Here are some more tips for getting started with Academic:&lt;/p&gt;
&lt;h2 id=&#34;tip-3&#34;&gt;Tip 3&lt;/h2&gt;
&lt;p&gt;Lorem ipsum dolor sit amet, consectetur adipiscing elit. Duis posuere tellus ac convallis placerat. Proin tincidunt magna sed ex sollicitudin condimentum. Sed ac faucibus dolor, scelerisque sollicitudin nisi. Cras purus urna, suscipit quis sapien eu, pulvinar tempor diam. Quisque risus orci, mollis id ante sit amet, gravida egestas nisl. Sed ac tempus magna. Proin in dui enim. Donec condimentum, sem id dapibus fringilla, tellus enim condimentum arcu, nec volutpat est felis vel metus. Vestibulum sit amet erat at nulla eleifend gravida.&lt;/p&gt;
&lt;p&gt;Nullam vel molestie justo. Curabitur vitae efficitur leo. In hac habitasse platea dictumst. Sed pulvinar mauris dui, eget varius purus congue ac. Nulla euismod, lorem vel elementum dapibus, nunc justo porta mi, sed tempus est est vel tellus. Nam et enim eleifend, laoreet sem sit amet, elementum sem. Morbi ut leo congue, maximus velit ut, finibus arcu. In et libero cursus, rutrum risus non, molestie leo. Nullam congue quam et volutpat malesuada. Sed risus tortor, pulvinar et dictum nec, sodales non mi. Phasellus lacinia commodo laoreet. Nam mollis, erat in feugiat consectetur, purus eros egestas tellus, in auctor urna odio at nibh. Mauris imperdiet nisi ac magna convallis, at rhoncus ligula cursus.&lt;/p&gt;
&lt;p&gt;Cras aliquam rhoncus ipsum, in hendrerit nunc mattis vitae. Duis vitae efficitur metus, ac tempus leo. Cras nec fringilla lacus. Quisque sit amet risus at ipsum pharetra commodo. Sed aliquam mauris at consequat eleifend. Praesent porta, augue sed viverra bibendum, neque ante euismod ante, in vehicula justo lorem ac eros. Suspendisse augue libero, venenatis eget tincidunt ut, malesuada at lorem. Donec vitae bibendum arcu. Aenean maximus nulla non pretium iaculis. Quisque imperdiet, nulla in pulvinar aliquet, velit quam ultrices quam, sit amet fringilla leo sem vel nunc. Mauris in lacinia lacus.&lt;/p&gt;
&lt;p&gt;Suspendisse a tincidunt lacus. Curabitur at urna sagittis, dictum ante sit amet, euismod magna. Sed rutrum massa id tortor commodo, vitae elementum turpis tempus. Lorem ipsum dolor sit amet, consectetur adipiscing elit. Aenean purus turpis, venenatis a ullamcorper nec, tincidunt et massa. Integer posuere quam rutrum arcu vehicula imperdiet. Mauris ullamcorper quam vitae purus congue, quis euismod magna eleifend. Vestibulum semper vel augue eget tincidunt. Fusce eget justo sodales, dapibus odio eu, ultrices lorem. Duis condimentum lorem id eros commodo, in facilisis mauris scelerisque. Morbi sed auctor leo. Nullam volutpat a lacus quis pharetra. Nulla congue rutrum magna a ornare.&lt;/p&gt;
&lt;p&gt;Aliquam in turpis accumsan, malesuada nibh ut, hendrerit justo. Cum sociis natoque penatibus et magnis dis parturient montes, nascetur ridiculus mus. Quisque sed erat nec justo posuere suscipit. Donec ut efficitur arcu, in malesuada neque. Nunc dignissim nisl massa, id vulputate nunc pretium nec. Quisque eget urna in risus suscipit ultricies. Pellentesque odio odio, tincidunt in eleifend sed, posuere a diam. Nam gravida nisl convallis semper elementum. Morbi vitae felis faucibus, vulputate orci placerat, aliquet nisi. Aliquam erat volutpat. Maecenas sagittis pulvinar purus, sed porta quam laoreet at.&lt;/p&gt;
&lt;h2 id=&#34;tip-4&#34;&gt;Tip 4&lt;/h2&gt;
&lt;p&gt;Lorem ipsum dolor sit amet, consectetur adipiscing elit. Duis posuere tellus ac convallis placerat. Proin tincidunt magna sed ex sollicitudin condimentum. Sed ac faucibus dolor, scelerisque sollicitudin nisi. Cras purus urna, suscipit quis sapien eu, pulvinar tempor diam. Quisque risus orci, mollis id ante sit amet, gravida egestas nisl. Sed ac tempus magna. Proin in dui enim. Donec condimentum, sem id dapibus fringilla, tellus enim condimentum arcu, nec volutpat est felis vel metus. Vestibulum sit amet erat at nulla eleifend gravida.&lt;/p&gt;
&lt;p&gt;Nullam vel molestie justo. Curabitur vitae efficitur leo. In hac habitasse platea dictumst. Sed pulvinar mauris dui, eget varius purus congue ac. Nulla euismod, lorem vel elementum dapibus, nunc justo porta mi, sed tempus est est vel tellus. Nam et enim eleifend, laoreet sem sit amet, elementum sem. Morbi ut leo congue, maximus velit ut, finibus arcu. In et libero cursus, rutrum risus non, molestie leo. Nullam congue quam et volutpat malesuada. Sed risus tortor, pulvinar et dictum nec, sodales non mi. Phasellus lacinia commodo laoreet. Nam mollis, erat in feugiat consectetur, purus eros egestas tellus, in auctor urna odio at nibh. Mauris imperdiet nisi ac magna convallis, at rhoncus ligula cursus.&lt;/p&gt;
&lt;p&gt;Cras aliquam rhoncus ipsum, in hendrerit nunc mattis vitae. Duis vitae efficitur metus, ac tempus leo. Cras nec fringilla lacus. Quisque sit amet risus at ipsum pharetra commodo. Sed aliquam mauris at consequat eleifend. Praesent porta, augue sed viverra bibendum, neque ante euismod ante, in vehicula justo lorem ac eros. Suspendisse augue libero, venenatis eget tincidunt ut, malesuada at lorem. Donec vitae bibendum arcu. Aenean maximus nulla non pretium iaculis. Quisque imperdiet, nulla in pulvinar aliquet, velit quam ultrices quam, sit amet fringilla leo sem vel nunc. Mauris in lacinia lacus.&lt;/p&gt;
&lt;p&gt;Suspendisse a tincidunt lacus. Curabitur at urna sagittis, dictum ante sit amet, euismod magna. Sed rutrum massa id tortor commodo, vitae elementum turpis tempus. Lorem ipsum dolor sit amet, consectetur adipiscing elit. Aenean purus turpis, venenatis a ullamcorper nec, tincidunt et massa. Integer posuere quam rutrum arcu vehicula imperdiet. Mauris ullamcorper quam vitae purus congue, quis euismod magna eleifend. Vestibulum semper vel augue eget tincidunt. Fusce eget justo sodales, dapibus odio eu, ultrices lorem. Duis condimentum lorem id eros commodo, in facilisis mauris scelerisque. Morbi sed auctor leo. Nullam volutpat a lacus quis pharetra. Nulla congue rutrum magna a ornare.&lt;/p&gt;
&lt;p&gt;Aliquam in turpis accumsan, malesuada nibh ut, hendrerit justo. Cum sociis natoque penatibus et magnis dis parturient montes, nascetur ridiculus mus. Quisque sed erat nec justo posuere suscipit. Donec ut efficitur arcu, in malesuada neque. Nunc dignissim nisl massa, id vulputate nunc pretium nec. Quisque eget urna in risus suscipit ultricies. Pellentesque odio odio, tincidunt in eleifend sed, posuere a diam. Nam gravida nisl convallis semper elementum. Morbi vitae felis faucibus, vulputate orci placerat, aliquet nisi. Aliquam erat volutpat. Maecenas sagittis pulvinar purus, sed porta quam laoreet at.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Example Talk</title>
      <link>https://nixramirez.github.io/talk/example/</link>
      <pubDate>Sat, 01 Jun 2030 13:00:00 +0000</pubDate>
      <guid>https://nixramirez.github.io/talk/example/</guid>
      <description>&lt;div class=&#34;alert alert-note&#34;&gt;
  &lt;div&gt;
    Click on the &lt;strong&gt;Slides&lt;/strong&gt; button above to view the built-in slides feature.
  &lt;/div&gt;
&lt;/div&gt;
&lt;p&gt;Slides can be added in a few ways:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Create&lt;/strong&gt; slides using Academic&amp;rsquo;s 
&lt;a href=&#34;https://sourcethemes.com/academic/docs/managing-content/#create-slides&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;em&gt;Slides&lt;/em&gt;&lt;/a&gt;
 feature and link using &lt;code&gt;slides&lt;/code&gt; parameter in the front matter of the talk file&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Upload&lt;/strong&gt; an existing slide deck to &lt;code&gt;static/&lt;/code&gt; and link using &lt;code&gt;url_slides&lt;/code&gt; parameter in the front matter of the talk file&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Embed&lt;/strong&gt; your slides (e.g. Google Slides) or presentation video on this page using 
&lt;a href=&#34;https://sourcethemes.com/academic/docs/writing-markdown-latex/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;shortcodes&lt;/a&gt;
.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Further talk details can easily be added to this page using &lt;em&gt;Markdown&lt;/em&gt; and $\rm \LaTeX$ math code.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Intro to Data Visualisation Workshop</title>
      <link>https://nixramirez.github.io/project/intro-to-data-visualisation/</link>
      <pubDate>Tue, 25 Aug 2020 00:00:00 +0000</pubDate>
      <guid>https://nixramirez.github.io/project/intro-to-data-visualisation/</guid>
      <description>&lt;p&gt;For the second semester of 2020, I had the privilege of teaching another workshop for the Data Science Club, this time covering Data Visualisation packages Matplotlib and Seaborn in Python.&lt;/p&gt;
&lt;p&gt;I co-taught the workshop with a fellow club executive member, Saahil, also another Data Science Master&amp;rsquo;s student from the University.&lt;/p&gt;
&lt;p&gt;In the workshop, we work through a data-set and explain syntax and customisation options for univariate (i.e. single variable) and multivariate (i.e. more than 1 variable) plotting tasks.&lt;/p&gt;
&lt;h2 id=&#34;watch-on-below&#34;&gt;Watch on below!&lt;/h2&gt;
&lt;iframe width=&#34;560&#34; height=&#34;315&#34; src=&#34;https://www.youtube.com/embed/E7XWgVBNzHA&#34; frameborder=&#34;0&#34; allow=&#34;accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture&#34; allowfullscreen&gt;&lt;/iframe&gt;
&lt;p&gt;&lt;strong&gt;The materials accompanying the workshop can be 
&lt;a href=&#34;https://drive.google.com/drive/folders/1qe5NwgELEvr4JrFwuZAiAAVS2rabnISp?usp=sharing&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;downloaded from here&lt;/a&gt;
&lt;/strong&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Assessing Classification Models</title>
      <link>https://nixramirez.github.io/project/classifier-diagnostics/</link>
      <pubDate>Mon, 08 Jun 2020 00:00:00 +0000</pubDate>
      <guid>https://nixramirez.github.io/project/classifier-diagnostics/</guid>
      <description>&lt;p&gt;&lt;strong&gt;TL;DR:&lt;/strong&gt; In applied machine learning, models are often optimised for their accuracy in predicting a target. When the target is quantitative, measures like the R^2 value and the Root Mean Squared Error (RMSE) etc., help in quantifying this accuracy. When the target is qualitative, metrics like Accuracy, Precision, Recall and several others are used. We term these measures &amp;lsquo;classification metrics&amp;rsquo; for the sake of this blog post.&lt;/p&gt;
&lt;p&gt;In this post, I explain classification metrics and provide python code to calculate them. I also produce a report including them that can be easily saved to a file for later use.&lt;/p&gt;
&lt;p&gt;Say you&amp;rsquo;ve run your data through a classifier, and the classifier has predicted labels for your data. To judge how accurate those predictions are, you can evaluate them by the metrics explained below&lt;/p&gt;
&lt;h3 id=&#34;section-1-classification-metrics&#34;&gt;Section 1: Classification Metrics&lt;/h3&gt;
&lt;p&gt;The metrics covered by this post is as follows:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Accuracy&lt;/strong&gt; - What proportion of sentences were correctly classified (i.e. as positive, negative, neutral)?&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Precision&lt;/strong&gt; - Answers the q: What proportion of &lt;em&gt;predicted&lt;/em&gt; positives were actual positives? The polar opposite for this is specificity:
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Specificity&lt;/strong&gt; - Answers the q: What proportion of &lt;em&gt;predicted&lt;/em&gt; negatives were actual negatives?&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Recall/Sensitivity&lt;/strong&gt; - Answers the q: What proportion of &lt;em&gt;actual&lt;/em&gt; positives were predicted positive?&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;F-1 Score&lt;/strong&gt; - The harmonic mean of a classifier&amp;rsquo;s precision and recall scores&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Generally, when someone mentions the last 3 metrics above, without a prefix, they mean them to be the &lt;em&gt;macro-precision&lt;/em&gt;, &lt;em&gt;macro-recall&lt;/em&gt;, and &lt;em&gt;macro-F1&lt;/em&gt;. These metrics come in two other flavours however, the micro- and the weighted. Let&amp;rsquo;s take the example of the precision score (but the exact same descriptions can be said for recall and the F-1 score). Whilst the macro precision is simply the average of the precision scores calculated for each class, the weighted precision weights this score by the number of examples of that class before averaging across classes. This is why macro precision is a better metric for balanced data-sets while weighted precision is better for imbalanced ones. Micro precision is more involved than both, and is explained later on.&lt;/p&gt;
&lt;p&gt;The above metrics can be calculated from a &lt;strong&gt;confusion matrix&lt;/strong&gt; which is a 2D matrix showing the predicted and actual counts of a target category, therefore showing the distribution of True Positives (TPs), False Positives (FPs), True Negatives (TNs), and False Negatives (FNs) in your data.&lt;/p&gt;
&lt;p&gt;As we see in Section 1.1 below, the equations of the above metrics require knowledge of the TP/FPs and TN/FNs. Calculating the above metrics by hand therefore requires a confusion matrix to be made beforehand.&lt;/p&gt;
&lt;h4 id=&#34;section-11-diving-deeper-into-the-metrics&#34;&gt;Section 1.1: Diving Deeper into the Metrics&lt;/h4&gt;
&lt;p&gt;Let&amp;rsquo;s display a confusion matrix for a binary classification (i.e. 2 classes for the target) problem to intuitively get a grasp of its contents.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;CM1.png&#34; alt=&#34;Confusion Matrix&#34;&gt;&lt;/p&gt;
&lt;p&gt;The goal for a classifier is to increase TP/TNs while reducing misclassification, or the number of FP/FNs&lt;/p&gt;
&lt;p&gt;Knowing the above, our metrics can be represented as the following equations:
$$
Accuracy = \frac{TP + TN}{FP + FN + TP + TN}
$$&lt;/p&gt;
&lt;p&gt;$$
Precision = \frac{TP}{TP + FP}
$$&lt;/p&gt;
&lt;p&gt;$$
Recall = \frac{TP}{TP + FN}
$$&lt;/p&gt;
&lt;p&gt;$$
F_1= \frac{2}{\frac{1}{Precision} \times \frac{1}{Recall}} = 2 \times \frac{Recall \times Precision}{Recall + Precision}
$$&lt;/p&gt;
&lt;h4 id=&#34;section-12-diving-deeper-into-various-flavours-of-precision&#34;&gt;Section 1.2: Diving Deeper into various flavours of Precision&lt;/h4&gt;
&lt;p&gt;Shown above is the equation for the precision score of &lt;strong&gt;one class&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;The macro and weighted precision scores for a binary classification (i.e. with 2 classes) can be represented as:&lt;/p&gt;
&lt;p&gt;$$
Macro Precision= \frac{Precision_{class1} + Precision_{class2}}{2}
$$&lt;/p&gt;
&lt;p&gt;$$
Weighted Precision= \frac{(Precision_{class1} \times c_1) + (Precision_{class2} \times c_2)}{c_1 + c_2}
$$&lt;/p&gt;
&lt;p&gt;where $c_1$ and $c_2$ are the number of examples in classes $1$ and $2$&lt;/p&gt;
&lt;p&gt;Note that if we replace &lt;code&gt;Precision&lt;/code&gt; above with &lt;code&gt;F-1 score&lt;/code&gt; or &lt;code&gt;Recall&lt;/code&gt;, the formulas would still work!&lt;/p&gt;
&lt;p&gt;To calculate micro precision, instead of taking measurements of precision &lt;strong&gt;for each class&lt;/strong&gt; (i.e. taking TPs and FPs for each class), we compute TPs and FPs out of the &lt;strong&gt;total pool&lt;/strong&gt; of examples.&lt;/p&gt;
&lt;p&gt;Take the confusion matrix below for another binary classification problem, classifying photos of a cat or a dog.
&lt;img src=&#34;confusion_matrix.png&#34; alt=&#34;confusion matrix 2&#34;&gt;&lt;/p&gt;
&lt;p&gt;We calculate the TP and FP out of the total number of examples as follows:
&lt;img src=&#34;TP+FP.png&#34; alt=&#34;TP+FP&#34;&gt;&lt;/p&gt;
&lt;p&gt;And therefore our micro precision is:
&lt;img src=&#34;microprecision.png&#34; alt=&#34;microprecision&#34;&gt;&lt;/p&gt;
&lt;h4 id=&#34;section-13-the-rocauc-scores&#34;&gt;Section 1.3: The ROC/AUC scores&lt;/h4&gt;
&lt;p&gt;In a later blog post, we&amp;rsquo;ll explore two additional metrics briefly described below, as they are derived simultaneously while training a machine learning model.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;ROC curve&lt;/strong&gt; - Shows how changing the 
&lt;a href=&#34;https://developers.google.com/machine-learning/crash-course/classification/thresholding&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;classification threshold&lt;/a&gt;
 changes the True Positive Rate and False Positive Rate. In a binary classification problem, the classification threshold is a probability value that delineates between two classes, and is a hyperparameter we have to optimise for. Why is it a probability value? Well that&amp;rsquo;s because classifiers return probability values before we associate a category to those values based on our interpretation of them! Say, for one image classified by our Dog vs. Cat image classifier above, the resulting probabilities are 0.95 that it&amp;rsquo;s a dog and 0.05 that it&amp;rsquo;s a cat. The image is &lt;strong&gt;likely&lt;/strong&gt; a dog&amp;hellip;but what if we had a second image where the probability of it being a dog was 0.6? The &lt;strong&gt;classification threshold&lt;/strong&gt; we set determines what class this second image belongs to, and if we set it to be 0.6, then our image &lt;em&gt;just barely made it to the threshold of being a dog image&lt;/em&gt;. Probabilities below 0.6 will classify it as a cat. You can see how changing this threshold might change the number of TPs/FPs and TNs/FNs in our data. See the image below for a visual depiction of this curve.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;AUC&lt;/strong&gt; - The area under the ROC curve. A high AUC value, associated with a ROC curve occupying the upper left quadrant is ideal and indicates a good classifier (i.e. because its True Positive Rate is high, whilst its False Positive Rate is low)&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;img src=&#34;roc-curve-v2.png&#34; alt=&#34;ROC-AUC curves&#34;&gt;&lt;/p&gt;
&lt;h4 id=&#34;what-metric-do-we-choose-to-evaluate-our-classifier-by&#34;&gt;What metric do we choose to evaluate our classifier by?&lt;/h4&gt;
&lt;p&gt;It ultimately depends on your research question. If you&amp;rsquo;re classifying who has diabetes from a patient pool, for example, you&amp;rsquo;d want recall to be high. If you&amp;rsquo;re recommending movies to a user, you&amp;rsquo;d want the user to like the movies you recommended, so you&amp;rsquo;d want precision to be high. Overall, with a balanced data-set, you&amp;rsquo;d want accuracy to be high. With an un-balanced data-set, you&amp;rsquo;d want the weighted F1-score to be high.&lt;/p&gt;
&lt;h3 id=&#34;section-2-code&#34;&gt;Section 2: Code&lt;/h3&gt;
&lt;p&gt;In this section, we write some code to generate the metrics above.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;#first we import necessary packages
from sklearn import metrics
import pandas as pd
import os
import numpy as np
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;After training a classifier to generate predictions, we compare predictions to true labels and apply the metrics above.&lt;/p&gt;
&lt;p&gt;Below, we read in a file containing predictions and true labels for a sentiment classification problem (i.e. classifying the sentiment of text entries as either positive, negative or neutral). This is a multi-class problem, as we have more than 2 classes to predict.&lt;/p&gt;
&lt;p&gt;In the resulting data-frame, we have a column housing true labels (i.e. &lt;code&gt;sentiment&lt;/code&gt;), and one housing predictions (i.e. &lt;code&gt;predictions&lt;/code&gt;)&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;#insert the data-set you&#39;d want to read
df = pd.read_csv(&#39;predictions.csv&#39;)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We convert numerical labels generated by our classifier to categorical labels for ease of interpretation.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;sentiment_to_categories_dict = {
    2: &#39;positive&#39;,
    1: &#39;neutral&#39;,
    0: &#39;negative&#39;
}
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;df.sentiment = df.sentiment.apply(lambda sentiment: sentiment_to_categories_dict[sentiment])
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;df.head()
&lt;/code&gt;&lt;/pre&gt;
&lt;div&gt;
&lt;style scoped&gt;
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }
&lt;pre&gt;&lt;code&gt;.dataframe tbody tr th {
    vertical-align: top;
}

.dataframe thead th {
    text-align: right;
}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;/style&gt;&lt;/p&gt;
&lt;table border=&#34;1&#34; class=&#34;dataframe&#34;&gt;
  &lt;thead&gt;
    &lt;tr style=&#34;text-align: right;&#34;&gt;
      &lt;th&gt;&lt;/th&gt;
      &lt;th&gt;Unnamed: 0&lt;/th&gt;
      &lt;th&gt;entry&lt;/th&gt;
      &lt;th&gt;sentiment&lt;/th&gt;
      &lt;th&gt;predictions&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;th&gt;0&lt;/th&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;Up early ferry to Rangitoto walked up snack at...&lt;/td&gt;
      &lt;td&gt;positive&lt;/td&gt;
      &lt;td&gt;neutral&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;1&lt;/th&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;P home on time. Nap then food shop girls over ...&lt;/td&gt;
      &lt;td&gt;neutral&lt;/td&gt;
      &lt;td&gt;negative&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;2&lt;/th&gt;
      &lt;td&gt;2&lt;/td&gt;
      &lt;td&gt;Morning nap did not get up until late shower t...&lt;/td&gt;
      &lt;td&gt;neutral&lt;/td&gt;
      &lt;td&gt;neutral&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;3&lt;/th&gt;
      &lt;td&gt;3&lt;/td&gt;
      &lt;td&gt;N. Slept well but tired. Busy board at start b...&lt;/td&gt;
      &lt;td&gt;neutral&lt;/td&gt;
      &lt;td&gt;neutral&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;4&lt;/th&gt;
      &lt;td&gt;4&lt;/td&gt;
      &lt;td&gt;N. Slept well woken by post. Slow but steady n...&lt;/td&gt;
      &lt;td&gt;neutral&lt;/td&gt;
      &lt;td&gt;neutral&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;
&lt;/div&gt;
&lt;p&gt;We store the predictions and true labels on separate vectors&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;#True values (actual)
y_true = df[&#39;sentiment&#39;]

#predicted values (from model output)
y_pred = df[&#39;predictions&#39;]
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We then create row and column labels to append to our confusion matrix later on&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;#get row and column labels for confusion matrix

#get unique row labels
row_labels = np.unique(y_true)

#get column labels
column_labels = [label + &amp;quot;_predicted&amp;quot; for label in row_labels]
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Here we create a confusion matrix, passing in the row and column labels we create above&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;#create a confusion matrix object and display it (with labels)
c_m = pd.DataFrame(metrics.confusion_matrix(y_true,y_pred), index=row_labels, columns=column_labels)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We display the confusion matrix:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;c_m
&lt;/code&gt;&lt;/pre&gt;
&lt;div&gt;
&lt;style scoped&gt;
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }
&lt;pre&gt;&lt;code&gt;.dataframe tbody tr th {
    vertical-align: top;
}

.dataframe thead th {
    text-align: right;
}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;/style&gt;&lt;/p&gt;
&lt;table border=&#34;1&#34; class=&#34;dataframe&#34;&gt;
  &lt;thead&gt;
    &lt;tr style=&#34;text-align: right;&#34;&gt;
      &lt;th&gt;&lt;/th&gt;
      &lt;th&gt;negative_predicted&lt;/th&gt;
      &lt;th&gt;neutral_predicted&lt;/th&gt;
      &lt;th&gt;positive_predicted&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;th&gt;negative&lt;/th&gt;
      &lt;td&gt;6&lt;/td&gt;
      &lt;td&gt;26&lt;/td&gt;
      &lt;td&gt;3&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;neutral&lt;/th&gt;
      &lt;td&gt;12&lt;/td&gt;
      &lt;td&gt;40&lt;/td&gt;
      &lt;td&gt;10&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;positive&lt;/th&gt;
      &lt;td&gt;8&lt;/td&gt;
      &lt;td&gt;21&lt;/td&gt;
      &lt;td&gt;2&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;
&lt;/div&gt;
&lt;p&gt;&amp;hellip;as a data-frame&amp;hellip;with all of the other metrics mentioned above:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;#display all diagnostics as a data-frame, save results to a variable too
confusion_matrix = pd.DataFrame(metrics.classification_report(y_true,y_pred, digits=3, output_dict=True))
confusion_matrix
&lt;/code&gt;&lt;/pre&gt;
&lt;div&gt;
&lt;style scoped&gt;
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }
&lt;pre&gt;&lt;code&gt;.dataframe tbody tr th {
    vertical-align: top;
}

.dataframe thead th {
    text-align: right;
}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;/style&gt;&lt;/p&gt;
&lt;table border=&#34;1&#34; class=&#34;dataframe&#34;&gt;
  &lt;thead&gt;
    &lt;tr style=&#34;text-align: right;&#34;&gt;
      &lt;th&gt;&lt;/th&gt;
      &lt;th&gt;negative&lt;/th&gt;
      &lt;th&gt;neutral&lt;/th&gt;
      &lt;th&gt;positive&lt;/th&gt;
      &lt;th&gt;accuracy&lt;/th&gt;
      &lt;th&gt;macro avg&lt;/th&gt;
      &lt;th&gt;weighted avg&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;th&gt;precision&lt;/th&gt;
      &lt;td&gt;0.230769&lt;/td&gt;
      &lt;td&gt;0.459770&lt;/td&gt;
      &lt;td&gt;0.133333&lt;/td&gt;
      &lt;td&gt;0.375&lt;/td&gt;
      &lt;td&gt;0.274624&lt;/td&gt;
      &lt;td&gt;0.318094&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;recall&lt;/th&gt;
      &lt;td&gt;0.171429&lt;/td&gt;
      &lt;td&gt;0.645161&lt;/td&gt;
      &lt;td&gt;0.064516&lt;/td&gt;
      &lt;td&gt;0.375&lt;/td&gt;
      &lt;td&gt;0.293702&lt;/td&gt;
      &lt;td&gt;0.375000&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;f1-score&lt;/th&gt;
      &lt;td&gt;0.196721&lt;/td&gt;
      &lt;td&gt;0.536913&lt;/td&gt;
      &lt;td&gt;0.086957&lt;/td&gt;
      &lt;td&gt;0.375&lt;/td&gt;
      &lt;td&gt;0.273530&lt;/td&gt;
      &lt;td&gt;0.334918&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;support&lt;/th&gt;
      &lt;td&gt;35.000000&lt;/td&gt;
      &lt;td&gt;62.000000&lt;/td&gt;
      &lt;td&gt;31.000000&lt;/td&gt;
      &lt;td&gt;0.375&lt;/td&gt;
      &lt;td&gt;128.000000&lt;/td&gt;
      &lt;td&gt;128.000000&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;
&lt;/div&gt;
&lt;p&gt;&lt;strong&gt;Notice&lt;/strong&gt; that another metric called &lt;code&gt;support&lt;/code&gt; is included in the classification report. Support is simply the number of examples for each class.&lt;/p&gt;
&lt;p&gt;We can save this full classification report to a file, for later indexing/reporting.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;#save results to a file
confusion_matrix.to_csv(&#39;fileName.csv&#39;, index=True)
&lt;/code&gt;&lt;/pre&gt;
</description>
    </item>
    
    <item>
      <title>Detecting Seasons in React.js</title>
      <link>https://nixramirez.github.io/project/reactfirstproj/</link>
      <pubDate>Sat, 02 May 2020 00:00:00 +0000</pubDate>
      <guid>https://nixramirez.github.io/project/reactfirstproj/</guid>
      <description>&lt;p&gt;Here&amp;rsquo;s a proud moment - the first front-end I&amp;rsquo;ve built with React.js and JSX! It&amp;rsquo;s very simple, but it taught me a lot on how React worked internally.&lt;/p&gt;
&lt;p&gt;This app displays either two screens dependent on whether it’s summer or winter wherever the user is currently.&lt;/p&gt;
&lt;p&gt;A user in New Zealand (i.e. me) is in the southern hemisphere where it’s currenty winter (May 2nd, when this blogpost was written). The app displays the winter screen in this case.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;winterscreen.png&#34; alt=&#34;Winter Screen&#34;&gt;&lt;/p&gt;
&lt;p&gt;A user in San Francisco, on the other hand, is in the northern hemisphere where it’s currently summer. The app displays the summer screen for them.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;summerscreen.png&#34; alt=&#34;Summer Screen&#34;&gt;&lt;/p&gt;
&lt;p&gt;How do we know where the user is? Well, we can detect user location with the 
&lt;a href=&#34;https://developer.mozilla.org/en-US/docs/Web/API/Geolocation_API&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Mozilla Geolocation API&lt;/a&gt;
. We can grab latitude and longitude information about the user from this service.&lt;/p&gt;
&lt;p&gt;We can also force the Google Chrome browser to consider a location like San Francisco away from us by changing its geolocation in its ‘Sensors’ tab like so (mostly for testing the different views):&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;forcelocationchange.png&#34; alt=&#34;Force location change&#34;&gt;&lt;/p&gt;
&lt;p&gt;If the user has disabled location-sharing in any-way, the screen below encourages them to enable it&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;acceptlocationscreen.png&#34; alt=&#34;Accept location screen&#34;&gt;&lt;/p&gt;
&lt;p&gt;Semantic UI and some CSS was used to style the webpage.&lt;/p&gt;
&lt;p&gt;This project taught me React&amp;rsquo;s fundamentals - class based and functional components, the component life cycle methods, how to update component state, and how to pass information down from parent to children components using the props system.&lt;/p&gt;
&lt;p&gt;I&amp;rsquo;m currently building and deploying a more complex full-stack web app, integrating node.js/express.js for the back-end and a mongodb database. I might write a tutorial on that when it&amp;rsquo;s done, so if you&amp;rsquo;re interested, please stay tuned for that post! :)&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Intro To Python Workshop</title>
      <link>https://nixramirez.github.io/project/pythonworkshop/</link>
      <pubDate>Fri, 17 Apr 2020 00:00:00 +0000</pubDate>
      <guid>https://nixramirez.github.io/project/pythonworkshop/</guid>
      <description>&lt;p&gt;In this project, I taught an introductory workshop on Python&amp;rsquo;s base syntax, plus some basic libraries for working with data (i.e. pandas, numpy, matplotlib).&lt;/p&gt;
&lt;p&gt;This was done for the University of Auckland Data Science Club, which I help run.&lt;/p&gt;
&lt;p&gt;It was one workshop in a series teaching essential end-to-end data science skills. Workshops on Data Cleaning, Data Visualisation and Data modelling were planned for the future.&lt;/p&gt;
&lt;p&gt;I work as a programming tutor, so I thought teaching such a workshop would be easy. &lt;strong&gt;It wasn&amp;rsquo;t!&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Firstly, I had to come up with &lt;strong&gt;all&lt;/strong&gt; of the material on my own &amp;ndash; the recordings, the jupyter notebooks, the coding challenges, and sourcing the data-set. The recorded format also made it all the more daunting, as any mistakes I made would go on record officially! I also worried about delivering an engaging workshop. After all, the online nature of the workshop meant people could drop out anytime they wished.&lt;/p&gt;
&lt;p&gt;We were surprised to see good engagement with this workshop, though, with people attending our Live Q&amp;amp;A sessions after the workshop to ask for help regarding our coding challenges. This suggested people were watching our videos &amp;lsquo;til the end. I&amp;rsquo;ve also received positive verbal feedback about the workshop from members, and the youtube view count for both videos suggested pretty good engagement!&lt;/p&gt;
&lt;p&gt;This was such a rewarding experience, which highlighted a few things: 1.) Teaching isn&amp;rsquo;t easy 2.) Teaching is &lt;strong&gt;very hard&lt;/strong&gt; to get right 3.) I love teaching and sharing the beauty of programming.&lt;/p&gt;
&lt;p&gt;So now I present, the University of Auckland Data Science Club&amp;rsquo;s Intro to Python workshop!&lt;/p&gt;
&lt;h3 id=&#34;part-one-python-base-snytax&#34;&gt;Part One: Python Base Snytax&lt;/h3&gt;
&lt;iframe width=&#34;560&#34; height=&#34;315&#34; src=&#34;https://www.youtube.com/embed/S6hwKDvI24c&#34; frameborder=&#34;0&#34; allow=&#34;accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture&#34; allowfullscreen&gt;&lt;/iframe&gt;
&lt;h3 id=&#34;part-two-python-for-data-science&#34;&gt;Part Two: Python for Data Science&lt;/h3&gt;
&lt;iframe width=&#34;560&#34; height=&#34;315&#34; src=&#34;https://www.youtube.com/embed/n9X6ObJ9kos&#34; frameborder=&#34;0&#34; allow=&#34;accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture&#34; allowfullscreen&gt;&lt;/iframe&gt;
&lt;br&gt;
&lt;p&gt;The materials accompanying the workshop are 
&lt;a href=&#34;https://drive.google.com/drive/folders/1o3mH_-6ANT7EfiFWV31IXbhkc9P8VsAn?usp=sharing&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;found here&lt;/a&gt;
. These include the Jupyter notebooks used in both parts, a set of coding challenges to try after going through the workshop (and their associated answers), and the data-set used in part two.&lt;/p&gt;
&lt;p&gt;Please reach out if you have any feedback or queries! :)&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Web Scraping</title>
      <link>https://nixramirez.github.io/project/data-scraping/</link>
      <pubDate>Thu, 27 Feb 2020 00:00:00 +0000</pubDate>
      <guid>https://nixramirez.github.io/project/data-scraping/</guid>
      <description>&lt;h3 id=&#34;the-idea&#34;&gt;The Idea&lt;/h3&gt;
&lt;p&gt;An ex-colleague, 
&lt;a href=&#34;https://www.linkedin.com/in/dhilip-subramanian-36021918b/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Dhilip&lt;/a&gt;
 and I wanted to do a series of end-to-end projects in Data Science and Analytics for fun and to pick up skills along the way that would serve our future careers in this space. When Dhilip approached me for project ideas, I already had one I&amp;rsquo;ve thought about for quite some time.&lt;/p&gt;
&lt;p&gt;It was a timely project for us Qrious interns, who were deliberating on our next career move following the summer internship. Towards the end of our internship, a few of us (excluding me but including Dhilip) were graduating and applying for full-time jobs in the data space. Our discussions naturally gravitated to what starting pay we could expect for careers in data science, data engineering, and analytics. I imagined if we had an amount for each job averaged from recent, accurate salary data collected over a large number of companies operating in different industries, we had evidence to make sure future salary negotiations were fair!&lt;/p&gt;
&lt;h3 id=&#34;the-execution&#34;&gt;The Execution&lt;/h3&gt;
&lt;p&gt;Glassdoor is a job reviews site with job benefits and salary information reported anonymously by employees of various companies. It was our main data source for this project. Collecting a vast amount of data from the site would require automation, using a technique called &lt;em&gt;scraping&lt;/em&gt;. This is a technique used to extract content from specific HTML tags in a webpage, and in our case, exploits two technologies to do so: Selenium and BeautifulSoup.&lt;/p&gt;
&lt;p&gt;Selenium is a Java-based tool used in website testing to automate specific interactions with webpages (i.e. clicking on links, logging in, navigating the page, etc). Since it automates interacting with the DOM, it&amp;rsquo;s being used in Data Science to scrape data from websites. BeautifulSoup, on the other hand, is a Python library that parses HTML and makes it easy to extract specific elements from it.&lt;/p&gt;
&lt;h3 id=&#34;the-code&#34;&gt;The code&lt;/h3&gt;
&lt;h4 id=&#34;1-first-we-import-the-necessary-libraries&#34;&gt;1. First, we import the necessary libraries:&lt;/h4&gt;
&lt;p&gt;&lt;em&gt;Python Selenium libraries (Scraping)&lt;/em&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;webdriver - contains tools for working with an automated web browser&lt;/li&gt;
&lt;li&gt;webdriver.chrome - specifies Google Chrome as our automated browser&lt;/li&gt;
&lt;li&gt;time - the sleep function allows our automated tasks (i.e. clicking on links) to have a delay. Important for not being blocked as a bot by some websites&lt;/li&gt;
&lt;li&gt;BeautifulSoup - a python library for parsing HTML pages and grabbing content from specific HTML tags easily&lt;/li&gt;
&lt;li&gt;lxml - a parser for BeautifulSoup, allowing the HTML pages to be parsed as xml (and queried with xPath)&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;em&gt;Standard Python Libraries&lt;/em&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;pandas - a library for working with data-frames&lt;/li&gt;
&lt;li&gt;csv - a tool for reading and writing CSV files&lt;/li&gt;
&lt;li&gt;itertools - an iterator library&lt;/li&gt;
&lt;/ul&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from selenium import webdriver 
from selenium.webdriver.chrome.options import Options
from webdriver_manager.chrome import ChromeDriverManager
from time import sleep
from bs4 import BeautifulSoup
import lxml

import pandas as pd 
import csv
from itertools import zip_longest
&lt;/code&gt;&lt;/pre&gt;
&lt;h4 id=&#34;2-then-we-set-up-an-automated-browser-for-scraping&#34;&gt;2. Then, we set up an automated browser for scraping&lt;/h4&gt;
&lt;p&gt;In this step, we navigate to 
&lt;a href=&#34;https://www.glassdoor.co.nz/Salaries/index.htm&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Glassdoor&amp;rsquo;s Salaries page&lt;/a&gt;
 and type in whatever Job Title we&amp;rsquo;d want salaries for, in whatever location. In this example, we&amp;rsquo;d want to grab Data Engineers&amp;rsquo; salary information from the United States. We have to be logged into a Glassdoor account to do this.&lt;/p&gt;
&lt;p&gt;We then copy paste the URL of the resulting page into the driver.get(url) method below.&lt;/p&gt;
&lt;p&gt;Next, we find the &lt;em&gt;last page of results&lt;/em&gt; Glassdoor has for this particular search. This is important as we&amp;rsquo;re setting up our browser to automatically cycle through all pages of the search, grabbing salary information from each. We do this by modifying the URL in our browser, adding the string &lt;em&gt;_IP1500&lt;/em&gt; just before the &lt;em&gt;.htm&lt;/em&gt;. This lets us jump to the very last page of salary information, as it is likely that there won&amp;rsquo;t be 1500 pages worth of search results. If there is, just adjust the IP number to be larger.&lt;/p&gt;
&lt;p&gt;Once we&amp;rsquo;ve jumped to the last page of results, note down what that page is (by looking at the last number in the carousel button as such (red in the image below):&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;LastPageScrape.png&#34; alt=&#34;last page scraping&#34;&gt;&lt;/p&gt;
&lt;p&gt;and copying that number onto the lastPageNo variable in the below code. In below&amp;rsquo;s example, page 191 is the last page, and the for loop cycles through each page until it reaches that, employing a delay of at least 1.5s before it goes on to the next page.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;#creating empty arrays to hold job title, company name, job mean pay and pay range information
job_title = []
company_name = []
mean_pay = []
pay_range = []

lastPageNo = 191;

#going through 184 pages of salary information
for pageno in range(1,lastPageNo):

    driver = webdriver.Chrome(ChromeDriverManager().install())
    
    #getting webpage in glassdoor
    if pageno == 1:
        driver.get(&amp;quot;https://www.glassdoor.co.nz/Salaries/us-data-engineer-salary-SRCH_IL.0,2_IN1_KO3,16.htm&amp;quot;)
    else:
        driver.get(
            &amp;quot;https://www.glassdoor.co.nz/Salaries/us-data-engineer-salary-SRCH_IL.0,2_IN1_KO3,16.htm&amp;quot; + &amp;quot;_IP&amp;quot; + str(pageno) + &amp;quot;.htm&amp;quot;
        )
    time.sleep(1.5)
&lt;/code&gt;&lt;/pre&gt;
&lt;h4 id=&#34;3-we-parse-the-html-of-each-search-result-pageand-scrape&#34;&gt;3. We parse the HTML of each search result page&amp;hellip;and SCRAPE!&lt;/h4&gt;
&lt;p&gt;&amp;hellip;Using Beautifulsoup. The &lt;em&gt;page_source&lt;/em&gt; attribute of the driver grabs the page with its corresponding HTML tags, and parses it with &lt;strong&gt;lxml&lt;/strong&gt;, which as mentioned above, allows us to query the results using xpath if we wished.&lt;/p&gt;
&lt;p&gt;After parsing, we then grab specific HTML content. We do this by:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Inspecting the html tags where our information lies, by using Google Chrome&amp;rsquo;s &lt;em&gt;inspect&lt;/em&gt; option&lt;/li&gt;
&lt;li&gt;Collecting information that would distinguish our target HTML tag/s from others&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Tag Classes and IDs are useful for this. We see in the below screenshot, for example, that each salary block is enclosed by a &lt;code&gt;&amp;lt;div&amp;gt;&lt;/code&gt; with class &lt;strong&gt;&amp;ldquo;row align-items-center m-0 salaryRow__SalaryRowStyle__row&amp;rdquo;&lt;/strong&gt;. To grab each salary block from a page, we then use BeautifulSoup&amp;rsquo;s findAll() method, passing on the &lt;code&gt;&amp;lt;div class=&amp;quot;&amp;quot;&amp;gt;&lt;/code&gt; information mentioned.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;salaryBlocks.png&#34; alt=&#34;salary blocks enclosed in divs&#34;&gt;&lt;/p&gt;
&lt;p&gt;We do the same for every piece of information we want (i.e. job titles, company name, average salary, and salary range in this example).
These bits of info were obtained the same way as above. We looped through each salary block above to grab the specific information, as the below code shows.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;    #continuation from code above (still inside the for loop)
    #parsing the page through lxml option of beautifulsoup
    html = driver.page_source
    soup = BeautifulSoup(html, &#39;lxml&#39;)

    #getting each salary block
    salaryBlocks = soup.findAll(&amp;quot;div&amp;quot;, {&#39;class&#39; : &#39;row align-items-center m-0 salaryRow__SalaryRowStyle__row&#39;})

    #for each salary block, find the job title, company name, average pay, and pay range, and append them to the lists initialised above
    for block in salaryBlocks:
        entry = []

        jobTitle = block.find(&amp;quot;div&amp;quot;, {&#39;class&#39; : &#39;salaryRow__JobInfoStyle__jobTitle strong&#39;}).find(&amp;quot;a&amp;quot;).text
        job_title.append(jobTitle)

        companyName = block.find(&amp;quot;div&amp;quot;, {&#39;class&#39; : &#39;salaryRow__JobInfoStyle__employerName&#39;}).text
        company_name.append(companyName)

        meanPay = block.find(&amp;quot;div&amp;quot;, {&#39;class&#39; : &#39;salaryRow__JobInfoStyle__meanBasePay common__formFactorHelpers__showHH&#39;}).find(&#39;span&#39;).text
        mean_pay.append(meanPay)
        
        #if a pay range exists, grab it, otherwise, indicate none exists
        try:
            if block.find(&amp;quot;div&amp;quot;, {&#39;class&#39; : &#39;col-2 d-none d-md-block px-0 py salaryRow__SalaryRowStyle__amt&#39;}).find(&amp;quot;div&amp;quot;, {&#39;class&#39; : &#39;strong&#39;}):
                payRange = block.find(&amp;quot;div&amp;quot;, {&#39;class&#39; : &#39;col-2 d-none d-md-block px-0 py salaryRow__SalaryRowStyle__amt&#39;}).find(&amp;quot;div&amp;quot;, {&#39;class&#39; : &#39;strong&#39;}).text
                pay_range.append(payRange)
            elif block.find(&amp;quot;div&amp;quot;, {&#39;class&#39; : &#39;col-2 d-none d-md-block px-0 py salaryRow__SalaryRowStyle__amt&#39;}).find(&amp;quot;span&amp;quot;, {&#39;class&#39; : &#39;strong&#39;}):
                pay_range.append(&amp;quot;N/A&amp;quot;)
        except:
            pay_range.append(&amp;quot;N/A&amp;quot;)

        driver.quit()
&lt;/code&gt;&lt;/pre&gt;
&lt;h4 id=&#34;4-we-save-the-results-to-a-csv-file&#34;&gt;4. We save the results to a .csv file&lt;/h4&gt;
&lt;p&gt;Once we&amp;rsquo;ve obtained all the information we need, we store them into a python data-frame, which allows us to store the data in a tabular format. The columns of the table correspond to job title, company name, mean pay, and the pay range, whilst the rows are individual companies.&lt;/p&gt;
&lt;p&gt;The below code shows how the results are stored in a data-frame and eventually converted to a .csv file for easy reading into your favourite data analysis program/language later on.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;#process the lists into a final dataframe, and save to a CSV
final = []
for item in zip_longest(job_title, company_name, mean_pay, pay_range):
    final.append(item)

df = pd.DataFrame(
    final, columns=[&#39;jobTitle&#39;, &#39;companyName&#39;, &#39;meanPay&#39;, &#39;payRange&#39;])

df.to_csv(&amp;quot;Data Engineer Salaries United States.csv&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;em&gt;The final output of this scraping is a 28,000 row file, containing salary information for Data Engineers, Analysts, Scientists, and Machine Learning Engineers in Australia, New Zealand, and the United States. The file can be downloaded &lt;a href=&#34;https://nixramirez.github.io/files/Data-Professional-Salaries-Master.csv&#34; target=&#34;_blank&#34;&gt;here&lt;/a&gt; for free :)&lt;/em&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Tableau Interactive Chart</title>
      <link>https://nixramirez.github.io/project/tableau-interactive-viz/</link>
      <pubDate>Fri, 14 Feb 2020 00:00:00 +0000</pubDate>
      <guid>https://nixramirez.github.io/project/tableau-interactive-viz/</guid>
      <description>&lt;p&gt;The below chart was made in Tableau and hosted in Tableau Public. It was created as part of my internship with Qrious and presented to stakeholders.&lt;/p&gt;
&lt;div class=&#39;tableauPlaceholder&#39; id=&#39;viz1589195919608&#39; style=&#39;position: relative&#39;&gt;&lt;noscript&gt;&lt;a href=&#39;#&#39;&gt;&lt;img alt=&#39; &#39; src=&#39;https:&amp;#47;&amp;#47;public.tableau.com&amp;#47;static&amp;#47;images&amp;#47;Te&amp;#47;TermFrequencyTopicModellingNMF&amp;#47;Dashboard1&amp;#47;1_rss.png&#39; style=&#39;border: none&#39; /&gt;&lt;/a&gt;&lt;/noscript&gt;&lt;object class=&#39;tableauViz&#39;  style=&#39;display:none;&#39;&gt;&lt;param name=&#39;host_url&#39; value=&#39;https%3A%2F%2Fpublic.tableau.com%2F&#39; /&gt; &lt;param name=&#39;embed_code_version&#39; value=&#39;3&#39; /&gt; &lt;param name=&#39;site_root&#39; value=&#39;&#39; /&gt;&lt;param name=&#39;name&#39; value=&#39;TermFrequencyTopicModellingNMF&amp;#47;Dashboard1&#39; /&gt;&lt;param name=&#39;tabs&#39; value=&#39;no&#39; /&gt;&lt;param name=&#39;toolbar&#39; value=&#39;no&#39; /&gt;&lt;param name=&#39;static_image&#39; value=&#39;https:&amp;#47;&amp;#47;public.tableau.com&amp;#47;static&amp;#47;images&amp;#47;Te&amp;#47;TermFrequencyTopicModellingNMF&amp;#47;Dashboard1&amp;#47;1.png&#39; /&gt; &lt;param name=&#39;animate_transition&#39; value=&#39;yes&#39; /&gt;&lt;param name=&#39;display_static_image&#39; value=&#39;yes&#39; /&gt;&lt;param name=&#39;display_spinner&#39; value=&#39;yes&#39; /&gt;&lt;param name=&#39;display_overlay&#39; value=&#39;yes&#39; /&gt;&lt;param name=&#39;display_count&#39; value=&#39;yes&#39; /&gt;&lt;/object&gt;&lt;/div&gt;                &lt;script type=&#39;text/javascript&#39;&gt;                    var divElement = document.getElementById(&#39;viz1589195919608&#39;);                    var vizElement = divElement.getElementsByTagName(&#39;object&#39;)[0];                    if ( divElement.offsetWidth &gt; 800 ) { vizElement.style.width=&#39;1000px&#39;;vizElement.style.height=&#39;800px&#39;;} else if ( divElement.offsetWidth &gt; 500 ) { vizElement.style.width=&#39;1000px&#39;;vizElement.style.height=&#39;800px&#39;;} else { vizElement.style.width=&#39;100%&#39;;vizElement.style.height=&#39;2100px&#39;;}                     var scriptElement = document.createElement(&#39;script&#39;);                    scriptElement.src = &#39;https://public.tableau.com/javascripts/api/viz_v1.js&#39;;                    vizElement.parentNode.insertBefore(scriptElement, vizElement);                &lt;/script&gt;
&lt;br&gt;
&lt;p&gt;This chart is the output of running the topic-modeling algorithm Latent Dirichlet Allocation on a corpus of social media commentary about Spark Sports. The topic to visualise can be selected from the drop-down menu. Once selected, the chart shows the frequency of each term associated to that topic.&lt;/p&gt;
&lt;p&gt;Each term was taken into consideration in naming the topic.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Tableau Interactive Map</title>
      <link>https://nixramirez.github.io/project/tableau-interactive-map/</link>
      <pubDate>Wed, 15 Jan 2020 00:00:00 +0000</pubDate>
      <guid>https://nixramirez.github.io/project/tableau-interactive-map/</guid>
      <description>&lt;p&gt;The below map was made in Tableau and hosted in Tableau Public. It was created as part of my internship with Qrious and presented to stakeholders.&lt;/p&gt;
&lt;p&gt;If you hover over each area unit, you&amp;rsquo;ll see more information about that area unit.&lt;/p&gt;
&lt;div class=&#39;tableauPlaceholder&#39; id=&#39;viz1589194826274&#39; style=&#39;position: relative&#39;&gt;&lt;noscript&gt;&lt;a href=&#39;#&#39;&gt;&lt;img alt=&#39; &#39; src=&#39;https:&amp;#47;&amp;#47;public.tableau.com&amp;#47;static&amp;#47;images&amp;#47;6N&amp;#47;6N7PHMYGS&amp;#47;1_rss.png&#39; style=&#39;border: none&#39; /&gt;&lt;/a&gt;&lt;/noscript&gt;&lt;object class=&#39;tableauViz&#39;  style=&#39;display:none;&#39;&gt;&lt;param name=&#39;host_url&#39; value=&#39;https%3A%2F%2Fpublic.tableau.com%2F&#39; /&gt; &lt;param name=&#39;embed_code_version&#39; value=&#39;3&#39; /&gt; &lt;param name=&#39;path&#39; value=&#39;shared&amp;#47;6N7PHMYGS&#39; /&gt; &lt;param name=&#39;toolbar&#39; value=&#39;yes&#39; /&gt;&lt;param name=&#39;static_image&#39; value=&#39;https:&amp;#47;&amp;#47;public.tableau.com&amp;#47;static&amp;#47;images&amp;#47;6N&amp;#47;6N7PHMYGS&amp;#47;1.png&#39; /&gt; &lt;param name=&#39;animate_transition&#39; value=&#39;yes&#39; /&gt;&lt;param name=&#39;display_static_image&#39; value=&#39;yes&#39; /&gt;&lt;param name=&#39;display_spinner&#39; value=&#39;yes&#39; /&gt;&lt;param name=&#39;display_overlay&#39; value=&#39;yes&#39; /&gt;&lt;param name=&#39;display_count&#39; value=&#39;yes&#39; /&gt;&lt;/object&gt;&lt;/div&gt;                &lt;script type=&#39;text/javascript&#39;&gt;                    var divElement = document.getElementById(&#39;viz1589194826274&#39;);                    var vizElement = divElement.getElementsByTagName(&#39;object&#39;)[0];                    vizElement.style.width=&#39;100%&#39;;vizElement.style.height=(divElement.offsetWidth*0.75)+&#39;px&#39;;                    var scriptElement = document.createElement(&#39;script&#39;);                    scriptElement.src = &#39;https://public.tableau.com/javascripts/api/viz_v1.js&#39;;                    vizElement.parentNode.insertBefore(scriptElement, vizElement);                &lt;/script&gt;
&lt;br&gt;
&lt;p&gt;The New Zealand map was created by using a shapefile of New Zealand Area Unit informations, obtained from Stats.NZ. The rest of the data was overlaid onto this map. The two data-sets (i.e. geospatial data + household internet access data) were combined by doing a left outer join of the former to the latter on Area Unit ID in tableau.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Writing technical content in Academic</title>
      <link>https://nixramirez.github.io/post/writing-technical-content/</link>
      <pubDate>Fri, 12 Jul 2019 00:00:00 +0000</pubDate>
      <guid>https://nixramirez.github.io/post/writing-technical-content/</guid>
      <description>&lt;p&gt;Academic is designed to give technical content creators a seamless experience. You can focus on the content and Academic handles the rest.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Highlight your code snippets, take notes on math classes, and draw diagrams from textual representation.&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;On this page, you&amp;rsquo;ll find some examples of the types of technical content that can be rendered with Academic.&lt;/p&gt;
&lt;h2 id=&#34;examples&#34;&gt;Examples&lt;/h2&gt;
&lt;h3 id=&#34;code&#34;&gt;Code&lt;/h3&gt;
&lt;p&gt;Academic supports a Markdown extension for highlighting code syntax. You can enable this feature by toggling the &lt;code&gt;highlight&lt;/code&gt; option in your &lt;code&gt;config/_default/params.toml&lt;/code&gt; file.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;```python
import pandas as pd
data = pd.read_csv(&amp;quot;data.csv&amp;quot;)
data.head()
```
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;renders as&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;import pandas as pd
data = pd.read_csv(&amp;quot;data.csv&amp;quot;)
data.head()
&lt;/code&gt;&lt;/pre&gt;
&lt;h3 id=&#34;math&#34;&gt;Math&lt;/h3&gt;
&lt;p&gt;Academic supports a Markdown extension for $\LaTeX$ math. You can enable this feature by toggling the &lt;code&gt;math&lt;/code&gt; option in your &lt;code&gt;config/_default/params.toml&lt;/code&gt; file.&lt;/p&gt;
&lt;p&gt;To render &lt;em&gt;inline&lt;/em&gt; or &lt;em&gt;block&lt;/em&gt; math, wrap your LaTeX math with &lt;code&gt;$...$&lt;/code&gt; or &lt;code&gt;$$...$$&lt;/code&gt;, respectively.&lt;/p&gt;
&lt;p&gt;Example &lt;strong&gt;math block&lt;/strong&gt;:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-tex&#34;&gt;$$\gamma_{n} = \frac{ 
\left | \left (\mathbf x_{n} - \mathbf x_{n-1} \right )^T 
\left [\nabla F (\mathbf x_{n}) - \nabla F (\mathbf x_{n-1}) \right ] \right |}
{\left \|\nabla F(\mathbf{x}_{n}) - \nabla F(\mathbf{x}_{n-1}) \right \|^2}$$
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;renders as&lt;/p&gt;
&lt;p&gt;$$\gamma_{n} = \frac{ \left | \left (\mathbf x_{n} - \mathbf x_{n-1} \right )^T \left [\nabla F (\mathbf x_{n}) - \nabla F (\mathbf x_{n-1}) \right ] \right |}{\left |\nabla F(\mathbf{x}_{n}) - \nabla F(\mathbf{x}_{n-1}) \right |^2}$$&lt;/p&gt;
&lt;p&gt;Example &lt;strong&gt;inline math&lt;/strong&gt; &lt;code&gt;$\nabla F(\mathbf{x}_{n})$&lt;/code&gt; renders as $\nabla F(\mathbf{x}_{n})$.&lt;/p&gt;
&lt;p&gt;Example &lt;strong&gt;multi-line math&lt;/strong&gt; using the &lt;code&gt;\\&lt;/code&gt; math linebreak:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-tex&#34;&gt;$$f(k;p_0^*) = \begin{cases} p_0^* &amp;amp; \text{if }k=1, \\
1-p_0^* &amp;amp; \text {if }k=0.\end{cases}$$
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;renders as&lt;/p&gt;
&lt;p&gt;$$f(k;p_0^*) = \begin{cases} p_0^* &amp;amp; \text{if }k=1, \&lt;br&gt;
1-p_0^* &amp;amp; \text {if }k=0.\end{cases}$$&lt;/p&gt;
&lt;h3 id=&#34;diagrams&#34;&gt;Diagrams&lt;/h3&gt;
&lt;p&gt;Academic supports a Markdown extension for diagrams. You can enable this feature by toggling the &lt;code&gt;diagram&lt;/code&gt; option in your &lt;code&gt;config/_default/params.toml&lt;/code&gt; file or by adding &lt;code&gt;diagram: true&lt;/code&gt; to your page front matter.&lt;/p&gt;
&lt;p&gt;An example &lt;strong&gt;flowchart&lt;/strong&gt;:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;```mermaid
graph TD
A[Hard] --&amp;gt;|Text| B(Round)
B --&amp;gt; C{Decision}
C --&amp;gt;|One| D[Result 1]
C --&amp;gt;|Two| E[Result 2]
```
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;renders as&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-mermaid&#34;&gt;graph TD
A[Hard] --&amp;gt;|Text| B(Round)
B --&amp;gt; C{Decision}
C --&amp;gt;|One| D[Result 1]
C --&amp;gt;|Two| E[Result 2]
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;An example &lt;strong&gt;sequence diagram&lt;/strong&gt;:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;```mermaid
sequenceDiagram
Alice-&amp;gt;&amp;gt;John: Hello John, how are you?
loop Healthcheck
    John-&amp;gt;&amp;gt;John: Fight against hypochondria
end
Note right of John: Rational thoughts!
John--&amp;gt;&amp;gt;Alice: Great!
John-&amp;gt;&amp;gt;Bob: How about you?
Bob--&amp;gt;&amp;gt;John: Jolly good!
```
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;renders as&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-mermaid&#34;&gt;sequenceDiagram
Alice-&amp;gt;&amp;gt;John: Hello John, how are you?
loop Healthcheck
    John-&amp;gt;&amp;gt;John: Fight against hypochondria
end
Note right of John: Rational thoughts!
John--&amp;gt;&amp;gt;Alice: Great!
John-&amp;gt;&amp;gt;Bob: How about you?
Bob--&amp;gt;&amp;gt;John: Jolly good!
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;An example &lt;strong&gt;Gantt diagram&lt;/strong&gt;:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;```mermaid
gantt
section Section
Completed :done,    des1, 2014-01-06,2014-01-08
Active        :active,  des2, 2014-01-07, 3d
Parallel 1   :         des3, after des1, 1d
Parallel 2   :         des4, after des1, 1d
Parallel 3   :         des5, after des3, 1d
Parallel 4   :         des6, after des4, 1d
```
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;renders as&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-mermaid&#34;&gt;gantt
section Section
Completed :done,    des1, 2014-01-06,2014-01-08
Active        :active,  des2, 2014-01-07, 3d
Parallel 1   :         des3, after des1, 1d
Parallel 2   :         des4, after des1, 1d
Parallel 3   :         des5, after des3, 1d
Parallel 4   :         des6, after des4, 1d
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;An example &lt;strong&gt;class diagram&lt;/strong&gt;:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;```mermaid
classDiagram
Class01 &amp;lt;|-- AveryLongClass : Cool
&amp;lt;&amp;lt;interface&amp;gt;&amp;gt; Class01
Class09 --&amp;gt; C2 : Where am i?
Class09 --* C3
Class09 --|&amp;gt; Class07
Class07 : equals()
Class07 : Object[] elementData
Class01 : size()
Class01 : int chimp
Class01 : int gorilla
class Class10 {
  &amp;lt;&amp;lt;service&amp;gt;&amp;gt;
  int id
  size()
}
```
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;renders as&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-mermaid&#34;&gt;classDiagram
Class01 &amp;lt;|-- AveryLongClass : Cool
&amp;lt;&amp;lt;interface&amp;gt;&amp;gt; Class01
Class09 --&amp;gt; C2 : Where am i?
Class09 --* C3
Class09 --|&amp;gt; Class07
Class07 : equals()
Class07 : Object[] elementData
Class01 : size()
Class01 : int chimp
Class01 : int gorilla
class Class10 {
  &amp;lt;&amp;lt;service&amp;gt;&amp;gt;
  int id
  size()
}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;An example &lt;strong&gt;state diagram&lt;/strong&gt;:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;```mermaid
stateDiagram
[*] --&amp;gt; Still
Still --&amp;gt; [*]
Still --&amp;gt; Moving
Moving --&amp;gt; Still
Moving --&amp;gt; Crash
Crash --&amp;gt; [*]
```
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;renders as&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-mermaid&#34;&gt;stateDiagram
[*] --&amp;gt; Still
Still --&amp;gt; [*]
Still --&amp;gt; Moving
Moving --&amp;gt; Still
Moving --&amp;gt; Crash
Crash --&amp;gt; [*]
&lt;/code&gt;&lt;/pre&gt;
&lt;h3 id=&#34;todo-lists&#34;&gt;Todo lists&lt;/h3&gt;
&lt;p&gt;You can even write your todo lists in Academic too:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-markdown&#34;&gt;- [x] Write math example
- [x] Write diagram example
- [ ] Do something else
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;renders as&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;input checked=&#34;&#34; disabled=&#34;&#34; type=&#34;checkbox&#34;&gt;Write math example&lt;/li&gt;
&lt;li&gt;&lt;input checked=&#34;&#34; disabled=&#34;&#34; type=&#34;checkbox&#34;&gt;Write diagram example&lt;/li&gt;
&lt;li&gt;&lt;input disabled=&#34;&#34; type=&#34;checkbox&#34;&gt;Do something else&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;tables&#34;&gt;Tables&lt;/h3&gt;
&lt;p&gt;Represent your data in tables:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-markdown&#34;&gt;| First Header  | Second Header |
| ------------- | ------------- |
| Content Cell  | Content Cell  |
| Content Cell  | Content Cell  |
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;renders as&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;First Header&lt;/th&gt;
&lt;th&gt;Second Header&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;Content Cell&lt;/td&gt;
&lt;td&gt;Content Cell&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Content Cell&lt;/td&gt;
&lt;td&gt;Content Cell&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;h3 id=&#34;asides&#34;&gt;Asides&lt;/h3&gt;
&lt;p&gt;Academic supports a 
&lt;a href=&#34;https://sourcethemes.com/academic/docs/writing-markdown-latex/#alerts&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;shortcode for asides&lt;/a&gt;
, also referred to as &lt;em&gt;notices&lt;/em&gt;, &lt;em&gt;hints&lt;/em&gt;, or &lt;em&gt;alerts&lt;/em&gt;. By wrapping a paragraph in &lt;code&gt;{{% alert note %}} ... {{% /alert %}}&lt;/code&gt;, it will render as an aside.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-markdown&#34;&gt;{{% alert note %}}
A Markdown aside is useful for displaying notices, hints, or definitions to your readers.
{{% /alert %}}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;renders as&lt;/p&gt;
&lt;div class=&#34;alert alert-note&#34;&gt;
  &lt;div&gt;
    A Markdown aside is useful for displaying notices, hints, or definitions to your readers.
  &lt;/div&gt;
&lt;/div&gt;
&lt;h3 id=&#34;icons&#34;&gt;Icons&lt;/h3&gt;
&lt;p&gt;Academic enables you to use a wide range of 
&lt;a href=&#34;https://sourcethemes.com/academic/docs/page-builder/#icons&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;icons from &lt;em&gt;Font Awesome&lt;/em&gt; and &lt;em&gt;Academicons&lt;/em&gt;&lt;/a&gt;
 in addition to 
&lt;a href=&#34;https://sourcethemes.com/academic/docs/writing-markdown-latex/#emojis&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;emojis&lt;/a&gt;
.&lt;/p&gt;
&lt;p&gt;Here are some examples using the &lt;code&gt;icon&lt;/code&gt; shortcode to render icons:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-markdown&#34;&gt;{{&amp;lt; icon name=&amp;quot;terminal&amp;quot; pack=&amp;quot;fas&amp;quot; &amp;gt;}} Terminal  
{{&amp;lt; icon name=&amp;quot;python&amp;quot; pack=&amp;quot;fab&amp;quot; &amp;gt;}} Python  
{{&amp;lt; icon name=&amp;quot;r-project&amp;quot; pack=&amp;quot;fab&amp;quot; &amp;gt;}} R
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;renders as&lt;/p&gt;
&lt;p&gt;
  &lt;i class=&#34;fas fa-terminal  pr-1 fa-fw&#34;&gt;&lt;/i&gt; Terminal&lt;br&gt;

  &lt;i class=&#34;fab fa-python  pr-1 fa-fw&#34;&gt;&lt;/i&gt; Python&lt;br&gt;

  &lt;i class=&#34;fab fa-r-project  pr-1 fa-fw&#34;&gt;&lt;/i&gt; R&lt;/p&gt;
&lt;h3 id=&#34;did-you-find-this-page-helpful-consider-sharing-it-&#34;&gt;Did you find this page helpful? Consider sharing it 🙌&lt;/h3&gt;
</description>
    </item>
    
    <item>
      <title>An example preprint / working paper</title>
      <link>https://nixramirez.github.io/publication/preprint/</link>
      <pubDate>Sun, 07 Apr 2019 00:00:00 +0000</pubDate>
      <guid>https://nixramirez.github.io/publication/preprint/</guid>
      <description>&lt;div class=&#34;alert alert-note&#34;&gt;
  &lt;div&gt;
    Click the &lt;em&gt;Slides&lt;/em&gt; button above to demo Academic&amp;rsquo;s Markdown slides feature.
  &lt;/div&gt;
&lt;/div&gt;
&lt;p&gt;Supplementary notes can be added here, including 
&lt;a href=&#34;https://sourcethemes.com/academic/docs/writing-markdown-latex/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;code and math&lt;/a&gt;
.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Display Jupyter Notebooks with Academic</title>
      <link>https://nixramirez.github.io/post/jupyter/</link>
      <pubDate>Tue, 05 Feb 2019 00:00:00 +0000</pubDate>
      <guid>https://nixramirez.github.io/post/jupyter/</guid>
      <description>&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from IPython.core.display import Image
Image(&#39;https://www.python.org/static/community_logos/python-logo-master-v3-TM-flattened.png&#39;)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;./index_1_0.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;print(&amp;quot;Welcome to Academic!&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;Welcome to Academic!
&lt;/code&gt;&lt;/pre&gt;
&lt;h2 id=&#34;install-python-and-jupyterlab&#34;&gt;Install Python and JupyterLab&lt;/h2&gt;
&lt;p&gt;
&lt;a href=&#34;https://www.anaconda.com/distribution/#download-section&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Install Anaconda&lt;/a&gt;
 which includes Python 3 and JupyterLab.&lt;/p&gt;
&lt;p&gt;Alternatively, install JupyterLab with &lt;code&gt;pip3 install jupyterlab&lt;/code&gt;.&lt;/p&gt;
&lt;h2 id=&#34;create-or-upload-a-jupyter-notebook&#34;&gt;Create or upload a Jupyter notebook&lt;/h2&gt;
&lt;p&gt;Run the following commands in your Terminal, substituting &lt;code&gt;&amp;lt;MY-WEBSITE-FOLDER&amp;gt;&lt;/code&gt; and &lt;code&gt;&amp;lt;SHORT-POST-TITLE&amp;gt;&lt;/code&gt; with the file path to your Academic website folder and a short title for your blog post (use hyphens instead of spaces), respectively:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;mkdir -p &amp;lt;MY-WEBSITE-FOLDER&amp;gt;/content/post/&amp;lt;SHORT-POST-TITLE&amp;gt;/
cd &amp;lt;MY-WEBSITE-FOLDER&amp;gt;/content/post/&amp;lt;SHORT-POST-TITLE&amp;gt;/
jupyter lab index.ipynb
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The &lt;code&gt;jupyter&lt;/code&gt; command above will launch the JupyterLab editor, allowing us to add Academic metadata and write the content.&lt;/p&gt;
&lt;h2 id=&#34;edit-your-post-metadata&#34;&gt;Edit your post metadata&lt;/h2&gt;
&lt;p&gt;The first cell of your Jupter notebook will contain your post metadata (
&lt;a href=&#34;https://sourcethemes.com/academic/docs/front-matter/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;front matter&lt;/a&gt;
).&lt;/p&gt;
&lt;p&gt;In Jupter, choose &lt;em&gt;Markdown&lt;/em&gt; as the type of the first cell and wrap your Academic metadata in three dashes, indicating that it is YAML front matter:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;---
title: My post&#39;s title
date: 2019-09-01

# Put any other Academic metadata here...
---
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Edit the metadata of your post, using the 
&lt;a href=&#34;https://sourcethemes.com/academic/docs/managing-content&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;documentation&lt;/a&gt;
 as a guide to the available options.&lt;/p&gt;
&lt;p&gt;To set a 
&lt;a href=&#34;https://sourcethemes.com/academic/docs/managing-content/#featured-image&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;featured image&lt;/a&gt;
, place an image named &lt;code&gt;featured&lt;/code&gt; into your post&amp;rsquo;s folder.&lt;/p&gt;
&lt;p&gt;For other tips, such as using math, see the guide on 
&lt;a href=&#34;https://sourcethemes.com/academic/docs/writing-markdown-latex/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;writing content with Academic&lt;/a&gt;
.&lt;/p&gt;
&lt;h2 id=&#34;convert-notebook-to-markdown&#34;&gt;Convert notebook to Markdown&lt;/h2&gt;
&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;jupyter nbconvert index.ipynb --to markdown --NbConvertApp.output_files_dir=.
&lt;/code&gt;&lt;/pre&gt;
&lt;h2 id=&#34;example&#34;&gt;Example&lt;/h2&gt;
&lt;p&gt;This post was created with Jupyter. The orginal files can be found at &lt;a href=&#34;https://github.com/gcushen/hugo-academic/tree/master/exampleSite/content/post/jupyter&#34;&gt;https://github.com/gcushen/hugo-academic/tree/master/exampleSite/content/post/jupyter&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Slides</title>
      <link>https://nixramirez.github.io/slides/example/</link>
      <pubDate>Tue, 05 Feb 2019 00:00:00 +0000</pubDate>
      <guid>https://nixramirez.github.io/slides/example/</guid>
      <description>&lt;h1 id=&#34;create-slides-in-markdown-with-academic&#34;&gt;Create slides in Markdown with Academic&lt;/h1&gt;
&lt;p&gt;
&lt;a href=&#34;https://sourcethemes.com/academic/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Academic&lt;/a&gt;
 | 
&lt;a href=&#34;https://sourcethemes.com/academic/docs/managing-content/#create-slides&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Documentation&lt;/a&gt;
&lt;/p&gt;
&lt;hr&gt;
&lt;h2 id=&#34;features&#34;&gt;Features&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Efficiently write slides in Markdown&lt;/li&gt;
&lt;li&gt;3-in-1: Create, Present, and Publish your slides&lt;/li&gt;
&lt;li&gt;Supports speaker notes&lt;/li&gt;
&lt;li&gt;Mobile friendly slides&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;
&lt;h2 id=&#34;controls&#34;&gt;Controls&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Next: &lt;code&gt;Right Arrow&lt;/code&gt; or &lt;code&gt;Space&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;Previous: &lt;code&gt;Left Arrow&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;Start: &lt;code&gt;Home&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;Finish: &lt;code&gt;End&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;Overview: &lt;code&gt;Esc&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;Speaker notes: &lt;code&gt;S&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;Fullscreen: &lt;code&gt;F&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;Zoom: &lt;code&gt;Alt + Click&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;
&lt;a href=&#34;https://github.com/hakimel/reveal.js#pdf-export&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;PDF Export&lt;/a&gt;
: &lt;code&gt;E&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;
&lt;h2 id=&#34;code-highlighting&#34;&gt;Code Highlighting&lt;/h2&gt;
&lt;p&gt;Inline code: &lt;code&gt;variable&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;Code block:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;porridge = &amp;quot;blueberry&amp;quot;
if porridge == &amp;quot;blueberry&amp;quot;:
    print(&amp;quot;Eating...&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;
&lt;hr&gt;
&lt;h2 id=&#34;math&#34;&gt;Math&lt;/h2&gt;
&lt;p&gt;In-line math: $x + y = z$&lt;/p&gt;
&lt;p&gt;Block math:&lt;/p&gt;
&lt;p&gt;$$
f\left( x \right) = ;\frac{{2\left( {x + 4} \right)\left( {x - 4} \right)}}{{\left( {x + 4} \right)\left( {x + 1} \right)}}
$$&lt;/p&gt;
&lt;hr&gt;
&lt;h2 id=&#34;fragments&#34;&gt;Fragments&lt;/h2&gt;
&lt;p&gt;Make content appear incrementally&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;{{% fragment %}} One {{% /fragment %}}
{{% fragment %}} **Two** {{% /fragment %}}
{{% fragment %}} Three {{% /fragment %}}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Press &lt;code&gt;Space&lt;/code&gt; to play!&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;fragment &#34; &gt;
One
&lt;/span&gt;
&lt;span class=&#34;fragment &#34; &gt;
&lt;strong&gt;Two&lt;/strong&gt;
&lt;/span&gt;
&lt;span class=&#34;fragment &#34; &gt;
Three
&lt;/span&gt;&lt;/p&gt;
&lt;hr&gt;
&lt;p&gt;A fragment can accept two optional parameters:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;class&lt;/code&gt;: use a custom style (requires definition in custom CSS)&lt;/li&gt;
&lt;li&gt;&lt;code&gt;weight&lt;/code&gt;: sets the order in which a fragment appears&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;
&lt;h2 id=&#34;speaker-notes&#34;&gt;Speaker Notes&lt;/h2&gt;
&lt;p&gt;Add speaker notes to your presentation&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-markdown&#34;&gt;{{% speaker_note %}}
- Only the speaker can read these notes
- Press `S` key to view
{{% /speaker_note %}}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Press the &lt;code&gt;S&lt;/code&gt; key to view the speaker notes!&lt;/p&gt;
&lt;aside class=&#34;notes&#34;&gt;
  &lt;ul&gt;
&lt;li&gt;Only the speaker can read these notes&lt;/li&gt;
&lt;li&gt;Press &lt;code&gt;S&lt;/code&gt; key to view&lt;/li&gt;
&lt;/ul&gt;
&lt;/aside&gt;
&lt;hr&gt;
&lt;h2 id=&#34;themes&#34;&gt;Themes&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;black: Black background, white text, blue links (default)&lt;/li&gt;
&lt;li&gt;white: White background, black text, blue links&lt;/li&gt;
&lt;li&gt;league: Gray background, white text, blue links&lt;/li&gt;
&lt;li&gt;beige: Beige background, dark text, brown links&lt;/li&gt;
&lt;li&gt;sky: Blue background, thin dark text, blue links&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;
&lt;ul&gt;
&lt;li&gt;night: Black background, thick white text, orange links&lt;/li&gt;
&lt;li&gt;serif: Cappuccino background, gray text, brown links&lt;/li&gt;
&lt;li&gt;simple: White background, black text, blue links&lt;/li&gt;
&lt;li&gt;solarized: Cream-colored background, dark green text, blue links&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;

&lt;section data-noprocess data-shortcode-slide
  
      
      data-background-image=&#34;/img/boards.jpg&#34;
  &gt;

&lt;h2 id=&#34;custom-slide&#34;&gt;Custom Slide&lt;/h2&gt;
&lt;p&gt;Customize the slide style and background&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-markdown&#34;&gt;{{&amp;lt; slide background-image=&amp;quot;/img/boards.jpg&amp;quot; &amp;gt;}}
{{&amp;lt; slide background-color=&amp;quot;#0000FF&amp;quot; &amp;gt;}}
{{&amp;lt; slide class=&amp;quot;my-style&amp;quot; &amp;gt;}}
&lt;/code&gt;&lt;/pre&gt;
&lt;hr&gt;
&lt;h2 id=&#34;custom-css-example&#34;&gt;Custom CSS Example&lt;/h2&gt;
&lt;p&gt;Let&amp;rsquo;s make headers navy colored.&lt;/p&gt;
&lt;p&gt;Create &lt;code&gt;assets/css/reveal_custom.css&lt;/code&gt; with:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-css&#34;&gt;.reveal section h1,
.reveal section h2,
.reveal section h3 {
  color: navy;
}
&lt;/code&gt;&lt;/pre&gt;
&lt;hr&gt;
&lt;h1 id=&#34;questions&#34;&gt;Questions?&lt;/h1&gt;
&lt;p&gt;
&lt;a href=&#34;https://spectrum.chat/academic&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Ask&lt;/a&gt;
&lt;/p&gt;
&lt;p&gt;
&lt;a href=&#34;https://sourcethemes.com/academic/docs/managing-content/#create-slides&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Documentation&lt;/a&gt;
&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Academic: the website builder for Hugo</title>
      <link>https://nixramirez.github.io/post/getting-started/</link>
      <pubDate>Wed, 20 Apr 2016 00:00:00 +0000</pubDate>
      <guid>https://nixramirez.github.io/post/getting-started/</guid>
      <description>&lt;p&gt;&lt;strong&gt;Create a free website with Academic using Markdown, Jupyter, or RStudio. Choose a beautiful color theme and build anything with the Page Builder - over 40 &lt;em&gt;widgets&lt;/em&gt;, &lt;em&gt;themes&lt;/em&gt;, and &lt;em&gt;language packs&lt;/em&gt; included!&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;
&lt;a href=&#34;https://academic-demo.netlify.com/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Check out the latest &lt;strong&gt;demo&lt;/strong&gt;&lt;/a&gt;
 of what you&amp;rsquo;ll get in less than 10 minutes, or 
&lt;a href=&#34;https://sourcethemes.com/academic/#expo&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;view the &lt;strong&gt;showcase&lt;/strong&gt;&lt;/a&gt;
 of personal, project, and business sites.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;👉 
&lt;a href=&#34;#install&#34;&gt;&lt;strong&gt;Get Started&lt;/strong&gt;&lt;/a&gt;
&lt;/li&gt;
&lt;li&gt;📚 
&lt;a href=&#34;https://sourcethemes.com/academic/docs/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;View the &lt;strong&gt;documentation&lt;/strong&gt;&lt;/a&gt;
&lt;/li&gt;
&lt;li&gt;💬 
&lt;a href=&#34;https://discourse.gohugo.io&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;strong&gt;Ask a question&lt;/strong&gt; on the forum&lt;/a&gt;
&lt;/li&gt;
&lt;li&gt;👥 
&lt;a href=&#34;https://spectrum.chat/academic&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Chat with the &lt;strong&gt;community&lt;/strong&gt;&lt;/a&gt;
&lt;/li&gt;
&lt;li&gt;🐦 Twitter: 
&lt;a href=&#34;https://twitter.com/source_themes&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;@source_themes&lt;/a&gt;
 
&lt;a href=&#34;https://twitter.com/GeorgeCushen&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;@GeorgeCushen&lt;/a&gt;
 
&lt;a href=&#34;https://twitter.com/search?q=%23MadeWithAcademic&amp;amp;src=typd&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;#MadeWithAcademic&lt;/a&gt;
&lt;/li&gt;
&lt;li&gt;💡 
&lt;a href=&#34;https://github.com/gcushen/hugo-academic/issues&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Request a &lt;strong&gt;feature&lt;/strong&gt; or report a &lt;strong&gt;bug&lt;/strong&gt;&lt;/a&gt;
&lt;/li&gt;
&lt;li&gt;⬆️ &lt;strong&gt;Updating?&lt;/strong&gt; View the 
&lt;a href=&#34;https://sourcethemes.com/academic/docs/update/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Update Guide&lt;/a&gt;
 and 
&lt;a href=&#34;https://sourcethemes.com/academic/updates/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Release Notes&lt;/a&gt;
&lt;/li&gt;
&lt;li&gt;❤ &lt;strong&gt;Support development&lt;/strong&gt; of Academic:
&lt;ul&gt;
&lt;li&gt;☕️ 
&lt;a href=&#34;https://paypal.me/cushen&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;strong&gt;Donate a coffee&lt;/strong&gt;&lt;/a&gt;
&lt;/li&gt;
&lt;li&gt;💵 
&lt;a href=&#34;https://www.patreon.com/cushen&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Become a backer on &lt;strong&gt;Patreon&lt;/strong&gt;&lt;/a&gt;
&lt;/li&gt;
&lt;li&gt;🖼️ 
&lt;a href=&#34;https://www.redbubble.com/people/neutreno/works/34387919-academic&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Decorate your laptop or journal with an Academic &lt;strong&gt;sticker&lt;/strong&gt;&lt;/a&gt;
&lt;/li&gt;
&lt;li&gt;👕 
&lt;a href=&#34;https://academic.threadless.com/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Wear the &lt;strong&gt;T-shirt&lt;/strong&gt;&lt;/a&gt;
&lt;/li&gt;
&lt;li&gt;👩‍💻 
&lt;a href=&#34;https://sourcethemes.com/academic/docs/contribute/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;strong&gt;Contribute&lt;/strong&gt;&lt;/a&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;















&lt;figure&gt;


  &lt;a data-fancybox=&#34;&#34; href=&#34;https://raw.githubusercontent.com/gcushen/hugo-academic/master/academic.png&#34; data-caption=&#34;Academic is mobile first with a responsive design to ensure that your site looks stunning on every device.&#34;&gt;


  &lt;img src=&#34;https://raw.githubusercontent.com/gcushen/hugo-academic/master/academic.png&#34; alt=&#34;&#34;  &gt;
&lt;/a&gt;


  
  
  &lt;figcaption&gt;
    Academic is mobile first with a responsive design to ensure that your site looks stunning on every device.
  &lt;/figcaption&gt;


&lt;/figure&gt;

&lt;p&gt;&lt;strong&gt;Key features:&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Page builder&lt;/strong&gt; - Create &lt;em&gt;anything&lt;/em&gt; with 
&lt;a href=&#34;https://sourcethemes.com/academic/docs/page-builder/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;strong&gt;widgets&lt;/strong&gt;&lt;/a&gt;
 and 
&lt;a href=&#34;https://sourcethemes.com/academic/docs/writing-markdown-latex/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;strong&gt;elements&lt;/strong&gt;&lt;/a&gt;
&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Edit any type of content&lt;/strong&gt; - Blog posts, publications, talks, slides, projects, and more!&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Create content&lt;/strong&gt; in 
&lt;a href=&#34;https://sourcethemes.com/academic/docs/writing-markdown-latex/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;strong&gt;Markdown&lt;/strong&gt;&lt;/a&gt;
, 
&lt;a href=&#34;https://sourcethemes.com/academic/docs/jupyter/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;strong&gt;Jupyter&lt;/strong&gt;&lt;/a&gt;
, or 
&lt;a href=&#34;https://sourcethemes.com/academic/docs/install/#install-with-rstudio&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;strong&gt;RStudio&lt;/strong&gt;&lt;/a&gt;
&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Plugin System&lt;/strong&gt; - Fully customizable 
&lt;a href=&#34;https://sourcethemes.com/academic/themes/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;strong&gt;color&lt;/strong&gt; and &lt;strong&gt;font themes&lt;/strong&gt;&lt;/a&gt;
&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Display Code and Math&lt;/strong&gt; - Code highlighting and 
&lt;a href=&#34;https://en.wikibooks.org/wiki/LaTeX/Mathematics&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;LaTeX math&lt;/a&gt;
 supported&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Integrations&lt;/strong&gt; - 
&lt;a href=&#34;https://analytics.google.com&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Google Analytics&lt;/a&gt;
, 
&lt;a href=&#34;https://disqus.com&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Disqus commenting&lt;/a&gt;
, Maps, Contact Forms, and more!&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Beautiful Site&lt;/strong&gt; - Simple and refreshing one page design&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Industry-Leading SEO&lt;/strong&gt; - Help get your website found on search engines and social media&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Media Galleries&lt;/strong&gt; - Display your images and videos with captions in a customizable gallery&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Mobile Friendly&lt;/strong&gt; - Look amazing on every screen with a mobile friendly version of your site&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Multi-language&lt;/strong&gt; - 15+ language packs including English, 中文, and Português&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Multi-user&lt;/strong&gt; - Each author gets their own profile page&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Privacy Pack&lt;/strong&gt; - Assists with GDPR&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Stand Out&lt;/strong&gt; - Bring your site to life with animation, parallax backgrounds, and scroll effects&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;One-Click Deployment&lt;/strong&gt; - No servers. No databases. Only files.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;themes&#34;&gt;Themes&lt;/h2&gt;
&lt;p&gt;Academic comes with &lt;strong&gt;automatic day (light) and night (dark) mode&lt;/strong&gt; built-in. Alternatively, visitors can  choose their preferred mode - click the sun/moon icon in the top right of the 
&lt;a href=&#34;https://academic-demo.netlify.com/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Demo&lt;/a&gt;
 to see it in action! Day/night mode can also be disabled by the site admin in &lt;code&gt;params.toml&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;
&lt;a href=&#34;https://sourcethemes.com/academic/themes/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Choose a stunning &lt;strong&gt;theme&lt;/strong&gt; and &lt;strong&gt;font&lt;/strong&gt;&lt;/a&gt;
 for your site. Themes are fully 
&lt;a href=&#34;https://sourcethemes.com/academic/docs/customization/#custom-theme&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;customizable&lt;/a&gt;
.&lt;/p&gt;
&lt;h2 id=&#34;ecosystem&#34;&gt;Ecosystem&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;
&lt;a href=&#34;https://github.com/sourcethemes/academic-admin&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Academic Admin&lt;/a&gt;
:&lt;/strong&gt; An admin tool to import publications from BibTeX or import assets for an offline site&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;
&lt;a href=&#34;https://github.com/sourcethemes/academic-scripts&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Academic Scripts&lt;/a&gt;
:&lt;/strong&gt; Scripts to help migrate content to new versions of Academic&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;install&#34;&gt;Install&lt;/h2&gt;
&lt;p&gt;You can choose from one of the following four methods to install:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;a href=&#34;https://sourcethemes.com/academic/docs/install/#install-with-web-browser&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;strong&gt;one-click install using your web browser (recommended)&lt;/strong&gt;&lt;/a&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;a href=&#34;https://sourcethemes.com/academic/docs/install/#install-with-git&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;install on your computer using &lt;strong&gt;Git&lt;/strong&gt; with the Command Prompt/Terminal app&lt;/a&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;a href=&#34;https://sourcethemes.com/academic/docs/install/#install-with-zip&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;install on your computer by downloading the &lt;strong&gt;ZIP files&lt;/strong&gt;&lt;/a&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;a href=&#34;https://sourcethemes.com/academic/docs/install/#install-with-rstudio&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;install on your computer with &lt;strong&gt;RStudio&lt;/strong&gt;&lt;/a&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Then 
&lt;a href=&#34;https://sourcethemes.com/academic/docs/get-started/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;personalize and deploy your new site&lt;/a&gt;
.&lt;/p&gt;
&lt;h2 id=&#34;updating&#34;&gt;Updating&lt;/h2&gt;
&lt;p&gt;
&lt;a href=&#34;https://sourcethemes.com/academic/docs/update/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;View the Update Guide&lt;/a&gt;
.&lt;/p&gt;
&lt;p&gt;Feel free to &lt;em&gt;star&lt;/em&gt; the project on 
&lt;a href=&#34;https://github.com/gcushen/hugo-academic/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Github&lt;/a&gt;
 to help keep track of 
&lt;a href=&#34;https://sourcethemes.com/academic/updates&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;updates&lt;/a&gt;
.&lt;/p&gt;
&lt;h2 id=&#34;license&#34;&gt;License&lt;/h2&gt;
&lt;p&gt;Copyright 2016-present 
&lt;a href=&#34;https://georgecushen.com&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;George Cushen&lt;/a&gt;
.&lt;/p&gt;
&lt;p&gt;Released under the 
&lt;a href=&#34;https://github.com/gcushen/hugo-academic/blob/master/LICENSE.md&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;MIT&lt;/a&gt;
 license.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>An example journal article</title>
      <link>https://nixramirez.github.io/publication/journal-article/</link>
      <pubDate>Tue, 01 Sep 2015 00:00:00 +0000</pubDate>
      <guid>https://nixramirez.github.io/publication/journal-article/</guid>
      <description>&lt;div class=&#34;alert alert-note&#34;&gt;
  &lt;div&gt;
    Click the &lt;em&gt;Cite&lt;/em&gt; button above to demo the feature to enable visitors to import publication metadata into their reference management software.
  &lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;alert alert-note&#34;&gt;
  &lt;div&gt;
    Click the &lt;em&gt;Slides&lt;/em&gt; button above to demo Academic&amp;rsquo;s Markdown slides feature.
  &lt;/div&gt;
&lt;/div&gt;
&lt;p&gt;Supplementary notes can be added here, including 
&lt;a href=&#34;https://sourcethemes.com/academic/docs/writing-markdown-latex/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;code and math&lt;/a&gt;
.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>An example conference paper</title>
      <link>https://nixramirez.github.io/publication/conference-paper/</link>
      <pubDate>Mon, 01 Jul 2013 00:00:00 +0000</pubDate>
      <guid>https://nixramirez.github.io/publication/conference-paper/</guid>
      <description>&lt;div class=&#34;alert alert-note&#34;&gt;
  &lt;div&gt;
    Click the &lt;em&gt;Cite&lt;/em&gt; button above to demo the feature to enable visitors to import publication metadata into their reference management software.
  &lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;alert alert-note&#34;&gt;
  &lt;div&gt;
    Click the &lt;em&gt;Slides&lt;/em&gt; button above to demo Academic&amp;rsquo;s Markdown slides feature.
  &lt;/div&gt;
&lt;/div&gt;
&lt;p&gt;Supplementary notes can be added here, including 
&lt;a href=&#34;https://sourcethemes.com/academic/docs/writing-markdown-latex/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;code and math&lt;/a&gt;
.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Deploying a deep learning model in AWS</title>
      <link>https://nixramirez.github.io/project/batch-inference-aws-deployment/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://nixramirez.github.io/project/batch-inference-aws-deployment/</guid>
      <description>&lt;p&gt;If you&amp;rsquo;re a data professional working with machine learning models in the cloud, then you might be familiar with Amazon Web Services&amp;rsquo; (AWS) SageMaker. 
&lt;a href=&#34;https://aws.amazon.com/sagemaker/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;SageMaker&lt;/a&gt;
 is AWS&amp;rsquo; machine learning platform allowing developers to build, train, and deploy models in the cloud or in embedded systems and edge devices.&lt;/p&gt;
&lt;p&gt;There are a dizzying number of services available within SageMaker for all kinds of machine learning use-cases. For the sake of brevity in this post, we will only cover a specific use-case&amp;ndash;that of bringing into AWS, and eventually deploying a deep learning model &lt;strong&gt;trained outside&lt;/strong&gt; of the AWS ecosystem. We&amp;rsquo;d also want the deployed model to perform predictions for a &lt;strong&gt;batch&lt;/strong&gt; of data (i.e. multiple rows), as opposed to a single entry. Also, we&amp;rsquo;d want the model to predict in &lt;strong&gt;serverless&lt;/strong&gt; fashion, meaning we only invoke the model and the hardware powering it &lt;strong&gt;when we&amp;rsquo;re doing predictions&lt;/strong&gt;. Serverless is &lt;em&gt;pay-as-you-go&lt;/em&gt;. The opposite of serverless would be to make the model&amp;rsquo;s hardware &lt;em&gt;persist&lt;/em&gt;, even when not actively predicting.&lt;/p&gt;
&lt;p&gt;In this post, we compare and contrast services relevant to our use-case and show how we deployed a deep learning model, trained outside AWS, for batch inference. You&amp;rsquo;ll hopefully learn how model deployment in SageMaker works and get an overview of other SageMaker features.&lt;/p&gt;
&lt;p&gt;So let&amp;rsquo;s dive right in!&lt;/p&gt;
&lt;h3 id=&#34;the-sagemaker-platform&#34;&gt;The SageMaker Platform&lt;/h3&gt;
&lt;p&gt;To help make sense of the various features SageMaker has, AWS offers a &lt;em&gt;fantastic&lt;/em&gt; 
&lt;a href=&#34;&#34;&gt;technical deep dive&lt;/a&gt;
 on the platform, which I&amp;rsquo;d highly recommend you browse before reading SageMaker documentation.&lt;/p&gt;
&lt;p&gt;SageMaker also has a 
&lt;a href=&#34;&#34;&gt;2 month free tier&lt;/a&gt;
 for hands-on tinkering with the platform, perfect for exploring what it offers!&lt;/p&gt;
&lt;h4 id=&#34;notebooks-and-s3&#34;&gt;Notebooks and S3&lt;/h4&gt;
&lt;p&gt;Central to SageMaker are 
&lt;a href=&#34;&#34;&gt;Notebook Instances&lt;/a&gt;
, Jupyter Notebook environments running on EC2 instances that are fully managed by AWS upon creation. While the &lt;em&gt;fully managed&lt;/em&gt; keyword implies that we can do very little altering of the EC2 instance&amp;rsquo;s system save from some non-persistent shell commands, it also means we can use it out-of-the-box for running common data science packages.&lt;/p&gt;
&lt;p&gt;A Jupyter notebook inside an instance can hold code for reading data in, pre-processing it, using it to train a model, validating the model, and deploying it later. While we can think of a notebook as the front-end for our interactions with SageMaker, we can think of an S3 storage bucket as our back-end. Our notebook instance has to be connected to an S3 bucket to read data from or save/load model artefacts to/from.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;NB:&lt;/strong&gt; When we deploy a model trained in a notebook, we are not deploying the notebook itself. We&amp;rsquo;re just using the notebook to &lt;em&gt;orchestrate&lt;/em&gt; training, validation and deployment.&lt;/p&gt;
&lt;p&gt;We don&amp;rsquo;t cover configuring Notebook Instances and S3 buckets here as there are a lot of tutorials on these subjects. Just make sure that the S3 bucket exists in the same region as the Notebook Instance for them to connect, and that going for the smallest compute hardware is fine for now, since we can modify the size as needed later on.&lt;/p&gt;















&lt;figure&gt;


  &lt;a data-fancybox=&#34;&#34; href=&#34;updating.png&#34; data-caption=&#34;You can update the size (i.e. it&amp;rsquo;s currently ml.t2.medium) of the instance when it&amp;rsquo;s stopped&#34;&gt;


  &lt;img data-src=&#34;updating.png&#34; class=&#34;lazyload&#34; alt=&#34;&#34; width=&#34;100%&#34; height=&#34;100%&#34;&gt;
&lt;/a&gt;


  
  
  &lt;figcaption data-pre=&#34;Figure &#34; data-post=&#34;:&#34; class=&#34;numbered&#34;&gt;
    You can update the size (i.e. it&amp;rsquo;s currently ml.t2.medium) of the instance when it&amp;rsquo;s stopped
  &lt;/figcaption&gt;


&lt;/figure&gt;

&lt;p&gt;After creating and starting a notebook instance, you&amp;rsquo;ll find a repository of example notebooks made by the AWS team for different machine learning use-cases. These notebooks contain code for implementing end-to-end data science pipelines, and are useful starting points when implementing your own.&lt;/p&gt;















&lt;figure&gt;


  &lt;a data-fancybox=&#34;&#34; href=&#34;SageMakerExamples.png&#34; data-caption=&#34;Find examples of end-to-end pipelines here&#34;&gt;


  &lt;img data-src=&#34;SageMakerExamples.png&#34; class=&#34;lazyload&#34; alt=&#34;&#34; width=&#34;50%&#34; height=&#34;50%&#34;&gt;
&lt;/a&gt;


  
  
  &lt;figcaption data-pre=&#34;Figure &#34; data-post=&#34;:&#34; class=&#34;numbered&#34;&gt;
    Find examples of end-to-end pipelines here
  &lt;/figcaption&gt;


&lt;/figure&gt;

&lt;h4 id=&#34;cloudwatch-logs&#34;&gt;CloudWatch Logs&lt;/h4&gt;
&lt;p&gt;The pipelines we run in SageMaker can be set-up programatically in notebooks using the SageMaker Python SDK, or through the SageMaker console otherwise. I prefer the former as the flow is more natural to me, having prototyped countless times with Jupyter Notebooks before. The latter is still worth noting and available as an option for those who prefer to code less.&lt;/p&gt;
&lt;p&gt;Using a notebook and the SDK however allows us to debug our code and monitor the progress of our processes better. To debug Python code in our local systems, we often add &lt;code&gt;print()&lt;/code&gt; statements for monitoring the flow of our program. The AWS equivalent to &lt;code&gt;print()&lt;/code&gt; is the &lt;code&gt;log()&lt;/code&gt; command&lt;/p&gt;
&lt;h4 id=&#34;cloudwatch-logs-1&#34;&gt;CloudWatch Logs&lt;/h4&gt;
&lt;h3 id=&#34;byo-deep-learning-model&#34;&gt;BYO Deep Learning Model&lt;/h3&gt;
&lt;p&gt;As we are bringing a model artifact trained outside of AWS, we first have to make sure it&amp;rsquo;s transferred to AWS S3 storage.&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Create an S3 bucket&lt;/li&gt;
&lt;/ol&gt;
&lt;ul&gt;
&lt;li&gt;S3 in the AWS Console&lt;/li&gt;
&lt;li&gt;Create a bucket, give it a descriptive name. Take note of the region you&amp;rsquo;re creating it in. This region should be the same as the Notebook Instance so the instance could connect to the bucket.&lt;/li&gt;
&lt;/ul&gt;
&lt;ol start=&#34;2&#34;&gt;
&lt;li&gt;Create a Notebook Instance&lt;/li&gt;
&lt;/ol&gt;
&lt;ul&gt;
&lt;li&gt;you will have to specify the instance size, version and bandwidth. The size can be altered later, so choose the smallest one for now to save on costs. The latest version will have the most up-to-date features. The bandwidth (i.e. include examples) is also alterable, so for now choose the smallest one.&lt;/li&gt;
&lt;li&gt;NB: Elastic inference&lt;/li&gt;
&lt;li&gt;Connect it to the S3 bucket you&amp;rsquo;ve made in&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
  </channel>
</rss>
