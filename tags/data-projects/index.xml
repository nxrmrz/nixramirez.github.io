<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Data Projects | Hello, World</title>
    <link>https://nixramirez.github.io/tags/data-projects/</link>
      <atom:link href="https://nixramirez.github.io/tags/data-projects/index.xml" rel="self" type="application/rss+xml" />
    <description>Data Projects</description>
    <generator>Source Themes Academic (https://sourcethemes.com/academic/)</generator><language>en-us</language>
    <image>
      <url>img/map[gravatar:%!s(bool=false) shape:circle]</url>
      <title>Data Projects</title>
      <link>https://nixramirez.github.io/tags/data-projects/</link>
    </image>
    
    <item>
      <title>Web Scraping</title>
      <link>https://nixramirez.github.io/project/data-scraping/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://nixramirez.github.io/project/data-scraping/</guid>
      <description>&lt;h3 id=&#34;the-idea&#34;&gt;The Idea&lt;/h3&gt;
&lt;p&gt;An ex-colleague, 
&lt;a href=&#34;https://www.linkedin.com/in/dhilip-subramanian-36021918b/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Dhilip&lt;/a&gt;
 and I wanted to do a series of end-to-end projects in Data Science and Analytics for fun and to pick up skills along the way that would serve our future careers in this space. When Dhilip approached me for project ideas, I already had one I&amp;rsquo;ve thought about for quite some time.&lt;/p&gt;
&lt;p&gt;It was a timely project for us Qrious interns, who were deliberating on our next career move following the summer internship. Towards the end of our internship, a few of us (excluding me but including Dhilip) were graduating and applying for full-time jobs in the data space. Our discussions naturally gravitated to what starting pay we could expect for careers in data science, data engineering, and analytics. I imagined if we had an amount for each job averaged from recent, accurate salary data collected over a large number of companies operating in different industries, we had evidence to make sure future salary negotiations were fair! It was ideal to have a large dataset&amp;ndash;the larger the more representative it was of reality.&lt;/p&gt;
&lt;p&gt;We didn&amp;rsquo;t stop there. Salary negotiations usually happen &lt;em&gt;after&lt;/em&gt; an offer is extended. It would be even more useful if we had accurate advice on &lt;em&gt;how&lt;/em&gt; to get an offer in the first place. Advice on the top skills a data professional needed, for example, as listed on job postings online, so we could make sure we built on them.&lt;/p&gt;
&lt;h3 id=&#34;the-execution&#34;&gt;The Execution&lt;/h3&gt;
&lt;p&gt;Glassdoor is a job reviews site with job benefits and salary information reported anonymously by employees of various companies. Indeed and Seek are job-hunting sites with job descriptions and the occasional salary information as well, reported by companies themselves. Data from all three sites would be perfect for our analysis! Collecting data from the sites would need to be automated, in a process called &lt;em&gt;scraping&lt;/em&gt;. This is a technique used to extract content from specific HTML tags in a webpage, and it can exploit two technologies to do so: Selenium and BeautifulSoup.&lt;/p&gt;
&lt;p&gt;Selenium is a Java-based tool used in website testing to automate specific interactions with webpages (i.e. clicking on links, logging in, navigating the page, etc). Since it automates interacting with the DOM, it&amp;rsquo;s being used in Data Science to scrape data from websites. BeautifulSoup, on the other hand, is a Python library that parses HTML and makes it easy to extract specific elements from it.&lt;/p&gt;
&lt;h3 id=&#34;the-code&#34;&gt;The code&lt;/h3&gt;
&lt;p&gt;A work in progress (explanations to follow:)&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from selenium import webdriver
from selenium.webdriver.chrome.options import Options
import pandas as pd
# from selenium.webdriver.common.keys import Keys
from time import sleep
from bs4 import BeautifulSoup, Comment
import time
import os
import csv
import lxml
from itertools import zip_longest
from webdriver_manager.chrome import ChromeDriverManager

#setting up an automated google chrome browser
driver = webdriver.Chrome(ChromeDriverManager().install())

job_title = []
company_name = []
mean_pay = []
pay_range = []

for pageno in range(1,184):

    driver = webdriver.Chrome(ChromeDriverManager().install())
    
    #getting webpage in glassdoor
    if pageno == 1:
        driver.get(&amp;quot;https://www.glassdoor.co.nz/Salaries/us-data-engineer-salary-SRCH_IL.0,2_IN1_KO3,16.htm&amp;quot;)
    else:
        driver.get(
            &amp;quot;https://www.glassdoor.co.nz/Salaries/us-data-engineer-salary-SRCH_IL.0,2_IN1_KO3,16.htm&amp;quot; + &amp;quot;_IP&amp;quot; + str(pageno) + &amp;quot;.htm&amp;quot;
        )
    time.sleep(1.5)

    #parsing the page through lxml option of beautifulsoup
    html = driver.page_source
    soup = BeautifulSoup(html, &#39;lxml&#39;)

    #getting each salary block
    salaryBlocks = soup.findAll(&amp;quot;div&amp;quot;, {&#39;class&#39; : &#39;row align-items-center m-0 salaryRow__SalaryRowStyle__row&#39;})
    #once you&#39;ve done that for initial page, scroll to next page and do again. save results into one main file

    for block in salaryBlocks:
        entry = []

        jobTitle = block.find(&amp;quot;div&amp;quot;, {&#39;class&#39; : &#39;salaryRow__JobInfoStyle__jobTitle strong&#39;}).find(&amp;quot;a&amp;quot;).text
        job_title.append(jobTitle)

        companyName = block.find(&amp;quot;div&amp;quot;, {&#39;class&#39; : &#39;salaryRow__JobInfoStyle__employerName&#39;}).text
        company_name.append(companyName)

        meanPay = block.find(&amp;quot;div&amp;quot;, {&#39;class&#39; : &#39;salaryRow__JobInfoStyle__meanBasePay common__formFactorHelpers__showHH&#39;}).find(&#39;span&#39;).text
        mean_pay.append(meanPay)
        
        try:
            if block.find(&amp;quot;div&amp;quot;, {&#39;class&#39; : &#39;col-2 d-none d-md-block px-0 py salaryRow__SalaryRowStyle__amt&#39;}).find(&amp;quot;div&amp;quot;, {&#39;class&#39; : &#39;strong&#39;}):
                payRange = block.find(&amp;quot;div&amp;quot;, {&#39;class&#39; : &#39;col-2 d-none d-md-block px-0 py salaryRow__SalaryRowStyle__amt&#39;}).find(&amp;quot;div&amp;quot;, {&#39;class&#39; : &#39;strong&#39;}).text
                pay_range.append(payRange)
            elif block.find(&amp;quot;div&amp;quot;, {&#39;class&#39; : &#39;col-2 d-none d-md-block px-0 py salaryRow__SalaryRowStyle__amt&#39;}).find(&amp;quot;span&amp;quot;, {&#39;class&#39; : &#39;strong&#39;}):
                pay_range.append(&amp;quot;N/A&amp;quot;)
        except:
            pay_range.append(&amp;quot;N/A&amp;quot;)

        driver.quit()

final = []
for item in zip_longest(job_title, company_name, mean_pay, pay_range):
    final.append(item)

df = pd.DataFrame(
    final, columns=[&#39;jobTitle&#39;, &#39;companyName&#39;, &#39;meanPay&#39;, &#39;payRange&#39;])

df.to_csv(&amp;quot;Data Engineer Salaries United States.csv&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;
</description>
    </item>
    
  </channel>
</rss>
