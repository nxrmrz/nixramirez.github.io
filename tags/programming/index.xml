<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Programming | Hello, World</title>
    <link>https://nixramirez.github.io/tags/programming/</link>
      <atom:link href="https://nixramirez.github.io/tags/programming/index.xml" rel="self" type="application/rss+xml" />
    <description>Programming</description>
    <generator>Source Themes Academic (https://sourcethemes.com/academic/)</generator><language>en-us</language><lastBuildDate>Sat, 02 May 2020 00:00:00 +0000</lastBuildDate>
    <image>
      <url>img/map[gravatar:%!s(bool=false) shape:circle]</url>
      <title>Programming</title>
      <link>https://nixramirez.github.io/tags/programming/</link>
    </image>
    
    <item>
      <title>Detecting Seasons in React.js</title>
      <link>https://nixramirez.github.io/project/reactfirstproj/</link>
      <pubDate>Sat, 02 May 2020 00:00:00 +0000</pubDate>
      <guid>https://nixramirez.github.io/project/reactfirstproj/</guid>
      <description>&lt;p&gt;Here&amp;rsquo;s a proud moment - the first front-end I&amp;rsquo;ve built with React.js and JSX! It&amp;rsquo;s very simple, but it taught me a lot on how React worked internally.&lt;/p&gt;
&lt;p&gt;This app displays either two screens dependent on whether it’s summer or winter wherever the user is currently.&lt;/p&gt;
&lt;p&gt;A user in New Zealand (i.e. me) is in the southern hemisphere where it’s currenty winter (May 2nd, when this blogpost was written). The app displays the winter screen in this case.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;winterscreen.png&#34; alt=&#34;Winter Screen&#34;&gt;&lt;/p&gt;
&lt;p&gt;A user in San Francisco, on the other hand, is in the northern hemisphere where it’s currently summer. The app displays the summer screen for them.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;summerscreen.png&#34; alt=&#34;Summer Screen&#34;&gt;&lt;/p&gt;
&lt;p&gt;How do we know where the user is? Well, we can detect user location with the 
&lt;a href=&#34;https://developer.mozilla.org/en-US/docs/Web/API/Geolocation_API&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Mozilla Geolocation API&lt;/a&gt;
. We can grab latitude and longitude information about the user from this service.&lt;/p&gt;
&lt;p&gt;We can also force the Google Chrome browser to consider a location like San Francisco away from us by changing its geolocation in its ‘Sensors’ tab like so (mostly for testing the different views):&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;forcelocationchange.png&#34; alt=&#34;Force location change&#34;&gt;&lt;/p&gt;
&lt;p&gt;If the user has disabled location-sharing in any-way, the screen below encourages them to enable it&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;acceptlocationscreen.png&#34; alt=&#34;Accept location screen&#34;&gt;&lt;/p&gt;
&lt;p&gt;Semantic UI and some CSS was used to style the webpage.&lt;/p&gt;
&lt;p&gt;This project taught me React&amp;rsquo;s fundamentals - class based and functional components, the component life cycle methods, how to update component state, and how to pass information down from parent to children components using the props system.&lt;/p&gt;
&lt;p&gt;I&amp;rsquo;m currently building and deploying a more complex full-stack web app, integrating node.js/express.js for the back-end and a mongodb database. I might write a tutorial on that when it&amp;rsquo;s done, so if you&amp;rsquo;re interested, please stay tuned for that post! :)&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Intro To Python Workshop</title>
      <link>https://nixramirez.github.io/project/pythonworkshop/</link>
      <pubDate>Fri, 17 Apr 2020 00:00:00 +0000</pubDate>
      <guid>https://nixramirez.github.io/project/pythonworkshop/</guid>
      <description>&lt;p&gt;In this project, I taught an introductory workshop on Python&amp;rsquo;s base syntax, plus some basic libraries for working with data (i.e. pandas, numpy, matplotlib).&lt;/p&gt;
&lt;p&gt;This was done for the University of Auckland Data Science Club, which I help run.&lt;/p&gt;
&lt;p&gt;It was one workshop in a series teaching essential end-to-end data science skills. Workshops on Data Cleaning, Data Visualisation and Data modelling were planned for the future.&lt;/p&gt;
&lt;p&gt;I work as a programming tutor, so I thought teaching such a workshop would be easy. &lt;strong&gt;It wasn&amp;rsquo;t!&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Firstly, I had to come up with &lt;strong&gt;all&lt;/strong&gt; of the material on my own &amp;ndash; the recordings, the jupyter notebooks, the coding challenges, and sourcing the data-set. The recorded format also made it all the more daunting, as any mistakes I made would go on record officially! I also worried about delivering an engaging workshop. After all, the online nature of the workshop meant people could drop out anytime they wished.&lt;/p&gt;
&lt;p&gt;We were surprised to see good engagement with this workshop, though, with people attending our Live Q&amp;amp;A sessions after the workshop to ask for help regarding our coding challenges. This suggested people were watching our videos &amp;lsquo;til the end. I&amp;rsquo;ve also received positive verbal feedback about the workshop from members, and the youtube view count for both videos suggested pretty good engagement!&lt;/p&gt;
&lt;p&gt;This was such a rewarding experience, which highlighted a few things: 1.) Teaching isn&amp;rsquo;t easy 2.) Teaching is &lt;strong&gt;very hard&lt;/strong&gt; to get right 3.) I love teaching and sharing the beauty of programming.&lt;/p&gt;
&lt;p&gt;So now I present, the University of Auckland Data Science Club&amp;rsquo;s Intro to Python workshop!&lt;/p&gt;
&lt;h3 id=&#34;part-one-python-base-snytax&#34;&gt;Part One: Python Base Snytax&lt;/h3&gt;
&lt;iframe width=&#34;560&#34; height=&#34;315&#34; src=&#34;https://www.youtube.com/embed/S6hwKDvI24c&#34; frameborder=&#34;0&#34; allow=&#34;accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture&#34; allowfullscreen&gt;&lt;/iframe&gt;
&lt;h3 id=&#34;part-two-python-for-data-science&#34;&gt;Part Two: Python for Data Science&lt;/h3&gt;
&lt;iframe width=&#34;560&#34; height=&#34;315&#34; src=&#34;https://www.youtube.com/embed/n9X6ObJ9kos&#34; frameborder=&#34;0&#34; allow=&#34;accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture&#34; allowfullscreen&gt;&lt;/iframe&gt;
&lt;br&gt;
&lt;p&gt;The materials accompanying the workshop are 
&lt;a href=&#34;https://drive.google.com/drive/folders/1o3mH_-6ANT7EfiFWV31IXbhkc9P8VsAn?usp=sharing&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;found here&lt;/a&gt;
. These include the Jupyter notebooks used in both parts, a set of coding challenges to try after going through the workshop (and their associated answers), and the data-set used in part two.&lt;/p&gt;
&lt;p&gt;Please reach out if you have any feedback or queries! :)&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Deploying a deep learning model in AWS</title>
      <link>https://nixramirez.github.io/project/batch-inference-aws-deployment/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://nixramirez.github.io/project/batch-inference-aws-deployment/</guid>
      <description>&lt;p&gt;If you&amp;rsquo;re a data professional working with machine learning models in the cloud, then you might be familiar with Amazon Web Services&amp;rsquo; (AWS) SageMaker. 
&lt;a href=&#34;https://aws.amazon.com/sagemaker/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;SageMaker&lt;/a&gt;
 is AWS&amp;rsquo; machine learning platform allowing developers to build, train, and deploy models in the cloud or in embedded systems and edge devices.&lt;/p&gt;
&lt;p&gt;There are a dizzying number of services available within SageMaker for all kinds of machine learning use-cases. For the sake of brevity in this post, we will only cover a specific use-case&amp;ndash;that of bringing into AWS, and eventually deploying a deep learning model &lt;strong&gt;trained outside&lt;/strong&gt; of the AWS ecosystem. We&amp;rsquo;d also want the deployed model to perform predictions for a &lt;strong&gt;batch&lt;/strong&gt; of data (i.e. multiple rows), as opposed to a single entry. Also, we&amp;rsquo;d want the model to predict in &lt;strong&gt;serverless&lt;/strong&gt; fashion, meaning we only invoke the model and the hardware powering it &lt;strong&gt;when we&amp;rsquo;re doing predictions&lt;/strong&gt;. Serverless is &lt;em&gt;pay-as-you-go&lt;/em&gt;. The opposite of serverless would be to make the model&amp;rsquo;s hardware &lt;em&gt;persist&lt;/em&gt;, even when not actively predicting.&lt;/p&gt;
&lt;p&gt;In this post, we compare and contrast services relevant to our use-case and show how we deployed a deep learning model, trained outside AWS, for batch inference. You&amp;rsquo;ll hopefully learn how model deployment in SageMaker works and get an overview of other SageMaker features.&lt;/p&gt;
&lt;p&gt;So let&amp;rsquo;s dive right in!&lt;/p&gt;
&lt;h3 id=&#34;the-sagemaker-platform&#34;&gt;The SageMaker Platform&lt;/h3&gt;
&lt;p&gt;To help make sense of the various features SageMaker has, AWS offers a &lt;em&gt;fantastic&lt;/em&gt; 
&lt;a href=&#34;&#34;&gt;technical deep dive&lt;/a&gt;
 on the platform, which I&amp;rsquo;d highly recommend you browse before reading SageMaker documentation.&lt;/p&gt;
&lt;p&gt;SageMaker also has a 
&lt;a href=&#34;&#34;&gt;2 month free tier&lt;/a&gt;
 for hands-on tinkering with the platform, perfect for exploring what it offers!&lt;/p&gt;
&lt;h4 id=&#34;notebooks-and-s3&#34;&gt;Notebooks and S3&lt;/h4&gt;
&lt;p&gt;Central to SageMaker are 
&lt;a href=&#34;&#34;&gt;Notebook Instances&lt;/a&gt;
, Jupyter Notebook environments running on EC2 instances that are fully managed by AWS upon creation. While the &lt;em&gt;fully managed&lt;/em&gt; keyword implies that we can do very little altering of the EC2 instance&amp;rsquo;s system save from some non-persistent shell commands, it also means we can use it out-of-the-box for running common data science packages.&lt;/p&gt;
&lt;p&gt;A Jupyter notebook inside an instance can hold code for reading data in, pre-processing it, using it to train a model, validating the model, and deploying it later. While we can think of a notebook as the front-end for our interactions with SageMaker, we can think of an S3 storage bucket as our back-end. Our notebook instance has to be connected to an S3 bucket to read data from or save/load model artefacts to/from.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;NB:&lt;/strong&gt; When we deploy a model trained in a notebook, we are not deploying the notebook itself. We&amp;rsquo;re just using the notebook to &lt;em&gt;orchestrate&lt;/em&gt; training, validation and deployment.&lt;/p&gt;
&lt;p&gt;We don&amp;rsquo;t cover configuring Notebook Instances and S3 buckets here as there are a lot of tutorials on these subjects. Just make sure that the S3 bucket exists in the same region as the Notebook Instance for them to connect, and that going for the smallest compute hardware is fine for now, since we can modify the size as needed later on.&lt;/p&gt;















&lt;figure&gt;


  &lt;a data-fancybox=&#34;&#34; href=&#34;updating.png&#34; data-caption=&#34;You can update the size (i.e. it&amp;rsquo;s currently ml.t2.medium) of the instance when it&amp;rsquo;s stopped&#34;&gt;


  &lt;img data-src=&#34;updating.png&#34; class=&#34;lazyload&#34; alt=&#34;&#34; width=&#34;100%&#34; height=&#34;100%&#34;&gt;
&lt;/a&gt;


  
  
  &lt;figcaption data-pre=&#34;Figure &#34; data-post=&#34;:&#34; class=&#34;numbered&#34;&gt;
    You can update the size (i.e. it&amp;rsquo;s currently ml.t2.medium) of the instance when it&amp;rsquo;s stopped
  &lt;/figcaption&gt;


&lt;/figure&gt;

&lt;p&gt;After creating and starting a notebook instance, you&amp;rsquo;ll find a repository of example notebooks made by the AWS team for different machine learning use-cases. These notebooks contain code for implementing end-to-end data science pipelines, and are useful starting points when implementing your own.&lt;/p&gt;















&lt;figure&gt;


  &lt;a data-fancybox=&#34;&#34; href=&#34;SageMakerExamples.png&#34; data-caption=&#34;Find examples of end-to-end pipelines here&#34;&gt;


  &lt;img data-src=&#34;SageMakerExamples.png&#34; class=&#34;lazyload&#34; alt=&#34;&#34; width=&#34;50%&#34; height=&#34;50%&#34;&gt;
&lt;/a&gt;


  
  
  &lt;figcaption data-pre=&#34;Figure &#34; data-post=&#34;:&#34; class=&#34;numbered&#34;&gt;
    Find examples of end-to-end pipelines here
  &lt;/figcaption&gt;


&lt;/figure&gt;

&lt;h4 id=&#34;cloudwatch-logs&#34;&gt;CloudWatch Logs&lt;/h4&gt;
&lt;p&gt;The pipelines we run in SageMaker can be set-up programatically in notebooks using the SageMaker Python SDK, or through the SageMaker console otherwise. I prefer the former as the flow is more natural to me, having prototyped countless times with Jupyter Notebooks before. The latter is still worth noting and available as an option for those who prefer to code less.&lt;/p&gt;
&lt;p&gt;Using a notebook and the SDK however allows us to debug our code and monitor the progress of our processes better. To debug Python code in our local systems, we often add &lt;code&gt;print()&lt;/code&gt; statements for monitoring the flow of our program. The AWS equivalent to &lt;code&gt;print()&lt;/code&gt; is the &lt;code&gt;log()&lt;/code&gt; command&lt;/p&gt;
&lt;h4 id=&#34;cloudwatch-logs-1&#34;&gt;CloudWatch Logs&lt;/h4&gt;
&lt;h3 id=&#34;byo-deep-learning-model&#34;&gt;BYO Deep Learning Model&lt;/h3&gt;
&lt;p&gt;As we are bringing a model artifact trained outside of AWS, we first have to make sure it&amp;rsquo;s transferred to AWS S3 storage.&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Create an S3 bucket&lt;/li&gt;
&lt;/ol&gt;
&lt;ul&gt;
&lt;li&gt;S3 in the AWS Console&lt;/li&gt;
&lt;li&gt;Create a bucket, give it a descriptive name. Take note of the region you&amp;rsquo;re creating it in. This region should be the same as the Notebook Instance so the instance could connect to the bucket.&lt;/li&gt;
&lt;/ul&gt;
&lt;ol start=&#34;2&#34;&gt;
&lt;li&gt;Create a Notebook Instance&lt;/li&gt;
&lt;/ol&gt;
&lt;ul&gt;
&lt;li&gt;you will have to specify the instance size, version and bandwidth. The size can be altered later, so choose the smallest one for now to save on costs. The latest version will have the most up-to-date features. The bandwidth (i.e. include examples) is also alterable, so for now choose the smallest one.&lt;/li&gt;
&lt;li&gt;NB: Elastic inference&lt;/li&gt;
&lt;li&gt;Connect it to the S3 bucket you&amp;rsquo;ve made in&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
  </channel>
</rss>
