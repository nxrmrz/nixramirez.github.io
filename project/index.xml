<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Projects | Hello, World</title>
    <link>https://nixramirez.github.io/project/</link>
      <atom:link href="https://nixramirez.github.io/project/index.xml" rel="self" type="application/rss+xml" />
    <description>Projects</description>
    <generator>Source Themes Academic (https://sourcethemes.com/academic/)</generator><language>en-us</language><lastBuildDate>Wed, 27 Apr 2016 00:00:00 +0000</lastBuildDate>
    <image>
      <url>img/map[gravatar:%!s(bool=false) shape:circle]</url>
      <title>Projects</title>
      <link>https://nixramirez.github.io/project/</link>
    </image>
    
    <item>
      <title>External Project</title>
      <link>https://nixramirez.github.io/project/external-project/</link>
      <pubDate>Wed, 27 Apr 2016 00:00:00 +0000</pubDate>
      <guid>https://nixramirez.github.io/project/external-project/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Internal Project</title>
      <link>https://nixramirez.github.io/project/internal-project/</link>
      <pubDate>Wed, 27 Apr 2016 00:00:00 +0000</pubDate>
      <guid>https://nixramirez.github.io/project/internal-project/</guid>
      <description>&lt;p&gt;Lorem ipsum dolor sit amet, consectetur adipiscing elit. Duis posuere tellus ac convallis placerat. Proin tincidunt magna sed ex sollicitudin condimentum. Sed ac faucibus dolor, scelerisque sollicitudin nisi. Cras purus urna, suscipit quis sapien eu, pulvinar tempor diam. Quisque risus orci, mollis id ante sit amet, gravida egestas nisl. Sed ac tempus magna. Proin in dui enim. Donec condimentum, sem id dapibus fringilla, tellus enim condimentum arcu, nec volutpat est felis vel metus. Vestibulum sit amet erat at nulla eleifend gravida.&lt;/p&gt;
&lt;p&gt;Nullam vel molestie justo. Curabitur vitae efficitur leo. In hac habitasse platea dictumst. Sed pulvinar mauris dui, eget varius purus congue ac. Nulla euismod, lorem vel elementum dapibus, nunc justo porta mi, sed tempus est est vel tellus. Nam et enim eleifend, laoreet sem sit amet, elementum sem. Morbi ut leo congue, maximus velit ut, finibus arcu. In et libero cursus, rutrum risus non, molestie leo. Nullam congue quam et volutpat malesuada. Sed risus tortor, pulvinar et dictum nec, sodales non mi. Phasellus lacinia commodo laoreet. Nam mollis, erat in feugiat consectetur, purus eros egestas tellus, in auctor urna odio at nibh. Mauris imperdiet nisi ac magna convallis, at rhoncus ligula cursus.&lt;/p&gt;
&lt;p&gt;Cras aliquam rhoncus ipsum, in hendrerit nunc mattis vitae. Duis vitae efficitur metus, ac tempus leo. Cras nec fringilla lacus. Quisque sit amet risus at ipsum pharetra commodo. Sed aliquam mauris at consequat eleifend. Praesent porta, augue sed viverra bibendum, neque ante euismod ante, in vehicula justo lorem ac eros. Suspendisse augue libero, venenatis eget tincidunt ut, malesuada at lorem. Donec vitae bibendum arcu. Aenean maximus nulla non pretium iaculis. Quisque imperdiet, nulla in pulvinar aliquet, velit quam ultrices quam, sit amet fringilla leo sem vel nunc. Mauris in lacinia lacus.&lt;/p&gt;
&lt;p&gt;Suspendisse a tincidunt lacus. Curabitur at urna sagittis, dictum ante sit amet, euismod magna. Sed rutrum massa id tortor commodo, vitae elementum turpis tempus. Lorem ipsum dolor sit amet, consectetur adipiscing elit. Aenean purus turpis, venenatis a ullamcorper nec, tincidunt et massa. Integer posuere quam rutrum arcu vehicula imperdiet. Mauris ullamcorper quam vitae purus congue, quis euismod magna eleifend. Vestibulum semper vel augue eget tincidunt. Fusce eget justo sodales, dapibus odio eu, ultrices lorem. Duis condimentum lorem id eros commodo, in facilisis mauris scelerisque. Morbi sed auctor leo. Nullam volutpat a lacus quis pharetra. Nulla congue rutrum magna a ornare.&lt;/p&gt;
&lt;p&gt;Aliquam in turpis accumsan, malesuada nibh ut, hendrerit justo. Cum sociis natoque penatibus et magnis dis parturient montes, nascetur ridiculus mus. Quisque sed erat nec justo posuere suscipit. Donec ut efficitur arcu, in malesuada neque. Nunc dignissim nisl massa, id vulputate nunc pretium nec. Quisque eget urna in risus suscipit ultricies. Pellentesque odio odio, tincidunt in eleifend sed, posuere a diam. Nam gravida nisl convallis semper elementum. Morbi vitae felis faucibus, vulputate orci placerat, aliquet nisi. Aliquam erat volutpat. Maecenas sagittis pulvinar purus, sed porta quam laoreet at.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Web Scraping</title>
      <link>https://nixramirez.github.io/project/data-scraping/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://nixramirez.github.io/project/data-scraping/</guid>
      <description>&lt;h3 id=&#34;the-idea&#34;&gt;The Idea&lt;/h3&gt;
&lt;p&gt;An ex-colleague, 
&lt;a href=&#34;https://www.linkedin.com/in/dhilip-subramanian-36021918b/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Dhilip&lt;/a&gt;
 and I wanted to do a series of end-to-end projects in Data Science and Analytics for fun and to pick up skills along the way that would serve our future careers in this space. When Dhilip approached me for project ideas, I already had one I&amp;rsquo;ve thought about for quite some time.&lt;/p&gt;
&lt;p&gt;It was a timely project for us Qrious interns, who were deliberating on our next career move following the summer internship. Towards the end of our internship, a few of us (excluding me but including Dhilip) were graduating and applying for full-time jobs in the data space. Our discussions naturally gravitated to what starting pay we could expect for careers in data science, data engineering, and analytics. I imagined if we had an amount for each job averaged from recent, accurate salary data collected over a large number of companies operating in different industries, we had evidence to make sure future salary negotiations were fair!&lt;/p&gt;
&lt;h3 id=&#34;the-execution&#34;&gt;The Execution&lt;/h3&gt;
&lt;p&gt;Glassdoor is a job reviews site with job benefits and salary information reported anonymously by employees of various companies. It was our main data source for this project. Collecting a vast amount of data from the site would require automation, using a technique called &lt;em&gt;scraping&lt;/em&gt;. This is a technique used to extract content from specific HTML tags in a webpage, and in our case, exploits two technologies to do so: Selenium and BeautifulSoup.&lt;/p&gt;
&lt;p&gt;Selenium is a Java-based tool used in website testing to automate specific interactions with webpages (i.e. clicking on links, logging in, navigating the page, etc). Since it automates interacting with the DOM, it&amp;rsquo;s being used in Data Science to scrape data from websites. BeautifulSoup, on the other hand, is a Python library that parses HTML and makes it easy to extract specific elements from it.&lt;/p&gt;
&lt;h3 id=&#34;the-code&#34;&gt;The code&lt;/h3&gt;
&lt;p&gt;A work in progress (explanations to follow:)&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from selenium import webdriver
from selenium.webdriver.chrome.options import Options
import pandas as pd
from time import sleep
from bs4 import BeautifulSoup, Comment
import time
import os
import csv
import lxml
from itertools import zip_longest
from webdriver_manager.chrome import ChromeDriverManager

#setting up an automated google chrome browser
driver = webdriver.Chrome(ChromeDriverManager().install())

job_title = []
company_name = []
mean_pay = []
pay_range = []

#going through 184 pages of salary information
for pageno in range(1,184):

    driver = webdriver.Chrome(ChromeDriverManager().install())
    
    #getting webpage in glassdoor
    if pageno == 1:
        driver.get(&amp;quot;https://www.glassdoor.co.nz/Salaries/us-data-engineer-salary-SRCH_IL.0,2_IN1_KO3,16.htm&amp;quot;)
    else:
        driver.get(
            &amp;quot;https://www.glassdoor.co.nz/Salaries/us-data-engineer-salary-SRCH_IL.0,2_IN1_KO3,16.htm&amp;quot; + &amp;quot;_IP&amp;quot; + str(pageno) + &amp;quot;.htm&amp;quot;
        )
    time.sleep(1.5)

    #parsing the page through lxml option of beautifulsoup
    html = driver.page_source
    soup = BeautifulSoup(html, &#39;lxml&#39;)

    #getting each salary block
    salaryBlocks = soup.findAll(&amp;quot;div&amp;quot;, {&#39;class&#39; : &#39;row align-items-center m-0 salaryRow__SalaryRowStyle__row&#39;})

    #for each salary block, find the job title, company name, average pay, and pay range, and append them to the lists initialised above
    for block in salaryBlocks:
        entry = []

        jobTitle = block.find(&amp;quot;div&amp;quot;, {&#39;class&#39; : &#39;salaryRow__JobInfoStyle__jobTitle strong&#39;}).find(&amp;quot;a&amp;quot;).text
        job_title.append(jobTitle)

        companyName = block.find(&amp;quot;div&amp;quot;, {&#39;class&#39; : &#39;salaryRow__JobInfoStyle__employerName&#39;}).text
        company_name.append(companyName)

        meanPay = block.find(&amp;quot;div&amp;quot;, {&#39;class&#39; : &#39;salaryRow__JobInfoStyle__meanBasePay common__formFactorHelpers__showHH&#39;}).find(&#39;span&#39;).text
        mean_pay.append(meanPay)
        
        #if a pay range exists, grab it, otherwise, indicate none exists
        try:
            if block.find(&amp;quot;div&amp;quot;, {&#39;class&#39; : &#39;col-2 d-none d-md-block px-0 py salaryRow__SalaryRowStyle__amt&#39;}).find(&amp;quot;div&amp;quot;, {&#39;class&#39; : &#39;strong&#39;}):
                payRange = block.find(&amp;quot;div&amp;quot;, {&#39;class&#39; : &#39;col-2 d-none d-md-block px-0 py salaryRow__SalaryRowStyle__amt&#39;}).find(&amp;quot;div&amp;quot;, {&#39;class&#39; : &#39;strong&#39;}).text
                pay_range.append(payRange)
            elif block.find(&amp;quot;div&amp;quot;, {&#39;class&#39; : &#39;col-2 d-none d-md-block px-0 py salaryRow__SalaryRowStyle__amt&#39;}).find(&amp;quot;span&amp;quot;, {&#39;class&#39; : &#39;strong&#39;}):
                pay_range.append(&amp;quot;N/A&amp;quot;)
        except:
            pay_range.append(&amp;quot;N/A&amp;quot;)

        driver.quit()

#process the lists into a final dataframe, and save to a CSV
final = []
for item in zip_longest(job_title, company_name, mean_pay, pay_range):
    final.append(item)

df = pd.DataFrame(
    final, columns=[&#39;jobTitle&#39;, &#39;companyName&#39;, &#39;meanPay&#39;, &#39;payRange&#39;])

df.to_csv(&amp;quot;Data Engineer Salaries United States.csv&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;
</description>
    </item>
    
  </channel>
</rss>
